"use strict";(self.webpackChunkstarrocks=self.webpackChunkstarrocks||[]).push([[59045],{63007:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var s=t(85893),r=t(11151);const i={displayed_sidebar:"English"},a="Continuously load data from Apache Flink\xae",o={id:"loading/Flink-connector-starrocks",title:"Continuously load data from Apache Flink\xae",description:"StarRocks provides a self-developed connector named Flink connector for Apache Flink\xae (Flink connector for short) to help you load data into a StarRocks table by using Flink. The basic principle is to accumulate the data and then load it all at a time into StarRocks through STREAM LOAD.",source:"@site/versioned_docs/version-2.5/loading/Flink-connector-starrocks.md",sourceDirName:"loading",slug:"/loading/Flink-connector-starrocks",permalink:"/docs/2.5/loading/Flink-connector-starrocks",draft:!1,unlisted:!1,editUrl:"https://github.com/StarRocks/starrocks/edit/main/docs/loading/Flink-connector-starrocks.md",tags:[],version:"2.5",frontMatter:{displayed_sidebar:"English"},sidebar:"English",previous:{title:"Transform data at loading",permalink:"/docs/2.5/loading/Etl_in_loading"},next:{title:"Synchronize data from MySQL in real time",permalink:"/docs/2.5/loading/Flink_cdc_load"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Procedure",id:"procedure",level:2},{value:"Usage notes",id:"usage-notes",level:2},{value:"Flush Policy",id:"flush-policy",level:3},{value:"Monitoring load metrics",id:"monitoring-load-metrics",level:3},{value:"Examples",id:"examples",level:2},{value:"Preparations",id:"preparations",level:3},{value:"Create a StarRocks table",id:"create-a-starrocks-table",level:4},{value:"Set up Flink environment",id:"set-up-flink-environment",level:4},{value:"Run with Flink SQL",id:"run-with-flink-sql",level:3},{value:"Run with Flink DataStream",id:"run-with-flink-datastream",level:3},{value:"Best practices",id:"best-practices",level:2},{value:"Load data to a Primary Key table",id:"load-data-to-a-primary-key-table",level:3},{value:"Preparations",id:"preparations-1",level:4},{value:"Partial update",id:"partial-update",level:4},{value:"Conditional update",id:"conditional-update",level:4},{value:"Load data into columns of BITMAP type",id:"load-data-into-columns-of-bitmap-type",level:3},{value:"Load data into columns of HLL type",id:"load-data-into-columns-of-hll-type",level:3}];function d(e){const n=Object.assign({h1:"h1",p:"p",a:"a",blockquote:"blockquote",strong:"strong",h2:"h2",ol:"ol",li:"li",code:"code",pre:"pre",ul:"ul",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",div:"div",h3:"h3",h4:"h4"},(0,r.ah)(),e.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"continuously-load-data-from-apache-flink",children:"Continuously load data from Apache Flink\xae"}),"\n",(0,s.jsxs)(n.p,{children:["StarRocks provides a self-developed connector named Flink connector for Apache Flink\xae (Flink connector for short) to help you load data into a StarRocks table by using Flink. The basic principle is to accumulate the data and then load it all at a time into StarRocks through ",(0,s.jsx)(n.a,{href:"/docs/2.5/sql-reference/sql-statements/data-manipulation/STREAM_LOAD",children:"STREAM LOAD"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["The Flink connector supports DataStream API, Table API & SQL, and Python API. It has a higher and more stable performance than ",(0,s.jsx)(n.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/jdbc/",children:"flink-connector-jdbc"})," provided by Apache Flink\xae."]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,s.jsxs)(n.p,{children:["Loading data into StarRocks tables with Flink connector needs SELECT and INSERT privileges. If you do not have these privileges, follow the instructions provided in ",(0,s.jsx)(n.a,{href:"/docs/2.5/sql-reference/sql-statements/account-management/GRANT",children:"GRANT"})," to grant these privileges to the user that you use to connect to your StarRocks cluster."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsxs)(n.p,{children:["The flink-connector-jdbc tool provided by Apache Flink\xae may not meet your performance requirements in certain scenarios. Therefore we provide a new connector named flink-connector-starrocks, which can cache data and then load data at a time by using ",(0,s.jsx)(n.a,{href:"/docs/2.5/loading/StreamLoad",children:"Stream Load"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"procedure",children:"Procedure"}),"\n",(0,s.jsx)(n.p,{children:"To load data from Apache Flink\xae into StarRocks by using flink-connector-starrocks, perform the following steps:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Download the ",(0,s.jsx)(n.a,{href:"https://github.com/StarRocks/flink-connector-starrocks",children:"source code"})," of flink-connector-starrocks."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Find a file named ",(0,s.jsx)(n.strong,{children:"pom.xml"}),". Add the following code snippet to ",(0,s.jsx)(n.strong,{children:"pom.xml"})," and replace ",(0,s.jsx)(n.code,{children:"x.x.x"})," in the code snippet with the latest version number of flink-connector-starrocks."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain_Text",children:"<dependency>\n    <groupId>com.starrocks</groupId>\n    <artifactId>flink-connector-starrocks</artifactId>\n    \x3c!-- for flink-1.11, flink-1.12 --\x3e\n    <version>x.x.x_flink-1.11</version>\n    \x3c!-- for flink-1.13 --\x3e\n    <version>x.x.x_flink-1.13</version>\n</dependency>\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Use one of the following methods to load data"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Load data as raw JSON string streams."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'// -------- sink with raw json string stream --------\nfromElements(new String[]{\n    "{\\"score\\": \\"99\\", \\"name\\": \\"stephen\\"}",\n    "{\\"score\\": \\"100\\", \\"name\\": \\"lebron\\"}"\n}).addSink(\n    StarRocksSink.sink(\n        // the sink options\n        StarRocksSinkOptions.builder()\n            .withProperty("jdbc-url", "jdbc:mysql://fe1_ip:query_port,fe2_ip:query_port,fe3_ip:query_port,xxxxx")\n            .withProperty("load-url", "fe1_ip:http_port;fe2_ip:http_port;fe3_ip:http_port")\n            .withProperty("username", "xxx")\n            .withProperty("password", "xxx")\n            .withProperty("table-name", "xxx")\n            .withProperty("database-name", "xxx")\n            // Since 2.4, StarRocks support partial updates for Primary Key tables. You can specify the columns to be updated by configuring the following two properties.\n            // The \'__op\' column must be specified at the end of \'sink.properties.columns\'.\n            // .withProperty("sink.properties.partial_update", "true")\n            // .withProperty("sink.properties.columns", "k1,k2,k3,__op")\n            .withProperty("sink.properties.format", "json")\n            .withProperty("sink.properties.strip_outer_array", "true")\n            .build()\n    )\n).setParallelism(1); // Define the parallelism of the sink. In the scenario of multiple paralel sinks, you need to guarantee the data order. \n\n// -------- sink with stream transformation --------\nclass RowData {\n    public int score;\n    public String name;\n    public RowData(int score, String name) {\n        ......\n    }\n}\nfromElements(\n    new RowData[]{\n        new RowData(99, "stephen"),\n        new RowData(100, "lebron")\n    }\n).addSink(\n    StarRocksSink.sink(\n        // the table structure\n        TableSchema.builder()\n            .field("score", DataTypes.INT())\n            .field("name", DataTypes.VARCHAR(20))\n            .build(),\n        // the sink options\n        StarRocksSinkOptions.builder()\n            .withProperty("jdbc-url", "jdbc:mysql://fe1_ip:query_port,fe2_ip:query_port,fe3_ip:query_port,xxxxx")\n            .withProperty("load-url", "fe1_ip:http_port;fe2_ip:http_port;fe3_ip:http_port")\n            .withProperty("username", "xxx")\n            .withProperty("password", "xxx")\n            .withProperty("table-name", "xxx")\n            .withProperty("database-name", "xxx")\n            // Since 2.4, StarRocks support partial updates for Primary Key tables. You can specify the columns to be updated by configuring the following two properties.\n            // The \'__op\' column must be specified at the end of \'sink.properties.columns\'.\n            // .withProperty("sink.properties.partial_update", "true")\n            // .withProperty("sink.properties.columns", "k1,k2,k3,__op")\n            .withProperty("sink.properties.format", "csv")  \n            .withProperty("sink.properties.column_separator", "\\\\x01")\n            .withProperty("sink.properties.row_delimiter", "\\\\x02")\n            .build(),\n        // set the slots with streamRowData\n        (slots, streamRowData) -> {\n            slots[0] = streamRowData.score;\n            slots[1] = streamRowData.name;\n        }\n    )\n);\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Load data as tables."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"// create a table with `structure` and `properties`\n// Needed: Add `com.starrocks.connector.flink.table.StarRocksDynamicTableSinkFactory` to: `src/main/resources/META-INF/services/org.apache.flink.table.factories.Factory`\ntEnv.executeSql(\n    \"CREATE TABLE USER_RESULT(\" +\n        \"name VARCHAR,\" +\n        \"score BIGINT\" +\n    \") WITH ( \" +\n        \"'connector' = 'starrocks',\" +\n        \"'jdbc-url'='jdbc:mysql://fe1_ip:query_port,fe2_ip:query_port,fe3_ip:query_port?xxxxx',\" +\n        \"'load-url'='fe1_ip:http_port;fe2_ip:http_port;fe3_ip:http_port',\" +\n        \"'database-name' = 'xxx',\" +\n        \"'table-name' = 'xxx',\" +\n        \"'username' = 'xxx',\" +\n        \"'password' = 'xxx',\" +\n        \"'sink.buffer-flush.max-rows' = '1000000',\" +\n        \"'sink.buffer-flush.max-bytes' = '300000000',\" +\n        \"'sink.buffer-flush.interval-ms' = '5000',\" +\n        // Since 2.4, StarRocks support partial updates for Primary Key tables. You can specify the columns to be updated by configuring the following two properties.\n        // The '__op' column must be specified at the end of 'sink.properties.columns'.\n        // \"'sink.properties.partial_update' = 'true',\" +\n        // \"'sink.properties.columns' = 'k1,k2,k3',\" + \n        \"'sink.properties.column_separator' = '\\\\x01',\" +\n        \"'sink.properties.row_delimiter' = '\\\\x02',\" +\n        \"'sink.max-retries' = '3',\" +\n        // Stream load properties like `'sink.properties.columns' = 'k1, v1'`\n        \"'sink.properties.*' = 'xxx',\" + \n        // Define the parallelism of the sink. In the scenario of multiple paralel sinks, you need to guarantee the data order.\n        \"'sink.parallelism' = '1'\"\n    \")\"\n);\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The following table describes the ",(0,s.jsx)(n.code,{children:"sink"})," options that you can configure when you load data as tables."]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Option"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Required"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Default value"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Data type"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Description"})})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"connector"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"NONE"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"The connector that you want to use. The value must be starrocks."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"jdbc-url"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"NONE"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"The URL that is used to query data from StarRocks."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"load-url"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"NONE"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsxs)(n.td,{children:["The URL that is used to load all data in a time. Format: fe_ip",(0,s.jsx)(n.div,{}),";fe_ip",(0,s.jsx)(n.div,{}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"database-name"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"NONE"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"The name of the StarRocks database into which you want to load data."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"table-name"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"NONE"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"The name of the table that you want to use to load data into StarRocks."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"username"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"NONE"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"The username of the account that you want to use to load data into StarRocks."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"password"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"NONE"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"The password of the preceding account."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"sink.semantic"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"at-least-once"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsxs)(n.td,{children:["The semantics that is supported by your sink. Valid values: ",(0,s.jsx)(n.strong,{children:"at-least-once"})," and ",(0,s.jsx)(n.strong,{children:"exactly-once"}),". If you specify the value as exactly-once, ",(0,s.jsx)(n.code,{children:"sink.buffer-flush.max-bytes"}),", ",(0,s.jsx)(n.code,{children:"sink.buffer-flush.max-bytes"}),", and ",(0,s.jsx)(n.code,{children:"sink.buffer-flush.interval-ms"})," are invalid."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"sink.buffer-flush.max-bytes"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"94371840(90M)"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"The maximum size of data that can be loaded into StarRocks at a time. Valid values: 64 MB to 10 GB."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"sink.buffer-flush.max-rows"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"500000"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"The maximum number of rows that can be loaded into StarRocks at a time. Valid values: 64000 to 5000000."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"sink.buffer-flush.interval-ms"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"300000"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"The interval at which data is flushed. Valid values: 1000 to 3600000. Unit: ms."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"sink.max-retries"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"3"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"The number of times that the system retries to perform the Stream Load. Valid values: 0 to 10."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"sink.connect.timeout-ms"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"1000"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"The period of time after which the stream load times out. Valid values: 100 to 60000. Unit: ms."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"sink.properties.*"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"NONE"}),(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"The properties of the stream load. The properties include k1, k2, and k3. Since 2.4, the flink-connector-starrocks supports partial updates for Primary Key tables."})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"usage-notes",children:"Usage notes"}),"\n",(0,s.jsx)(n.p,{children:"When you load data from Apache Flink\xae into StarRocks, take note of the following points:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"If you specify the exactly-once semantics, the two-phase commit (2PC) protocol must be supported to ensure efficient data loading. StarRocks does not support this protocol. Therefore we need to rely on Apache Flink\xae to achieve exactly-once. The overall process is as follows:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Save data and its label at each checkpoint that is completed at a specific checkpoint interval."}),"\n",(0,s.jsx)(n.li,{children:"After data and labels are saved, block the flushing of data cached in the state at the first invoke after each checkpoint is completed."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"If StarRocks unexpectedly exits, the operators for Apache Flink\xae sink streaming are blocked for a long time and Apache Flink\xae issues a monitoring alert or shuts down."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["By default, data is loaded in the CSV format. You can set the ",(0,s.jsx)(n.code,{children:"sink.properties.row_delimiter"})," parameter to ",(0,s.jsx)(n.code,{children:"\\\\x02"})," to specify a row separator and set the ",(0,s.jsx)(n.code,{children:"sink.properties.column_separator"})," parameter to ",(0,s.jsx)(n.code,{children:"\\\\x01"})," to specify a column separator."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"If data loading pauses, you can increase the memory of the Flink task."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If the version of Flink connector is 1.2.8 and later, it is recommended to specify the value of ",(0,s.jsx)(n.code,{children:"sink.label-prefix"}),". Note that the label prefix must be unique among all types of loading in StarRocks, such as Flink jobs, Routine Load, and Broker Load."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If the label prefix is specified, the Flink connector will use the label prefix to clean up lingering transactions that may be generated in some Flink\nfailure scenarios, such as the Flink job fails when a checkpoint is still in progress. These lingering transactions\nare generally in ",(0,s.jsx)(n.code,{children:"PREPARED"})," status if you use ",(0,s.jsx)(n.code,{children:"SHOW PROC '/transactions/<db_id>/running';"})," to view them in StarRocks. When the Flink job restores from checkpoint,\nthe Flink connector will find these lingering transactions according to the label prefix and some information in\ncheckpoint, and abort them. The Flink connector can not abort them when the Flink job exits because of the two-phase-commit\nmechanism to implement the exactly-once. When the Flink job exits, the Flink connector has not received the notification from\nFlink checkpoint coordinator whether the transactions should be included in a successful checkpoint, and it may\nlead to data loss if these transactions are aborted anyway. You can have an overview about how to achieve end-to-end exactly-once\nin Flink in this ",(0,s.jsx)(n.a,{href:"https://flink.apache.org/2018/02/28/an-overview-of-end-to-end-exactly-once-processing-in-apache-flink-with-apache-kafka-too/",children:"blogpost"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If the label prefix is not specified, lingering transactions will be cleaned up by StarRocks only after they time out. However the number of running transactions can reach the limitation of StarRocks ",(0,s.jsx)(n.code,{children:"max_running_txn_num_per_db"})," if\nFlink jobs fail frequently before transactions time out. The timeout length is controlled by StarRocks FE configuration\n",(0,s.jsx)(n.code,{children:"prepared_transaction_default_timeout_second"})," whose default value is ",(0,s.jsx)(n.code,{children:"86400"})," (1 day). You can set a smaller value to it\nto make transactions expired faster when the label prefix is not specified."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"If you are certain that the Flink job will eventually recover from checkpoint or savepoint after a long downtime because of stop or continuous failover,\nplease adjust the following StarRocks configurations accordingly, to avoid data loss."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"prepared_transaction_default_timeout_second"}),": StarRocks FE configuration, default value is ",(0,s.jsx)(n.code,{children:"86400"}),". The value of this configuration needs to be larger than the downtime\nof the Flink job. Otherwise, the lingering transactions that are included in a successful checkpoint may be aborted because of timeout before you restart the\nFlink job, which leads to data loss."]}),"\n",(0,s.jsxs)(n.p,{children:["Note that when you set a larger value to this configuration, it is better to specify the value of ",(0,s.jsx)(n.code,{children:"sink.label-prefix"})," so that the lingering transactions can be cleaned according to the label prefix and some information in\ncheckpoint, instead of due to timeout (which may cause data loss)."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"label_keep_max_second"})," and ",(0,s.jsx)(n.code,{children:"label_keep_max_num"}),": StarRocks FE configurations, default values are ",(0,s.jsx)(n.code,{children:"259200"})," and ",(0,s.jsx)(n.code,{children:"1000"}),"\nrespectively. For details, see ",(0,s.jsx)(n.a,{href:"/docs/2.5/loading/Loading_intro#fe-configurations",children:"FE configurations"}),". The value of ",(0,s.jsx)(n.code,{children:"label_keep_max_second"})," needs to be larger than the downtime of the Flink job. Otherwise, the Flink connector can not check the state of transactions in StarRocks by using the transaction labels saved in the Flink's savepoint or checkpoint and figure out whether these transactions are committed, which may eventually lead to data loss."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["These configurations are mutable and can be modified by using ",(0,s.jsx)(n.code,{children:"ADMIN SET FRONTEND CONFIG"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'  ADMIN SET FRONTEND CONFIG ("prepared_transaction_default_timeout_second" = "3600");\n  ADMIN SET FRONTEND CONFIG ("label_keep_max_second" = "259200");\n  ADMIN SET FRONTEND CONFIG ("label_keep_max_num" = "1000");\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"flush-policy",children:"Flush Policy"}),"\n",(0,s.jsx)(n.p,{children:"The Flink connector will buffer the data in memory, and flush them in batch to StarRocks via Stream Load. How the flush\nis triggered is different between at-least-once and exactly-once."}),"\n",(0,s.jsx)(n.p,{children:"For at-least-once, the flush will be triggered when any of the following conditions are met:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["the bytes of buffered rows reaches the limit ",(0,s.jsx)(n.code,{children:"sink.buffer-flush.max-bytes"})]}),"\n",(0,s.jsxs)(n.li,{children:["the number of buffered rows reaches the limit ",(0,s.jsx)(n.code,{children:"sink.buffer-flush.max-rows"}),". (Only valid for sink version V1)"]}),"\n",(0,s.jsxs)(n.li,{children:["the elapsed time since the last flush reaches the limit ",(0,s.jsx)(n.code,{children:"sink.buffer-flush.interval-ms"})]}),"\n",(0,s.jsx)(n.li,{children:"a checkpoint is triggered"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"For exactly-once, the flush only happens when a checkpoint is triggered."}),"\n",(0,s.jsx)(n.h3,{id:"monitoring-load-metrics",children:"Monitoring load metrics"}),"\n",(0,s.jsx)(n.p,{children:"The Flink connector provides the following metrics to monitor loading."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Metric"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"totalFlushBytes"}),(0,s.jsx)(n.td,{children:"counter"}),(0,s.jsx)(n.td,{children:"successfully flushed bytes."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"totalFlushRows"}),(0,s.jsx)(n.td,{children:"counter"}),(0,s.jsx)(n.td,{children:"number of rows successfully flushed."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"totalFlushSucceededTimes"}),(0,s.jsx)(n.td,{children:"counter"}),(0,s.jsx)(n.td,{children:"number of times that the data is successfully flushed."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"totalFlushFailedTimes"}),(0,s.jsx)(n.td,{children:"counter"}),(0,s.jsx)(n.td,{children:"number of times that the data fails to be flushed."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"totalFilteredRows"}),(0,s.jsx)(n.td,{children:"counter"}),(0,s.jsx)(n.td,{children:"number of rows filtered, which is also included in totalFlushRows."})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,s.jsx)(n.p,{children:"The following examples show how to use the Flink connector to load data into a StarRocks table with Flink SQL or Flink DataStream."}),"\n",(0,s.jsx)(n.h3,{id:"preparations",children:"Preparations"}),"\n",(0,s.jsx)(n.h4,{id:"create-a-starrocks-table",children:"Create a StarRocks table"}),"\n",(0,s.jsxs)(n.p,{children:["Create a database ",(0,s.jsx)(n.code,{children:"test"})," and create a Primary Key table ",(0,s.jsx)(n.code,{children:"score_board"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:'CREATE DATABASE `test`;\n\nCREATE TABLE `test`.`score_board`\n(\n    `id` int(11) NOT NULL COMMENT "",\n    `name` varchar(65533) NULL DEFAULT "" COMMENT "",\n    `score` int(11) NOT NULL DEFAULT "0" COMMENT ""\n)\nENGINE=OLAP\nPRIMARY KEY(`id`)\nCOMMENT "OLAP"\nDISTRIBUTED BY HASH(`id`);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"set-up-flink-environment",children:"Set up Flink environment"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Download Flink binary ",(0,s.jsx)(n.a,{href:"https://archive.apache.org/dist/flink/flink-1.15.2/flink-1.15.2-bin-scala_2.12.tgz",children:"Flink 1.15.2"}),", and unzip it to directory ",(0,s.jsx)(n.code,{children:"flink-1.15.2"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Download ",(0,s.jsx)(n.a,{href:"https://repo1.maven.org/maven2/com/starrocks/flink-connector-starrocks/1.2.7_flink-1.15/flink-connector-starrocks-1.2.7_flink-1.15.jar",children:"Flink connector 1.2.7"}),", and put it into the directory ",(0,s.jsx)(n.code,{children:"flink-1.15.2/lib"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run the following commands to start a Flink cluster:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"cd flink-1.15.2\n./bin/start-cluster.sh\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"run-with-flink-sql",children:"Run with Flink SQL"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run the following command to start a Flink SQL client."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"./bin/sql-client.sh\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Create a Flink table ",(0,s.jsx)(n.code,{children:"score_board"}),", and insert values into the table via Flink SQL Client.\nNote you must define the primary key in the Flink DDL if you want to load data into a Primary Key table of StarRocks. It's optional for other types of StarRocks tables."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"CREATE TABLE `score_board` (\n    `id` INT,\n    `name` STRING,\n    `score` INT,\n    PRIMARY KEY (id) NOT ENFORCED\n) WITH (\n    'connector' = 'starrocks',\n    'jdbc-url' = 'jdbc:mysql://127.0.0.1:9030',\n    'load-url' = '127.0.0.1:8030',\n    'database-name' = 'test',\n    \n    'table-name' = 'score_board',\n    'username' = 'root',\n    'password' = ''\n);\n\nINSERT INTO `score_board` VALUES (1, 'starrocks', 100), (2, 'flink', 100);\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"run-with-flink-datastream",children:"Run with Flink DataStream"}),"\n",(0,s.jsxs)(n.p,{children:["There are several ways to implement a Flink DataStream job according to the type of the input records, such as a CSV Java ",(0,s.jsx)(n.code,{children:"String"}),", a JSON Java ",(0,s.jsx)(n.code,{children:"String"})," or a custom Java object."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The input records are CSV-format ",(0,s.jsx)(n.code,{children:"String"}),". See ",(0,s.jsx)(n.a,{href:"https://github.com/StarRocks/starrocks-connector-for-apache-flink/tree/main/examples/src/main/java/com/starrocks/connector/flink/examples/datastream/LoadCsvRecords.java",children:"LoadCsvRecords"})," for a complete example."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'/**\n * Generate CSV-format records. Each record has three values separated by "\\t". \n * These values will be loaded to the columns `id`, `name`, and `score` in the StarRocks table.\n */\nString[] records = new String[]{\n        "1\\tstarrocks-csv\\t100",\n        "2\\tflink-csv\\t100"\n};\nDataStream<String> source = env.fromElements(records);\n\n/**\n * Configure the connector with the required properties.\n * You also need to add properties "sink.properties.format" and "sink.properties.column_separator"\n * to tell the connector the input records are CSV-format, and the column separator is "\\t".\n * You can also use other column separators in the CSV-format records,\n * but remember to modify the "sink.properties.column_separator" correspondingly.\n */\nStarRocksSinkOptions options = StarRocksSinkOptions.builder()\n        .withProperty("jdbc-url", jdbcUrl)\n        .withProperty("load-url", loadUrl)\n        .withProperty("database-name", "test")\n        .withProperty("table-name", "score_board")\n        .withProperty("username", "root")\n        .withProperty("password", "")\n        .withProperty("sink.properties.format", "csv")\n        .withProperty("sink.properties.column_separator", "\\t")\n        .build();\n// Create the sink with the options.\nSinkFunction<String> starRockSink = StarRocksSink.sink(options);\nsource.addSink(starRockSink);\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The input records are JSON-format ",(0,s.jsx)(n.code,{children:"String"}),". See ",(0,s.jsx)(n.a,{href:"https://github.com/StarRocks/starrocks-connector-for-apache-flink/tree/main/examples/src/main/java/com/starrocks/connector/flink/examples/datastream/LoadJsonRecords.java",children:"LoadJsonRecords"})," for a complete example."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'/**\n * Generate JSON-format records. \n * Each record has three key-value pairs corresponding to the columns `id`, `name`, and `score` in the StarRocks table.\n */\nString[] records = new String[]{\n        "{\\"id\\":1, \\"name\\":\\"starrocks-json\\", \\"score\\":100}",\n        "{\\"id\\":2, \\"name\\":\\"flink-json\\", \\"score\\":100}",\n};\nDataStream<String> source = env.fromElements(records);\n\n/** \n * Configure the connector with the required properties.\n * You also need to add properties "sink.properties.format" and "sink.properties.strip_outer_array"\n * to tell the connector the input records are JSON-format and to strip the outermost array structure. \n */\nStarRocksSinkOptions options = StarRocksSinkOptions.builder()\n        .withProperty("jdbc-url", jdbcUrl)\n        .withProperty("load-url", loadUrl)\n        .withProperty("database-name", "test")\n        .withProperty("table-name", "score_board")\n        .withProperty("username", "root")\n        .withProperty("password", "")\n        .withProperty("sink.properties.format", "json")\n        .withProperty("sink.properties.strip_outer_array", "true")\n        .build();\n// Create the sink with the options.\nSinkFunction<String> starRockSink = StarRocksSink.sink(options);\nsource.addSink(starRockSink);\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The input records are custom Java objects. See ",(0,s.jsx)(n.a,{href:"https://github.com/StarRocks/starrocks-connector-for-apache-flink/tree/main/examples/src/main/java/com/starrocks/connector/flink/examples/datastream/LoadCustomJavaRecords.java",children:"LoadCustomJavaRecords"})," for a complete example."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In this example, the input record is a simple POJO ",(0,s.jsx)(n.code,{children:"RowData"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"public static class RowData {\n        public int id;\n        public String name;\n        public int score;\n\n        public RowData() {}\n\n        public RowData(int id, String name, int score) {\n            this.id = id;\n            this.name = name;\n            this.score = score;\n        }\n    }\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The main program is as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'// Generate records which use RowData as the container.\nRowData[] records = new RowData[]{\n        new RowData(1, "starrocks-rowdata", 100),\n        new RowData(2, "flink-rowdata", 100),\n    };\nDataStream<RowData> source = env.fromElements(records);\n\n// Configure the connector with the required properties.\nStarRocksSinkOptions options = StarRocksSinkOptions.builder()\n        .withProperty("jdbc-url", jdbcUrl)\n        .withProperty("load-url", loadUrl)\n        .withProperty("database-name", "test")\n        .withProperty("table-name", "score_board")\n        .withProperty("username", "root")\n        .withProperty("password", "")\n        .build();\n\n/**\n * The Flink connector will use a Java object array (Object[]) to represent a row to be loaded into the StarRocks table,\n * and each element is the value for a column.\n * You need to define the schema of the Object[] which matches that of the StarRocks table.\n */\nTableSchema schema = TableSchema.builder()\n        .field("id", DataTypes.INT().notNull())\n        .field("name", DataTypes.STRING())\n        .field("score", DataTypes.INT())\n        // When the StarRocks table is a Primary Key table, you must specify notNull(), for example, DataTypes.INT().notNull(), for the primary key `id`.\n        .primaryKey("id")\n        .build();\n// Transform the RowData to the Object[] according to the schema.\nRowDataTransformer transformer = new RowDataTransformer();\n// Create the sink with the schema, options, and transformer.\nSinkFunction<RowData> starRockSink = StarRocksSink.sink(schema, options, transformer);\nsource.addSink(starRockSink);\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"RowDataTransformer"})," in the main program is defined as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"private static class RowDataTransformer implements StarRocksSinkRowBuilder<RowData> {\n\n    /**\n     * Set each element of the object array according to the input RowData.\n     * The schema of the array matches that of the StarRocks table.\n     */\n    @Override\n    public void accept(Object[] internalRow, RowData rowData) {\n        internalRow[0] = rowData.id;\n        internalRow[1] = rowData.name;\n        internalRow[2] = rowData.score;\n        // When the StarRocks table is a Primary Key table, you need to set the last element to indicate whether the data loading is an UPSERT or DELETE operation.\n        internalRow[internalRow.length - 1] = StarRocksSinkOP.UPSERT.ordinal();\n    }\n}  \n"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best practices"}),"\n",(0,s.jsx)(n.h3,{id:"load-data-to-a-primary-key-table",children:"Load data to a Primary Key table"}),"\n",(0,s.jsxs)(n.p,{children:["This section will show how to load data to a StarRocks Primary Key table to achieve partial updates and conditional updates.\nYou can see ",(0,s.jsx)(n.a,{href:"https://docs.starrocks.io/en-us/latest/loading/Load_to_Primary_Key_tables",children:"Change data through loading"})," for the introduction of those features.\nThese examples use Flink SQL."]}),"\n",(0,s.jsx)(n.h4,{id:"preparations-1",children:"Preparations"}),"\n",(0,s.jsxs)(n.p,{children:["Create a database ",(0,s.jsx)(n.code,{children:"test"})," and create a Primary Key table ",(0,s.jsx)(n.code,{children:"score_board"})," in StarRocks."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE DATABASE `test`;\n\nCREATE TABLE `test`.`score_board`\n(\n    `id` int(11) NOT NULL COMMENT "",\n    `name` varchar(65533) NULL DEFAULT "" COMMENT "",\n    `score` int(11) NOT NULL DEFAULT "0" COMMENT ""\n)\nENGINE=OLAP\nPRIMARY KEY(`id`)\nCOMMENT "OLAP"\nDISTRIBUTED BY HASH(`id`);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"partial-update",children:"Partial update"}),"\n",(0,s.jsxs)(n.p,{children:["This example will show how to load data only to columns ",(0,s.jsx)(n.code,{children:"id"})," and ",(0,s.jsx)(n.code,{children:"name"}),"."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Insert two data rows into the StarRocks table ",(0,s.jsx)(n.code,{children:"score_board"})," in MySQL client."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"mysql> INSERT INTO `score_board` VALUES (1, 'starrocks', 100), (2, 'flink', 100);\n\nmysql> select * from score_board;\n+------+-----------+-------+\n| id   | name      | score |\n+------+-----------+-------+\n|    1 | starrocks |   100 |\n|    2 | flink     |   100 |\n+------+-----------+-------+\n2 rows in set (0.02 sec)\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Create a Flink table ",(0,s.jsx)(n.code,{children:"score_board"})," in Flink SQL client."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Define the DDL which only includes the columns ",(0,s.jsx)(n.code,{children:"id"})," and ",(0,s.jsx)(n.code,{children:"name"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Set the option ",(0,s.jsx)(n.code,{children:"sink.properties.partial_update"})," to ",(0,s.jsx)(n.code,{children:"true"})," which tells the Flink connector to perform partial updates."]}),"\n",(0,s.jsxs)(n.li,{children:["If the Flink connector version ",(0,s.jsx)(n.code,{children:"<="})," 1.2.7, you also need to set the option ",(0,s.jsx)(n.code,{children:"sink.properties.columns"})," to ",(0,s.jsx)(n.code,{children:"id,name,__op"})," to tells the Flink connector which columns need to be updated. Note that you need to append the field ",(0,s.jsx)(n.code,{children:"__op"})," at the end. The field ",(0,s.jsx)(n.code,{children:"__op"})," indicates that the data loading is an UPSERT or DELETE operation, and its values are set by the connector automatically."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"CREATE TABLE `score_board` (\n    `id` INT,\n    `name` STRING,\n    PRIMARY KEY (id) NOT ENFORCED\n) WITH (\n    'connector' = 'starrocks',\n    'jdbc-url' = 'jdbc:mysql://127.0.0.1:9030',\n    'load-url' = '127.0.0.1:8030',\n    'database-name' = 'test',\n    'table-name' = 'score_board',\n    'username' = 'root',\n    'password' = '',\n    'sink.properties.partial_update' = 'true',\n    -- only for Flink connector version <= 1.2.7\n    'sink.properties.columns' = 'id,name,__op'\n); \n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Insert two data rows into the Flink table. The primary keys of the data rows are as same as these of rows in the StarRocks table. but the values in the column ",(0,s.jsx)(n.code,{children:"name"})," are modified."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO `score_board` VALUES (1, 'starrocks-update'), (2, 'flink-update');\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Query the StarRocks table in MySQL client."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"mysql> select * from score_board;\n+------+------------------+-------+\n| id   | name             | score |\n+------+------------------+-------+\n|    1 | starrocks-update |   100 |\n|    2 | flink-update     |   100 |\n+------+------------------+-------+\n2 rows in set (0.02 sec)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["You can see that only values for ",(0,s.jsx)(n.code,{children:"name"})," change, and the values for ",(0,s.jsx)(n.code,{children:"score"})," do not change."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"conditional-update",children:"Conditional update"}),"\n",(0,s.jsxs)(n.p,{children:["This example will show how to do conditional update according to the value of column ",(0,s.jsx)(n.code,{children:"score"}),". The update for an ",(0,s.jsx)(n.code,{children:"id"}),"\ntakes effect only when the new value for ",(0,s.jsx)(n.code,{children:"score"})," is has a greater or equal to the old value."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Insert two data rows into the StarRocks table in MySQL client."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"mysql> INSERT INTO `score_board` VALUES (1, 'starrocks', 100), (2, 'flink', 100);\n\nmysql> select * from score_board;\n+------+-----------+-------+\n| id   | name      | score |\n+------+-----------+-------+\n|    1 | starrocks |   100 |\n|    2 | flink     |   100 |\n+------+-----------+-------+\n2 rows in set (0.02 sec)\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Create a Flink table ",(0,s.jsx)(n.code,{children:"score_board"})," in the following ways:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Define the DDL including all of columns."}),"\n",(0,s.jsxs)(n.li,{children:["Set the option ",(0,s.jsx)(n.code,{children:"sink.properties.merge_condition"})," to ",(0,s.jsx)(n.code,{children:"score"})," to tell the connector to use the column ",(0,s.jsx)(n.code,{children:"score"}),"\nas the condition."]}),"\n",(0,s.jsxs)(n.li,{children:["Set the option ",(0,s.jsx)(n.code,{children:"sink.version"})," to ",(0,s.jsx)(n.code,{children:"V1"})," which tells the connector to use Stream Load."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"CREATE TABLE `score_board` (\n    `id` INT,\n    `name` STRING,\n    `score` INT,\n    PRIMARY KEY (id) NOT ENFORCED\n) WITH (\n    'connector' = 'starrocks',\n    'jdbc-url' = 'jdbc:mysql://127.0.0.1:9030',\n    'load-url' = '127.0.0.1:8030',\n    'database-name' = 'test',\n    'table-name' = 'score_board',\n    'username' = 'root',\n    'password' = '',\n    'sink.properties.merge_condition' = 'score',\n    'sink.version' = 'V1'\n    );\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Insert two data rows into the Flink table. The primary keys of the data rows are as same as these of rows in the StarRocks table. The first data row has a smaller value in the column ",(0,s.jsx)(n.code,{children:"score"}),", and the second data row has a larger  value in the column ",(0,s.jsx)(n.code,{children:"score"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO `score_board` VALUES (1, 'starrocks-update', 99), (2, 'flink-update', 101);\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Query the StarRocks table in MySQL client."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"mysql> select * from score_board;\n+------+--------------+-------+\n| id   | name         | score |\n+------+--------------+-------+\n|    1 | starrocks    |   100 |\n|    2 | flink-update |   101 |\n+------+--------------+-------+\n2 rows in set (0.03 sec)\n"})}),"\n",(0,s.jsx)(n.p,{children:"You can see that only the values of the second data row change, and the values of the first data row do not change."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"load-data-into-columns-of-bitmap-type",children:"Load data into columns of BITMAP type"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://docs.starrocks.io/en-us/latest/sql-reference/sql-statements/data-types/BITMAP",children:(0,s.jsx)(n.code,{children:"BITMAP"})})," is often used to accelerate count distinct, such as counting UV, see ",(0,s.jsx)(n.a,{href:"https://docs.starrocks.io/en-us/latest/using_starrocks/Using_bitmap",children:"Use Bitmap for exact Count Distinct"}),".\nHere we take the counting of UV as an example to show how to load data into columns of the ",(0,s.jsx)(n.code,{children:"BITMAP"})," type."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Create a StarRocks Aggregate table in MySQL client."}),"\n",(0,s.jsxs)(n.p,{children:["In the database ",(0,s.jsx)(n.code,{children:"test"}),", create an Aggregate table ",(0,s.jsx)(n.code,{children:"page_uv"})," where the column ",(0,s.jsx)(n.code,{children:"visit_users"})," is defined as the ",(0,s.jsx)(n.code,{children:"BITMAP"})," type and configured with the aggregate function ",(0,s.jsx)(n.code,{children:"BITMAP_UNION"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"CREATE TABLE `test`.`page_uv` (\n  `page_id` INT NOT NULL COMMENT 'page ID',\n  `visit_date` datetime NOT NULL COMMENT 'access time',\n  `visit_users` BITMAP BITMAP_UNION NOT NULL COMMENT 'user ID'\n) ENGINE=OLAP\nAGGREGATE KEY(`page_id`, `visit_date`)\nDISTRIBUTED BY HASH(`page_id`);\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Create a Flink table in Flink SQL client."}),"\n",(0,s.jsxs)(n.p,{children:["The column ",(0,s.jsx)(n.code,{children:"visit_user_id"})," in the Flink table is of ",(0,s.jsx)(n.code,{children:"BIGINT"})," type, and we want to load this column to the column ",(0,s.jsx)(n.code,{children:"visit_users"})," of ",(0,s.jsx)(n.code,{children:"BITMAP"})," type in the StarRocks table. So when defining the DDL of the Flink table, note that:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Because Flink does not support ",(0,s.jsx)(n.code,{children:"BITMAP"}),", you need to define a column ",(0,s.jsx)(n.code,{children:"visit_user_id"})," as ",(0,s.jsx)(n.code,{children:"BIGINT"})," type to represent the column ",(0,s.jsx)(n.code,{children:"visit_users"})," of ",(0,s.jsx)(n.code,{children:"BITMAP"})," type in the StarRocks table."]}),"\n",(0,s.jsxs)(n.li,{children:["You need to set the option ",(0,s.jsx)(n.code,{children:"sink.properties.columns"})," to ",(0,s.jsx)(n.code,{children:"page_id,visit_date,user_id,visit_users=to_bitmap(visit_user_id)"}),", which tells the connector the column mapping beween the Flink table and StarRocks table. Also you need to use ",(0,s.jsx)(n.a,{href:"https://docs.starrocks.io/en-us/latest/sql-reference/sql-functions/bitmap-functions/to_bitmap",children:(0,s.jsx)(n.code,{children:"to_bitmap"})}),"\nfunction to tell the connector to convert the data of ",(0,s.jsx)(n.code,{children:"BIGINT"})," type into ",(0,s.jsx)(n.code,{children:"BITMAP"})," type."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"CREATE TABLE `page_uv` (\n    `page_id` INT,\n    `visit_date` TIMESTAMP,\n    `visit_user_id` BIGINT\n) WITH (\n    'connector' = 'starrocks',\n    'jdbc-url' = 'jdbc:mysql://127.0.0.1:9030',\n    'load-url' = '127.0.0.1:8030',\n    'database-name' = 'test',\n    'table-name' = 'page_uv',\n    'username' = 'root',\n    'password' = '',\n    'sink.properties.columns' = 'page_id,visit_date,visit_user_id,visit_users=to_bitmap(visit_user_id)'\n);\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Load data into Flink table in Flink SQL client."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO `page_uv` VALUES\n   (1, CAST('2020-06-23 01:30:30' AS TIMESTAMP), 13),\n   (1, CAST('2020-06-23 01:30:30' AS TIMESTAMP), 23),\n   (1, CAST('2020-06-23 01:30:30' AS TIMESTAMP), 33),\n   (1, CAST('2020-06-23 02:30:30' AS TIMESTAMP), 13),\n   (2, CAST('2020-06-23 01:30:30' AS TIMESTAMP), 23);\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Calculate page UVs from the StarRocks table in MySQL client."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"MySQL [test]> SELECT `page_id`, COUNT(DISTINCT `visit_users`) FROM `page_uv` GROUP BY `page_id`;\n+---------+-----------------------------+\n| page_id | count(DISTINCT visit_users) |\n+---------+-----------------------------+\n|       2 |                           1 |\n|       1 |                           3 |\n+---------+-----------------------------+\n2 rows in set (0.05 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"load-data-into-columns-of-hll-type",children:"Load data into columns of HLL type"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"/docs/2.5/sql-reference/sql-statements/data-types/HLL",children:(0,s.jsx)(n.code,{children:"HLL"})})," can be used for approximate count distinct, see ",(0,s.jsx)(n.a,{href:"/docs/2.5/using_starrocks/Using_HLL",children:"Use HLL for approximate count distinct"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["Here we take the counting of UV as an example to show how to load data into columns of the ",(0,s.jsx)(n.code,{children:"HLL"})," type."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Create a StarRocks Aggregate table"}),"\n",(0,s.jsxs)(n.p,{children:["In the database ",(0,s.jsx)(n.code,{children:"test"}),", create an Aggregate table ",(0,s.jsx)(n.code,{children:"hll_uv"})," where the column ",(0,s.jsx)(n.code,{children:"visit_users"})," is defined as the ",(0,s.jsx)(n.code,{children:"HLL"})," type and configured with the aggregate function ",(0,s.jsx)(n.code,{children:"HLL_UNION"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"CREATE TABLE `hll_uv` (\n  `page_id` INT NOT NULL COMMENT 'page ID',\n  `visit_date` datetime NOT NULL COMMENT 'access time',\n  `visit_users` HLL HLL_UNION NOT NULL COMMENT 'user ID'\n) ENGINE=OLAP\nAGGREGATE KEY(`page_id`, `visit_date`)\nDISTRIBUTED BY HASH(`page_id`);\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Create a Flink table in Flink SQL client."}),"\n",(0,s.jsxs)(n.p,{children:["The column ",(0,s.jsx)(n.code,{children:"visit_user_id"})," in the Flink table is of ",(0,s.jsx)(n.code,{children:"BIGINT"})," type, and we want to load this column to the column ",(0,s.jsx)(n.code,{children:"visit_users"})," of ",(0,s.jsx)(n.code,{children:"HLL"})," type in the StarRocks table. So when defining the DDL of the Flink table, note that:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Because Flink does not support ",(0,s.jsx)(n.code,{children:"BITMAP"}),", you need to define a column ",(0,s.jsx)(n.code,{children:"visit_user_id"})," as ",(0,s.jsx)(n.code,{children:"BIGINT"})," type to represent the column ",(0,s.jsx)(n.code,{children:"visit_users"})," of ",(0,s.jsx)(n.code,{children:"HLL"})," type in the StarRocks table."]}),"\n",(0,s.jsxs)(n.li,{children:["You need to set the option ",(0,s.jsx)(n.code,{children:"sink.properties.columns"})," to ",(0,s.jsx)(n.code,{children:"page_id,visit_date,user_id,visit_users=hll_hash(visit_user_id)"})," which tells the connector the column mapping between Flink table and StarRocks table.  Also you need to use ",(0,s.jsx)(n.a,{href:"/docs/2.5/sql-reference/sql-functions/aggregate-functions/hll_hash",children:(0,s.jsx)(n.code,{children:"hll_hash"})})," function to tell the connector to convert the data of ",(0,s.jsx)(n.code,{children:"BIGINT"})," type into ",(0,s.jsx)(n.code,{children:"HLL"})," type."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"CREATE TABLE `hll_uv` (\n    `page_id` INT,\n    `visit_date` TIMESTAMP,\n    `visit_user_id` BIGINT\n) WITH (\n    'connector' = 'starrocks',\n    'jdbc-url' = 'jdbc:mysql://127.0.0.1:9030',\n    'load-url' = '127.0.0.1:8030',\n    'database-name' = 'test',\n    'table-name' = 'hll_uv',\n    'username' = 'root',\n    'password' = '',\n    'sink.properties.columns' = 'page_id,visit_date,visit_user_id,visit_users=hll_hash(visit_user_id)'\n);\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Load data into Flink table in Flink SQL client."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO `hll_uv` VALUES\n   (3, CAST('2023-07-24 12:00:00' AS TIMESTAMP), 78),\n   (4, CAST('2023-07-24 13:20:10' AS TIMESTAMP), 2),\n   (3, CAST('2023-07-24 12:30:00' AS TIMESTAMP), 674);\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Calculate page UVs from the StarRocks table in MySQL client."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"mysql> SELECT `page_id`, COUNT(DISTINCT `visit_users`) FROM `hll_uv` GROUP BY `page_id`;\n**+---------+-----------------------------+\n| page_id | count(DISTINCT visit_users) |\n+---------+-----------------------------+\n|       3 |                           2 |\n|       4 |                           1 |\n+---------+-----------------------------+\n2 rows in set (0.04 sec)\n"})}),"\n"]}),"\n"]})]})}const h=function(e={}){const{wrapper:n}=Object.assign({},(0,r.ah)(),e.components);return n?(0,s.jsx)(n,Object.assign({},e,{children:(0,s.jsx)(d,e)})):d(e)}},11151:(e,n,t)=>{t.d(n,{Zo:()=>o,ah:()=>i});var s=t(67294);const r=s.createContext({});function i(e){const n=s.useContext(r);return s.useMemo((()=>"function"==typeof e?e(n):{...n,...e}),[n,e])}const a={};function o({components:e,children:n,disableParentContext:t}){let o;return o=t?"function"==typeof e?e({}):e||a:i(e),s.createElement(r.Provider,{value:o},n)}}}]);