"use strict";(self.webpackChunkstarrocks=self.webpackChunkstarrocks||[]).push([[28381],{14166:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var a=t(85893),i=t(11151);const o={displayed_sidebar:"English"},s="ETL When Loading",l={id:"loading/Etl_in_loading",title:"ETL When Loading",description:"When importing data into StarRocks tables, sometimes the content of the target table is not exactly the same as the content of the data source. For example:",source:"@site/versioned_docs/version-2.0/loading/Etl_in_loading.md",sourceDirName:"loading",slug:"/loading/Etl_in_loading",permalink:"/docs/2.0/loading/Etl_in_loading",draft:!1,unlisted:!1,editUrl:"https://github.com/StarRocks/starrocks/edit/main/docs/loading/Etl_in_loading.md",tags:[],version:"2.0",frontMatter:{displayed_sidebar:"English"},sidebar:"English",previous:{title:"Introduction",permalink:"/docs/2.0/loading/DataX-starrocks-writer"},next:{title:"Load data by using flink-connector-starrocks",permalink:"/docs/2.0/loading/Flink-connector-starrocks"}},r={},d=[{value:"Select the Columns to be Imported",id:"select-the-columns-to-be-imported",level:2},{value:"Sample Data",id:"sample-data",level:3},{value:"Local File Import",id:"local-file-import",level:3},{value:"HDFS Import",id:"hdfs-import",level:3},{value:"Kafka Import",id:"kafka-import",level:3},{value:"Query",id:"query",level:3},{value:"Skip Rows That Do Not Need to be Imported",id:"skip-rows-that-do-not-need-to-be-imported",level:2},{value:"Sample Data",id:"sample-data-1",level:3},{value:"Local File Import",id:"local-file-import-1",level:3},{value:"HDFS Import",id:"hdfs-import-1",level:3},{value:"Kafka Import",id:"kafka-import-1",level:3},{value:"Query",id:"query-1",level:3},{value:"Generating Derived Columns",id:"generating-derived-columns",level:2},{value:"Local File Import",id:"local-file-import-2",level:3},{value:"HDFS Import",id:"hdfs-import-2",level:3},{value:"Kafka Import",id:"kafka-import-2",level:3},{value:"Query",id:"query-2",level:3},{value:"Get the Column Content from the File Path",id:"get-the-column-content-from-the-file-path",level:2},{value:"Sample Data",id:"sample-data-2",level:3},{value:"HDFS Import",id:"hdfs-import-3",level:3},{value:"Query",id:"query-3",level:3}];function c(e){const n=Object.assign({h1:"h1",p:"p",strong:"strong",ul:"ul",li:"li",ol:"ol",hr:"hr",h2:"h2",h3:"h3",pre:"pre",code:"code",em:"em"},(0,i.ah)(),e.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"etl-when-loading",children:"ETL When Loading"}),"\n",(0,a.jsxs)(n.p,{children:["When importing data into StarRocks tables, sometimes the content of the target table is ",(0,a.jsx)(n.strong,{children:"not"})," exactly the same as the content of the data source. For example:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Scenario 1: The data source contains some content that is not needed, for example ",(0,a.jsx)(n.strong,{children:"redundant rows"})," or ",(0,a.jsx)(n.strong,{children:"redundant columns"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Scenario 2: The content in the data source is not directly imported into StarRocks, and may ",(0,a.jsx)(n.strong,{children:"require some transformation"})," work done before or during the imports. For example, the data in the original file is in timestamp format, whereas the data type of the target table is Datetime. In this case, the type conversion needs to be completed during the data import."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"StarRocks can perform data transformation while executing data import. This way, in case of inconsistency between the content of the data source and the target table, users can complete the data transformation directly without external ETL work."}),"\n",(0,a.jsx)(n.p,{children:"With the capabilities provided by StarRocks, users can achieve the following during data import."}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Select the columns that need to be imported."})," On one hand, this function allows you to skip the columns that do not need to be imported; on the other hand, when the order of the columns in the table does not match the order of the fields in the file, you can use this function to establish a field mapping between the two tables."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Filter unwanted rows."})," It is possible to skip rows that do not need to be imported by specifying expressions during import."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Derived columns"})," (i.e., new columns generated by computational processing) can be generated and imported into the StarRocks target table."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Support Hive partition path naming."})," StarRocks can get the content of partition columns from the file path."]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"select-the-columns-to-be-imported",children:"Select the Columns to be Imported"}),"\n",(0,a.jsx)(n.h3,{id:"sample-data",children:"Sample Data"}),"\n",(0,a.jsx)(n.p,{children:"Suppose you need to import a piece of data into the following table:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE event (\n    `event_date` DATE,\n    `event_type` TINYINT,\n    `user_id` BIGINT\n)\nDISTRIBUTED BY HASH(user_id) BUCKETS 3;\n"})}),"\n",(0,a.jsxs)(n.p,{children:["However, the data file contains ",(0,a.jsx)(n.code,{children:"user_id, user_gender, event_date, event_type"})," and the sample data is shown below."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"354,female,2020-05-20,1\n465,male,2020-05-21,2\n576,female,2020-05-22,1\n687,male,2020-05-23,2\n"})}),"\n",(0,a.jsx)(n.h3,{id:"local-file-import",children:"Local File Import"}),"\n",(0,a.jsx)(n.p,{children:"The following command enables importing  local data into the corresponding table."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl --location-trusted -u root -H "column_separator:," \\\n    -H "columns: user_id, user_gender, event_date, event_type" -T load-columns.txt \\\n    http://{FE_HOST}:{FE_HTTP_PORT}/api/test/event/_stream_load\n'})}),"\n",(0,a.jsxs)(n.p,{children:["The columns in CSV format files are originally unnamed. By setting ",(0,a.jsx)(n.code,{children:"columns"})," you can name them in order (in some CSVs, the column names are given in the first line, but in fact the system is not aware of this and will treat it as normal data). In this case, the ",(0,a.jsx)(n.code,{children:"columns"})," parameter describes the column names ",(0,a.jsx)(n.strong,{children:"in order"}),", that is",(0,a.jsx)(n.code,{children:"user_id, user_gender, event_date, event_type"}),". The data will be imported accordingly."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Columns with the same name as those in the imported table are imported directly"}),"\n",(0,a.jsx)(n.li,{children:"Columns that do not exist in the imported table will be ignored during the import"}),"\n",(0,a.jsxs)(n.li,{children:["Columns that exist in the import table but are not specified in ",(0,a.jsx)(n.code,{children:"columns"})," are reported as errors"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["For this example, ",(0,a.jsx)(n.code,{children:"user_id, event_date, event_type"})," can be found in the table, so the corresponding content will be imported into the StarRocks table. ",(0,a.jsx)(n.code,{children:"user_gender"})," does not exist in the table, so it will be ignored during the import."]}),"\n",(0,a.jsx)(n.h3,{id:"hdfs-import",children:"HDFS Import"}),"\n",(0,a.jsx)(n.p,{children:"HDFS data can be imported into the corresponding table by using the following command:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL test.label_load (\n    DATA INFILE("hdfs://{HDFS_HOST}:{HDFS_PORT}/tmp/zc/starrocks/data/date=*/*")\n    INTO TABLE `event`\n    COLUMNS TERMINATED BY ","\n    FORMAT AS "csv"\n    (user_id, user_gender, event_date, event_type)\n)\nWITH BROKER hdfs;\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Columns can be specified by ",(0,a.jsx)(n.code,{children:"user_id, user_gender, event_date, event_type"}),". The process of StarRocks import is the same as the local file import. Required columns will be imported into StarRocks and nonrequired columns will be ignored."]}),"\n",(0,a.jsx)(n.h3,{id:"kafka-import",children:"Kafka Import"}),"\n",(0,a.jsx)(n.p,{children:"The following command enables importing data from Kafka:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD test.event_load ON event\n    COLUMNS TERMINATED BY ",",\n    COLUMNS(user_id, user_gender, event_date, event_type),\nWHERE event_type = 1\nFROM KAFKA (\n    "kafka_broker_list" = "{KAFKA_BROKER_HOST}:{KAFKA_BROKER_PORT}",\n    "kafka_topic" = "event"\n);\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"COLUMNS(user_id, user_gender, event_date, event_type)"})," can be used to indicate the fields in the Kafka stream message. The process of StarRocks import is the same as the local file import. Required columns will be imported into StarRocks and nonrequired columns will be ignored."]}),"\n",(0,a.jsx)(n.h3,{id:"query",children:"Query"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-SQL",children:"> select * from event;\n+------------+------------+---------+\n| event_date | event_type | user_id |\n+------------+------------+---------+\n| 2020-05-22 |          1 |     576 |\n| 2020-05-20 |          1 |     354 |\n| 2020-05-21 |          2 |     465 |\n| 2020-05-23 |          2 |     687 |\n+------------+------------+---------+\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"skip-rows-that-do-not-need-to-be-imported",children:"Skip Rows That Do Not Need to be Imported"}),"\n",(0,a.jsx)(n.h3,{id:"sample-data-1",children:"Sample Data"}),"\n",(0,a.jsx)(n.p,{children:"Suppose you need to import a data duplicate into the following table."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE event (\n    `event_date` DATE,\n    `event_type` TINYINT,\n    `user_id` BIGINT\n)\nDISTRIBUTED BY HASH(user_id) BUCKETS 3;\n"})}),"\n",(0,a.jsx)(n.p,{children:"Assuming that the data file contains three columns, the sample data is shown below:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"2020-05-20,1,354\n2020-05-21,2,465\n2020-05-22,1,576\n2020-05-23,2,687\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Noted that only the data with ",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"event_type"})})," of 1 needs to be analyzed in the destination table, then:"]}),"\n",(0,a.jsx)(n.h3,{id:"local-file-import-1",children:"Local File Import"}),"\n",(0,a.jsxs)(n.p,{children:["When importing local files, data can be filtered by specifying the Header ",(0,a.jsx)(n.code,{children:"where:event_type=1"})," in the HTTP request."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl --location-trusted -u root -H "column_separator:," \\\n    -H "where:event_type=1" -T load-rows.txt \\\n    http://{FE_HOST}:{FE_HTTP_PORT}/test/event/_stream_load\n'})}),"\n",(0,a.jsx)(n.h3,{id:"hdfs-import-1",children:"HDFS Import"}),"\n",(0,a.jsxs)(n.p,{children:["With the following command, data can be imported with ",(0,a.jsx)(n.code,{children:"event_type=1"}),' by the "WHERE event_type = 1" condition.']}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL test.label_load (\n    DATA INFILE("hdfs://{HDFS_HOST}:{HDFS_PORT}/tmp/zc/starrocks/data/date=*/*")\n    INTO TABLE `event`\n    COLUMNS TERMINATED BY ","\n    FORMAT AS "csv"\n    WHERE event_type = 1\n)\nWITH BROKER hdfs;\n'})}),"\n",(0,a.jsx)(n.h3,{id:"kafka-import-1",children:"Kafka Import"}),"\n",(0,a.jsxs)(n.p,{children:["With the following command, data can be imported with ",(0,a.jsx)(n.code,{children:"event_type=1"}),' by the "WHERE event_type = 1" condition.']}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD test.event_load ON event\nCOLUMNS TERMINATED BY ",",\nWHERE event_type = 1\nFROM KAFKA (\n    "kafka_broker_list" = "{KAFKA_BROKER_HOST}:{KAFKA_BROKER_PORT}",\n    "kafka_topic" = "event"\n);\n'})}),"\n",(0,a.jsx)(n.h3,{id:"query-1",children:"Query"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-SQL",children:"> select * from event;\n+------------+------------+---------+\n| event_date | event_type | user_id |\n+------------+------------+---------+\n| 2020-05-20 |          1 |     354 |\n| 2020-05-22 |          1 |     576 |\n+------------+------------+---------+\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"generating-derived-columns",children:"Generating Derived Columns"}),"\n",(0,a.jsx)(n.p,{children:"Suppose you need to import a data duplicate into the following table."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE dim_date (\n    `date` DATE,\n    `year` INT,\n    `month` TINYINT,\n    `day` TINYINT\n)\nDISTRIBUTED BY HASH(date) BUCKETS 1;\n"})}),"\n",(0,a.jsx)(n.p,{children:"However, the original data file contains only one column:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"2020-05-20\n2020-05-21\n2020-05-22\n2020-05-23\n"})}),"\n",(0,a.jsx)(n.p,{children:"When importing, the data is transformed by the following command."}),"\n",(0,a.jsx)(n.h3,{id:"local-file-import-2",children:"Local File Import"}),"\n",(0,a.jsxs)(n.p,{children:["With the following command, StarRocks can generate the corresponding derived columns while importing local files with ",(0,a.jsx)(n.code,{children:'Header "columns:date, year=year(date), month=month(date), day=day(date)"'})," in the HTTP request."]}),"\n",(0,a.jsx)(n.p,{children:"This allows StarRocks to calculate and generate the corresponding columns based on the file content being imported."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl --location-trusted -u root -H "column_separator:," \\\n    -H "columns:date,year=year(date),month=month(date),day=day(date)" -T load-date.txt \\\n    http://127.0.0.1:8431/api/test/dim_date/_stream_load\n'})}),"\n",(0,a.jsx)(n.p,{children:"Note:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"First, you need to list all the columns in the CSV format file followed by the derived columns;"}),"\n",(0,a.jsx)(n.li,{}),"\n",(0,a.jsxs)(n.li,{children:["Don\u2019t use ",(0,a.jsx)(n.code,{children:"col_name = func(col_name)"}),". Rename the column name, e.g. ",(0,a.jsx)(n.code,{children:"col_name = func(col_name0)"}),"."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"hdfs-import-2",children:"HDFS Import"}),"\n",(0,a.jsx)(n.p,{children:"Similar to the aforementioned local file import, HDFS file import is possible with the following command."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL test.label_load (\n    DATA INFILE("hdfs://{HDFS_HOST}:{HDFS_PORT}/tmp/zc/starrocks/data/date=*/*")\n    INTO TABLE `event`\n    COLUMNS TERMINATED BY ","\n    FORMAT AS "csv"\n    (date)\n    SET(year=year(date), month=month(date), day=day(date))\n)\nWITH BROKER hdfs;\n'})}),"\n",(0,a.jsx)(n.h3,{id:"kafka-import-2",children:"Kafka Import"}),"\n",(0,a.jsx)(n.p,{children:"Similarly, importing the corresponding data from Kafka can be achieved with the following command."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD test.event_load ON event\n    COLUMNS TERMINATED BY ",",\n    COLUMNS(date,year=year(date),month=month(date),day=day(date))\nFROM KAFKA (\n    "kafka_broker_list" = "{KAFKA_BROKER_HOST}:{KAFKA_BROKER_PORT}",\n    "kafka_topic" = "event"\n);\n'})}),"\n",(0,a.jsx)(n.h3,{id:"query-2",children:"Query"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-SQL",children:"> SELECT * FROM dim_date;\n+------------+------+-------+------+\n| date       | year | month | day  |\n+------------+------+-------+------+\n| 2020-05-20 | 2020 |  5    | 20   |\n| 2020-05-21 | 2020 |  5    | 21   |\n| 2020-05-22 | 2020 |  5    | 22   |\n| 2020-05-23 | 2020 |  5    | 23   |\n+------------+------+-------+------+\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"get-the-column-content-from-the-file-path",children:"Get the Column Content from the File Path"}),"\n",(0,a.jsx)(n.h3,{id:"sample-data-2",children:"Sample Data"}),"\n",(0,a.jsx)(n.p,{children:"Suppose we want to import data into the following table."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE event (\n    `event_date` DATE,\n    `event_type` TINYINT,\n    `user_id` BIGINT\n)\nDISTRIBUTED BY HASH(user_id) BUCKETS 3;\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The data to be imported is the data generated by Hive, which is partitioned by ",(0,a.jsx)(n.code,{children:"event_date"}),". Each file contains only two columns \u2013 ",(0,a.jsx)(n.code,{children:"event_type"})," and ",(0,a.jsx)(n.code,{children:"user_id"}),". The specific data content is shown below."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"/tmp/starrocks/data/date=2020-05-20/data\n1,354\n/tmp/starrocks/data/date=2020-05-21/data\n2,465\n/tmp/starrocks/data/date=2020-05-22/data\n1,576\n/tmp/starrocks/data/date=2020-05-23/data\n2,687\n"})}),"\n",(0,a.jsxs)(n.p,{children:['The following command imports the data into the "event" table and gets "',(0,a.jsx)(n.strong,{children:"event_date"}),'" from the file path.']}),"\n",(0,a.jsx)(n.h3,{id:"hdfs-import-3",children:"HDFS Import"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test.label_load (\n    DATA INFILE("hdfs://{HDFS_HOST}:{HDFS_PORT}/tmp/starrocks/data/date=*/*")\n    INTO TABLE `event`\n    COLUMNS TERMINATED BY ","\n    FORMAT AS "csv"\n    (event_type, user_id)\n    COLUMNS FROM PATH AS (date)\n    SET(event_date = date)\n)\nWITH BROKER hdfs;\n'})}),"\n",(0,a.jsxs)(n.p,{children:['The above command imports all files matching the path wildcard into the "event" table. The files are in CSV format, and the columns are split by ',(0,a.jsx)(n.code,{children:","}),'. The file contains two columns \u2013 "event_type" and "user_id". We can ',(0,a.jsx)(n.strong,{children:'get the  "date" column from the file path'}),' because the corresponding name of the date column in the table is "',(0,a.jsx)(n.strong,{children:"event_date"}),'". So the mapping is done by the ',(0,a.jsx)(n.code,{children:"SET"})," statement."]}),"\n",(0,a.jsx)(n.h3,{id:"query-3",children:"Query"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-SQL",children:"> select * from event;\n+------------+------------+---------+\n| event_date | event_type | user_id |\n+------------+------------+---------+\n| 2020-05-22 |          1 |     576 |\n| 2020-05-20 |          1 |     354 |\n| 2020-05-21 |          2 |     465 |\n| 2020-05-23 |          2 |     687 |\n+------------+------------+---------+\n"})})]})}const h=function(e={}){const{wrapper:n}=Object.assign({},(0,i.ah)(),e.components);return n?(0,a.jsx)(n,Object.assign({},e,{children:(0,a.jsx)(c,e)})):c(e)}},11151:(e,n,t)=>{t.d(n,{Zo:()=>l,ah:()=>o});var a=t(67294);const i=a.createContext({});function o(e){const n=a.useContext(i);return a.useMemo((()=>"function"==typeof e?e(n):{...n,...e}),[n,e])}const s={};function l({components:e,children:n,disableParentContext:t}){let l;return l=t?"function"==typeof e?e({}):e||s:o(e),a.createElement(i.Provider,{value:l},n)}}}]);