"use strict";(self.webpackChunkstarrocks=self.webpackChunkstarrocks||[]).push([[13157],{31856:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>d,toc:()=>i});var s=r(85893),t=r(11151);const a={},o="Read data from StarRocks using Spark connector",d={id:"unloading/Spark_connector",title:"Read data from StarRocks using Spark connector",description:"StarRocks provides a self-developed connector named StarRocks Connector for Apache Spark\u2122 (Spark connector for short) to help you read data from a StarRocks table by using Spark. You can use Spark for complex processing and machine learning on the data you have read from StarRocks.",source:"@site/versioned_docs/version-3.0/unloading/Spark_connector.md",sourceDirName:"unloading",slug:"/unloading/Spark_connector",permalink:"/docs/3.0/unloading/Spark_connector",draft:!1,unlisted:!1,editUrl:"https://github.com/StarRocks/starrocks/tree/main/versioned_docs/version-3.0/unloading/Spark_connector.md",tags:[],version:"3.0",frontMatter:{},sidebar:"documentation",previous:{title:"Read data from StarRocks using Flink connector",permalink:"/docs/3.0/unloading/Flink_connector"},next:{title:"Bitmap indexing",permalink:"/docs/3.0/using_starrocks/Bitmap_index"}},c={},i=[{value:"Usage notes",id:"usage-notes",level:2},{value:"Version requirements",id:"version-requirements",level:2},{value:"Obtain Spark connector",id:"obtain-spark-connector",level:2},{value:"Spark connector 1.1.0 and later",id:"spark-connector-110-and-later",level:3},{value:"Download a compiled package",id:"download-a-compiled-package",level:4},{value:"Add Maven dependencies",id:"add-maven-dependencies",level:4},{value:"Manually compile a package",id:"manually-compile-a-package",level:4},{value:"Spark connector 1.0.0",id:"spark-connector-100",level:3},{value:"Download a compiled package",id:"download-a-compiled-package-1",level:4},{value:"Manually compile a package",id:"manually-compile-a-package-1",level:4},{value:"Parameters",id:"parameters",level:2},{value:"Common parameters",id:"common-parameters",level:3},{value:"Parameters for Spark SQL and Spark DataFrame",id:"parameters-for-spark-sql-and-spark-dataframe",level:3},{value:"Parameters for Spark RDD",id:"parameters-for-spark-rdd",level:3},{value:"Data type mapping between StarRocks and Spark",id:"data-type-mapping-between-starrocks-and-spark",level:2},{value:"Spark connector 1.1.0 and later",id:"spark-connector-110-and-later-1",level:3},{value:"Spark connector 1.0.0",id:"spark-connector-100-1",level:3},{value:"Upgrade Spark connector",id:"upgrade-spark-connector",level:2},{value:"Upgrade from version 1.0.0 to version 1.1.0",id:"upgrade-from-version-100-to-version-110",level:3},{value:"Examples",id:"examples",level:2},{value:"Data example",id:"data-example",level:3},{value:"Read data using Spark SQL",id:"read-data-using-spark-sql",level:3},{value:"Read data using Spark DataFrame",id:"read-data-using-spark-dataframe",level:3},{value:"Read data using Spark RDD",id:"read-data-using-spark-rdd",level:3},{value:"Best practices",id:"best-practices",level:2},{value:"Environment setup",id:"environment-setup",level:3},{value:"Data example",id:"data-example-1",level:3},{value:"Full table scan",id:"full-table-scan",level:3},{value:"Partition pruning",id:"partition-pruning",level:3},{value:"Bucket pruning",id:"bucket-pruning",level:3},{value:"Partition pruning and bucket pruning",id:"partition-pruning-and-bucket-pruning",level:3},{value:"Prefix index filtering",id:"prefix-index-filtering",level:3}];function l(e){const n=Object.assign({h1:"h1",p:"p",blockquote:"blockquote",strong:"strong",a:"a",h2:"h2",ul:"ul",li:"li",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",h3:"h3",code:"code",h4:"h4",pre:"pre",ol:"ol"},(0,t.ah)(),e.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"read-data-from-starrocks-using-spark-connector",children:"Read data from StarRocks using Spark connector"}),"\n",(0,s.jsx)(n.p,{children:"StarRocks provides a self-developed connector named StarRocks Connector for Apache Spark\u2122 (Spark connector for short) to help you read data from a StarRocks table by using Spark. You can use Spark for complex processing and machine learning on the data you have read from StarRocks."}),"\n",(0,s.jsx)(n.p,{children:"The Spark connector supports three reading methods: Spark SQL, Spark DataFrame, and Spark RDD."}),"\n",(0,s.jsx)(n.p,{children:"You can use Spark SQL to create a temporary view on the StarRocks table, and then directly read data from the StarRocks table by using that temporary view."}),"\n",(0,s.jsx)(n.p,{children:"You can also map the StarRocks table to a Spark DataFrame or a Spark RDD, and then read data from the Spark DataFrame or Spark RDD. We recommend the use of a Spark DataFrame."}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,s.jsxs)(n.p,{children:["Only users with the SELECT privilege on a StarRocks table can read data from this table. You can follow the instructions provided in ",(0,s.jsx)(n.a,{href:"/docs/3.0/sql-reference/sql-statements/account-management/GRANT",children:"GRANT"})," to grant the privilege to a user."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"usage-notes",children:"Usage notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"You can filter data on StarRocks before you read the data, thereby reducing the amount of data transferred."}),"\n",(0,s.jsx)(n.li,{children:"If the overhead of reading data is substantial, you can employ appropriate table design and filter conditions to prevent Spark from reading an excessive amount of data at a time. As such, you can reduce I/O pressure on your disk and network connection, thereby ensuring routine queries can be run properly."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"version-requirements",children:"Version requirements"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Spark connector"}),(0,s.jsx)(n.th,{children:"Spark"}),(0,s.jsx)(n.th,{children:"StarRocks"}),(0,s.jsx)(n.th,{children:"Java"}),(0,s.jsx)(n.th,{children:"Scala"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1.1.1"}),(0,s.jsx)(n.td,{children:"3.2, 3.3, 3.4"}),(0,s.jsx)(n.td,{children:"2.5 and later"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"2.12"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1.1.0"}),(0,s.jsx)(n.td,{children:"3.2, 3.3, 3.4"}),(0,s.jsx)(n.td,{children:"2.5 and later"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"2.12"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1.0.0"}),(0,s.jsx)(n.td,{children:"3.x"}),(0,s.jsx)(n.td,{children:"1.18 and later"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"2.12"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1.0.0"}),(0,s.jsx)(n.td,{children:"2.x"}),(0,s.jsx)(n.td,{children:"1.18 and later"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"2.11"})]})]})]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Please see ",(0,s.jsx)(n.a,{href:"#upgrade-spark-connector",children:"Upgrade Spark connector"})," for behaviour changes among different connector versions."]}),"\n",(0,s.jsxs)(n.li,{children:["The connector does not provide MySQL JDBC driver since version 1.1.1, and you need import the driver to the Spark classpath manually. You can find the driver on ",(0,s.jsx)(n.a,{href:"https://repo1.maven.org/maven2/mysql/mysql-connector-java/",children:"Maven Central"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"In version 1.0.0, the Spark connector only supports reading data from StarRocks. From version 1.1.0 onwards, the Spark connector supports both reading data from and writing data to StarRocks."}),"\n",(0,s.jsxs)(n.li,{children:["Version 1.0.0 differs from version 1.1.0 in terms of parameters and data type mappings. See ",(0,s.jsx)(n.a,{href:"#upgrade-spark-connector",children:"Upgrade Spark connector"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"In general cases, no new features will be added to version 1.0.0. We recommend that you upgrade your Spark connector at your earliest opportunity."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"obtain-spark-connector",children:"Obtain Spark connector"}),"\n",(0,s.jsxs)(n.p,{children:["Use one of the following methods to obtain the Spark connector ",(0,s.jsx)(n.strong,{children:".jar"})," package that suits your business needs:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Download a compiled package."}),"\n",(0,s.jsx)(n.li,{children:"Use Maven to add the dependencies required by the Spark connector. (This method is supported only for Spark connector 1.1.0 and later.)"}),"\n",(0,s.jsx)(n.li,{children:"Manually compile a package."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"spark-connector-110-and-later",children:"Spark connector 1.1.0 and later"}),"\n",(0,s.jsxs)(n.p,{children:["Spark connector ",(0,s.jsx)(n.strong,{children:".jar"})," packages are named in the following format:"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.code,{children:"starrocks-spark-connector-${spark_version}_${scala_version}-${connector_version}.jar"})}),"\n",(0,s.jsxs)(n.p,{children:["For example, if you want to use Spark connector 1.1.0 with Spark 3.2 and Scala 2.12, you can choose ",(0,s.jsx)(n.code,{children:"starrocks-spark-connector-3.2_2.12-1.1.0.jar"}),"."]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,s.jsx)(n.p,{children:"In normal cases, the latest Spark connector version can be used with the most recent three Spark versions."}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"download-a-compiled-package",children:"Download a compiled package"}),"\n",(0,s.jsxs)(n.p,{children:["You can obtain Spark connector ",(0,s.jsx)(n.strong,{children:".jar"})," packages of various versions at ",(0,s.jsx)(n.a,{href:"https://repo1.maven.org/maven2/com/starrocks",children:"Maven Central Repository"}),"."]}),"\n",(0,s.jsx)(n.h4,{id:"add-maven-dependencies",children:"Add Maven dependencies"}),"\n",(0,s.jsx)(n.p,{children:"Configure the dependencies required by the Spark connector as follows:"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,s.jsxs)(n.p,{children:["You must replace ",(0,s.jsx)(n.code,{children:"spark_version"}),", ",(0,s.jsx)(n.code,{children:"scala_version"}),", and ",(0,s.jsx)(n.code,{children:"connector_version"})," with the Spark version, Scala version, and Spark connector version you use."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"<dependency>\n  <groupId>com.starrocks</groupId>\n  <artifactId>starrocks-spark-connector-${spark_version}_${scala_version}</artifactId>\n  <version>${connector_version}</version>\n</dependency>\n"})}),"\n",(0,s.jsx)(n.p,{children:"For example, if you want to use Spark connector 1.1.0 with Spark 3.2 and Scala 2.12, configure the dependencies as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"<dependency>\n  <groupId>com.starrocks</groupId>\n  <artifactId>starrocks-spark-connector-3.2_2.12</artifactId>\n  <version>1.1.0</version>\n</dependency>\n"})}),"\n",(0,s.jsx)(n.h4,{id:"manually-compile-a-package",children:"Manually compile a package"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Download ",(0,s.jsx)(n.a,{href:"https://github.com/StarRocks/starrocks-connector-for-apache-spark",children:"the Spark connector code"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Use the following command to compile the Spark connector:"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,s.jsxs)(n.p,{children:["You must replace ",(0,s.jsx)(n.code,{children:"spark_version"})," with the Spark version you use."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"sh build.sh <spark_version>\n"})}),"\n",(0,s.jsx)(n.p,{children:"For example, if you want to use the Spark connector with Spark 3.2, compile the Spark connector as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"sh build.sh 3.2\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Go to the ",(0,s.jsx)(n.code,{children:"target/"})," path, in which a Spark connector ",(0,s.jsx)(n.strong,{children:".jar"})," package like ",(0,s.jsx)(n.code,{children:"starrocks-spark-connector-3.2_2.12-1.1.0-SNAPSHOT.jar"})," is generated upon compilation."]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,s.jsxs)(n.p,{children:["If you are using a Spark connector version that is not officially released, the name of the generated Spark connector ",(0,s.jsx)(n.strong,{children:".jar"})," package contains ",(0,s.jsx)(n.code,{children:"SNAPSHOT"})," as a suffix."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"spark-connector-100",children:"Spark connector 1.0.0"}),"\n",(0,s.jsx)(n.h4,{id:"download-a-compiled-package-1",children:"Download a compiled package"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://cdn-thirdparty.starrocks.com/spark/starrocks-spark2_2.11-1.0.0.jar",children:"Spark 2.x"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://cdn-thirdparty.starrocks.com/spark/starrocks-spark3_2.12-1.0.0.jar",children:"Spark 3.x"})}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"manually-compile-a-package-1",children:"Manually compile a package"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Download ",(0,s.jsx)(n.a,{href:"https://github.com/StarRocks/starrocks-connector-for-apache-spark/tree/spark-1.0",children:"the Spark connector code"}),"."]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,s.jsxs)(n.p,{children:["You must switch to ",(0,s.jsx)(n.code,{children:"spark-1.0"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Take one of the following actions to compile the Spark connector:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"If you are using Spark 2.x, run the following command, which compiles the Spark connector to suit Spark 2.3.4 by default:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain",children:"sh build.sh 2\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"If you are using Spark 3.x, run the following command, which compiles the Spark connector to suit Spark 3.1.2 by default:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain",children:"sh build.sh 3\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Go to the ",(0,s.jsx)(n.code,{children:"output/"})," path, in which the ",(0,s.jsx)(n.code,{children:"starrocks-spark2_2.11-1.0.0.jar"})," file is generated upon compilation. Then, copy the file to the classpath of Spark:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["If your Spark cluster runs in ",(0,s.jsx)(n.code,{children:"Local"})," mode, place the file into the ",(0,s.jsx)(n.code,{children:"jars/"})," path."]}),"\n",(0,s.jsxs)(n.li,{children:["If your Spark cluster runs in ",(0,s.jsx)(n.code,{children:"Yarn"})," mode, place the file into the pre-deployment package."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"You can use the Spark connector to read data from StarRocks only after you place the file into the specified location."}),"\n",(0,s.jsx)(n.h2,{id:"parameters",children:"Parameters"}),"\n",(0,s.jsx)(n.p,{children:"This section describes the parameters you need to configure when you use the Spark connector to read data from StarRocks."}),"\n",(0,s.jsx)(n.h3,{id:"common-parameters",children:"Common parameters"}),"\n",(0,s.jsx)(n.p,{children:"The following parameters apply to all three reading methods: Spark SQL, Spark DataFrame, and Spark RDD."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Default value"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.fenodes"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsxs)(n.td,{children:["The HTTP URL of the FE in your StarRocks cluster. Format ",(0,s.jsx)(n.code,{children:"<fe_host>:<fe_http_port>"}),". You can specify multiple URLs, which must be separated by a comma (,)."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.table.identifier"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsxs)(n.td,{children:["The name of the StarRocks table. Format: ",(0,s.jsx)(n.code,{children:"<database_name>.<table_name>"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.request.retries"}),(0,s.jsx)(n.td,{children:"3"}),(0,s.jsx)(n.td,{children:"The maximum number of times that Spark can retry to send a read request o StarRocks."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.request.connect.timeout.ms"}),(0,s.jsx)(n.td,{children:"30000"}),(0,s.jsx)(n.td,{children:"The maximum amount of time after which a read request sent to StarRocks times out."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.request.read.timeout.ms"}),(0,s.jsx)(n.td,{children:"30000"}),(0,s.jsx)(n.td,{children:"The maximum amount of time after which the reading for a request sent to StarRocks times out."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.request.query.timeout.s"}),(0,s.jsx)(n.td,{children:"3600"}),(0,s.jsxs)(n.td,{children:["The maximum amount of time after which a query of data from StarRocks times out. The default timeout period is 1 hour. ",(0,s.jsx)(n.code,{children:"-1"})," means that no timeout period is specified."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.request.tablet.size"}),(0,s.jsx)(n.td,{children:"Integer.MAX_VALUE"}),(0,s.jsx)(n.td,{children:"The number of StarRocks tablets grouped into each Spark RDD partition. A smaller value of this parameter indicates that a larger number of Spark RDD partitions will be generated. A larger number of Spark RDD partitions means higher parallelism on Spark but greater pressure on StarRocks."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.batch.size"}),(0,s.jsx)(n.td,{children:"4096"}),(0,s.jsx)(n.td,{children:"The maximum number of rows that can be read from BEs at a time. Increasing the value of this parameter can reduce the number of connections established between Spark and StarRocks, thereby mitigating extra time overheads caused by network latency."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.exec.mem.limit"}),(0,s.jsx)(n.td,{children:"2147483648"}),(0,s.jsx)(n.td,{children:"The maximum amount of memory allowed per query. Unit: bytes. The default memory limit is 2 GB."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.deserialize.arrow.async"}),(0,s.jsx)(n.td,{children:"false"}),(0,s.jsx)(n.td,{children:"Specifies whether to support asynchronously converting the Arrow memory format to RowBatches required for the iteration of the Spark connector."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.deserialize.queue.size"}),(0,s.jsx)(n.td,{children:"64"}),(0,s.jsxs)(n.td,{children:["The size of the internal queue that holds tasks for asynchronously converting the Arrow memory format to RowBatches. This parameter is valid when ",(0,s.jsx)(n.code,{children:"starrocks.deserialize.arrow.async"})," is set to ",(0,s.jsx)(n.code,{children:"true"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.filter.query"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsxs)(n.td,{children:["The condition based on which you want to filter data on StarRocks. You can specify multiple filter conditions, which must be joined by ",(0,s.jsx)(n.code,{children:"and"}),". StarRocks filters the data from the StarRocks table based on the specified filter conditions before the data is read by Spark."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.timezone"}),(0,s.jsx)(n.td,{children:"Default timezone of JVM"}),(0,s.jsxs)(n.td,{children:["Supported since 1.1.1. The timezone used to convert StarRocks ",(0,s.jsx)(n.code,{children:"DATETIME"})," to Spark ",(0,s.jsx)(n.code,{children:"TimestampType"}),". The default is the timezone of JVM returned by ",(0,s.jsx)(n.code,{children:"ZoneId#systemDefault()"}),". The format could be a timezone name such as ",(0,s.jsx)(n.code,{children:"Asia/Shanghai"}),", or a zone offset such as ",(0,s.jsx)(n.code,{children:"+08:00"}),"."]})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"parameters-for-spark-sql-and-spark-dataframe",children:"Parameters for Spark SQL and Spark DataFrame"}),"\n",(0,s.jsx)(n.p,{children:"The following parameters apply only to the Spark SQL and Spark DataFrame reading methods."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Default value"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.fe.http.url"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsxs)(n.td,{children:["The HTTP IP address of the FE. This parameter is supported from Spark connector 1.1.0 onwards. This parameter is equivalent to ",(0,s.jsx)(n.code,{children:"starrocks.fenodes"}),". You only need to configure one of them. In Spark connector 1.1.0 and later, we recommend that you use ",(0,s.jsx)(n.code,{children:"starrocks.fe.http.url"})," because ",(0,s.jsx)(n.code,{children:"starrocks.fenodes"})," may be deprecated."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.fe.jdbc.url"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsxs)(n.td,{children:["The address that is used to connect to the MySQL server of the FE. Format: ",(0,s.jsx)(n.code,{children:"jdbc:mysql://<fe_host>:<fe_query_port>"}),".",(0,s.jsx)("br",{}),(0,s.jsx)(n.strong,{children:"NOTICE"}),(0,s.jsx)("br",{}),"In Spark connector 1.1.0 and later, this parameter is mandatory."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"user"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsxs)(n.td,{children:["The username of your StarRocks cluster account. The user needs the ",(0,s.jsx)(n.a,{href:"/docs/3.0/sql-reference/sql-statements/account-management/GRANT",children:"SELECT privilege"})," on the StarRocks table."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.user"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsxs)(n.td,{children:["The username of your StarRocks cluster account. This parameter is supported from Spark connector 1.1.0 onwards. This parameter is equivalent to ",(0,s.jsx)(n.code,{children:"user"}),". You only need to configure one of them. In Spark connector 1.1.0 and later, we recommend that you use ",(0,s.jsx)(n.code,{children:"starrocks.user"})," because ",(0,s.jsx)(n.code,{children:"user"})," may be deprecated."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"password"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsx)(n.td,{children:"The password of your StarRocks cluster account."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.password"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsxs)(n.td,{children:["The password of your StarRocks cluster account. This parameter is supported from Spark connector 1.1.0 onwards. This parameter is equivalent to ",(0,s.jsx)(n.code,{children:"password"}),". You only need to configure one of them. In Spark connector 1.1.0 and later, we recommend that you use ",(0,s.jsx)(n.code,{children:"starrocks.password"})," because ",(0,s.jsx)(n.code,{children:"password"})," may be deprecated."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.filter.query.in.max.count"}),(0,s.jsx)(n.td,{children:"100"}),(0,s.jsx)(n.td,{children:"The maximum number of values supported by the IN expression during predicate pushdown. If the number of values specified in the IN expression exceeds this limit, the filter conditions specified in the IN expression are processed on Spark."})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"parameters-for-spark-rdd",children:"Parameters for Spark RDD"}),"\n",(0,s.jsx)(n.p,{children:"The following parameters apply only to the Spark RDD reading method."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Default value"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.request.auth.user"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsx)(n.td,{children:"The username of your StarRocks cluster account."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.request.auth.password"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsx)(n.td,{children:"The password of your StarRocks cluster account."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"starrocks.read.field"}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsx)(n.td,{children:"The StarRocks table column from which you want to read data. You can specify multiple columns, which must be separated by a comma (,)."})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"data-type-mapping-between-starrocks-and-spark",children:"Data type mapping between StarRocks and Spark"}),"\n",(0,s.jsx)(n.h3,{id:"spark-connector-110-and-later-1",children:"Spark connector 1.1.0 and later"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"StarRocks data type"}),(0,s.jsx)(n.th,{children:"Spark data type"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BOOLEAN"}),(0,s.jsx)(n.td,{children:"DataTypes.BooleanType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"TINYINT"}),(0,s.jsx)(n.td,{children:"DataTypes.ByteType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"SMALLINT"}),(0,s.jsx)(n.td,{children:"DataTypes.ShortType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"INT"}),(0,s.jsx)(n.td,{children:"DataTypes.IntegerType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BIGINT"}),(0,s.jsx)(n.td,{children:"DataTypes.LongType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"LARGEINT"}),(0,s.jsx)(n.td,{children:"DataTypes.StringType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"FLOAT"}),(0,s.jsx)(n.td,{children:"DataTypes.FloatType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"DOUBLE"}),(0,s.jsx)(n.td,{children:"DataTypes.DoubleType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"DECIMAL"}),(0,s.jsx)(n.td,{children:"DecimalType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"CHAR"}),(0,s.jsx)(n.td,{children:"DataTypes.StringType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"VARCHAR"}),(0,s.jsx)(n.td,{children:"DataTypes.StringType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"STRING"}),(0,s.jsx)(n.td,{children:"DataTypes.StringType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"DATE"}),(0,s.jsx)(n.td,{children:"DataTypes.DateType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"DATETIME"}),(0,s.jsx)(n.td,{children:"DataTypes.TimestampType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ARRAY"}),(0,s.jsx)(n.td,{children:"Unsupported datatype"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"HLL"}),(0,s.jsx)(n.td,{children:"Unsupported datatype"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BITMAP"}),(0,s.jsx)(n.td,{children:"Unsupported datatype"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"spark-connector-100-1",children:"Spark connector 1.0.0"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"StarRocks data type"}),(0,s.jsx)(n.th,{children:"Spark data type"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BOOLEAN"}),(0,s.jsx)(n.td,{children:"DataTypes.BooleanType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"TINYINT"}),(0,s.jsx)(n.td,{children:"DataTypes.ByteType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"SMALLINT"}),(0,s.jsx)(n.td,{children:"DataTypes.ShortType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"INT"}),(0,s.jsx)(n.td,{children:"DataTypes.IntegerType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BIGINT"}),(0,s.jsx)(n.td,{children:"DataTypes.LongType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"LARGEINT"}),(0,s.jsx)(n.td,{children:"DataTypes.StringType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"FLOAT"}),(0,s.jsx)(n.td,{children:"DataTypes.FloatType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"DOUBLE"}),(0,s.jsx)(n.td,{children:"DataTypes.DoubleType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"DECIMAL"}),(0,s.jsx)(n.td,{children:"DecimalType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"CHAR"}),(0,s.jsx)(n.td,{children:"DataTypes.StringType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"VARCHAR"}),(0,s.jsx)(n.td,{children:"DataTypes.StringType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"DATE"}),(0,s.jsx)(n.td,{children:"DataTypes.StringType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"DATETIME"}),(0,s.jsx)(n.td,{children:"DataTypes.StringType"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ARRAY"}),(0,s.jsx)(n.td,{children:"Unsupported datatype"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"HLL"}),(0,s.jsx)(n.td,{children:"Unsupported datatype"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BITMAP"}),(0,s.jsx)(n.td,{children:"Unsupported datatype"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"The processing logic of the underlying storage engine used by StarRocks cannot cover an expected time range when DATE and DATETIME data types are directly used. Therefore, the Spark connector maps the DATE and DATETIME data types from StarRocks to the STRING data type from Spark, and generates readable string texts matching the date and time data read from StarRocks."}),"\n",(0,s.jsx)(n.h2,{id:"upgrade-spark-connector",children:"Upgrade Spark connector"}),"\n",(0,s.jsx)(n.h3,{id:"upgrade-from-version-100-to-version-110",children:"Upgrade from version 1.0.0 to version 1.1.0"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Since 1.1.1, the Spark connector does not provide ",(0,s.jsx)(n.code,{children:"mysql-connector-java"})," which is the official JDBC driver for MySQL, because of the limitations of the GPL license used by ",(0,s.jsx)(n.code,{children:"mysql-connector-java"}),".\nHowever, the Spark connector still needs the ",(0,s.jsx)(n.code,{children:"mysql-connector-java"})," to connect to StarRocks for the table metadata, so you need to add the driver to the Spark classpath manually. You can find the\ndriver on ",(0,s.jsx)(n.a,{href:"https://dev.mysql.com/downloads/connector/j/",children:"MySQL site"})," or ",(0,s.jsx)(n.a,{href:"https://repo1.maven.org/maven2/mysql/mysql-connector-java/",children:"Maven Central"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In version 1.1.0, the Spark connector uses JDBC to access StarRocks to obtain more detailed table information. Therefore, you must configure ",(0,s.jsx)(n.code,{children:"starrocks.fe.jdbc.url"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"In version 1.1.0, some parameters are renamed. Both the old and new parameters are retained for now. For each pair of equivalent parameters, you only need to configure one of them, but we recommend that you use the new one because the old one may be deprecated."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"starrocks.fenodes"})," is renamed as ",(0,s.jsx)(n.code,{children:"starrocks.fe.http.url"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"user"})," is renamed as ",(0,s.jsx)(n.code,{children:"starrocks.user"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"password"})," is renamed as ",(0,s.jsx)(n.code,{children:"starrocks.password"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"In version 1.1.0, the mappings of some data types are adjusted based on Spark 3.x:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"DATE"})," in StarRocks is mapped to ",(0,s.jsx)(n.code,{children:"DataTypes.DateType"})," (originally ",(0,s.jsx)(n.code,{children:"DataTypes.StringType"}),") in Spark."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"DATETIME"})," in StarRocks is mapped to ",(0,s.jsx)(n.code,{children:"DataTypes.TimestampType"})," (originally ",(0,s.jsx)(n.code,{children:"DataTypes.StringType"}),") in Spark."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,s.jsxs)(n.p,{children:["The following examples assume you have created a database named ",(0,s.jsx)(n.code,{children:"test"})," in your StarRocks cluster and you have the permissions of user ",(0,s.jsx)(n.code,{children:"root"}),". The parameter settings in the examples are based on Spark Connector 1.1.0."]}),"\n",(0,s.jsx)(n.h3,{id:"data-example",children:"Data example"}),"\n",(0,s.jsx)(n.p,{children:"Do as follows to prepare a sample table:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Go to the ",(0,s.jsx)(n.code,{children:"test"})," database and create a table named ",(0,s.jsx)(n.code,{children:"score_board"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'MySQL [test]> CREATE TABLE `score_board`\n(\n    `id` int(11) NOT NULL COMMENT "",\n    `name` varchar(65533) NULL DEFAULT "" COMMENT "",\n    `score` int(11) NOT NULL DEFAULT "0" COMMENT ""\n)\nENGINE=OLAP\nPRIMARY KEY(`id`)\nCOMMENT "OLAP"\nDISTRIBUTED BY HASH(`id`)\nPROPERTIES (\n    "replication_num" = "3"\n);\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Insert data into the ",(0,s.jsx)(n.code,{children:"score_board"})," table."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"MySQL [test]> INSERT INTO score_board\nVALUES\n    (1, 'Bob', 21),\n    (2, 'Stan', 21),\n    (3, 'Sam', 22),\n    (4, 'Tony', 22),\n    (5, 'Alice', 22),\n    (6, 'Lucy', 23),\n    (7, 'Polly', 23),\n    (8, 'Tom', 23),\n    (9, 'Rose', 24),\n    (10, 'Jerry', 24),\n    (11, 'Jason', 24),\n    (12, 'Lily', 25),\n    (13, 'Stephen', 25),\n    (14, 'David', 25),\n    (15, 'Eddie', 26),\n    (16, 'Kate', 27),\n    (17, 'Cathy', 27),\n    (18, 'Judy', 27),\n    (19, 'Julia', 28),\n    (20, 'Robert', 28),\n    (21, 'Jack', 29);\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Query the ",(0,s.jsx)(n.code,{children:"score_board"})," table."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"MySQL [test]> SELECT * FROM score_board;\n+------+---------+-------+\n| id   | name    | score |\n+------+---------+-------+\n|    1 | Bob     |    21 |\n|    2 | Stan    |    21 |\n|    3 | Sam     |    22 |\n|    4 | Tony    |    22 |\n|    5 | Alice   |    22 |\n|    6 | Lucy    |    23 |\n|    7 | Polly   |    23 |\n|    8 | Tom     |    23 |\n|    9 | Rose    |    24 |\n|   10 | Jerry   |    24 |\n|   11 | Jason   |    24 |\n|   12 | Lily    |    25 |\n|   13 | Stephen |    25 |\n|   14 | David   |    25 |\n|   15 | Eddie   |    26 |\n|   16 | Kate    |    27 |\n|   17 | Cathy   |    27 |\n|   18 | Judy    |    27 |\n|   19 | Julia   |    28 |\n|   20 | Robert  |    28 |\n|   21 | Jack    |    29 |\n+------+---------+-------+\n21 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"read-data-using-spark-sql",children:"Read data using Spark SQL"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run the following command in the Spark directory to start Spark SQL:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain",children:"sh spark-sql\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Run the following command to create a temporary view named ",(0,s.jsx)(n.code,{children:"spark_starrocks"})," on the ",(0,s.jsx)(n.code,{children:"score_board"})," table which belongs to the ",(0,s.jsx)(n.code,{children:"test"})," database:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'spark-sql> CREATE TEMPORARY VIEW spark_starrocks\n           USING starrocks\n           OPTIONS\n           (\n               "starrocks.table.identifier" = "test.score_board",\n               "starrocks.fe.http.url" = "<fe_host>:<fe_http_port>",\n               "starrocks.fe.jdbc.url" = "jdbc:mysql://<fe_host>:<fe_query_port>",\n               "starrocks.user" = "root",\n               "starrocks.password" = ""\n           );\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run the following command to read data from the temporary view:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"spark-sql> SELECT * FROM spark_starrocks;\n"})}),"\n",(0,s.jsx)(n.p,{children:"Spark returns the following data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"1        Bob        21\n2        Stan        21\n3        Sam        22\n4        Tony        22\n5        Alice        22\n6        Lucy        23\n7        Polly        23\n8        Tom        23\n9        Rose        24\n10        Jerry        24\n11        Jason        24\n12        Lily        25\n13        Stephen        25\n14        David        25\n15        Eddie        26\n16        Kate        27\n17        Cathy        27\n18        Judy        27\n19        Julia        28\n20        Robert        28\n21        Jack        29\nTime taken: 1.883 seconds, Fetched 21 row(s)\n22/08/09 15:29:36 INFO thriftserver.SparkSQLCLIDriver: Time taken: 1.883 seconds, Fetched 21 row(s)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"read-data-using-spark-dataframe",children:"Read data using Spark DataFrame"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run the following command in the Spark directory to start Spark Shell:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain",children:"sh spark-shell\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Run the following command to create a DataFrame named ",(0,s.jsx)(n.code,{children:"starrocksSparkDF"})," on the ",(0,s.jsx)(n.code,{children:"score_board"})," table which belongs to the ",(0,s.jsx)(n.code,{children:"test"})," database:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:'scala> val starrocksSparkDF = spark.read.format("starrocks")\n           .option("starrocks.table.identifier", s"test.score_board")\n           .option("starrocks.fe.http.url", s"<fe_host>:<fe_http_port>")\n           .option("starrocks.fe.jdbc.url", s"jdbc:mysql://<fe_host>:<fe_query_port>")\n           .option("starrocks.user", s"root")\n           .option("starrocks.password", s"")\n           .load()\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Read data from the DataFrame. For example, if you want to read the first 10 rows, run the following command:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:"scala> starrocksSparkDF.show(10)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Spark returns the following data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:"+---+-----+-----+\n| id| name|score|\n+---+-----+-----+\n|  1|  Bob|   21|\n|  2| Stan|   21|\n|  3|  Sam|   22|\n|  4| Tony|   22|\n|  5|Alice|   22|\n|  6| Lucy|   23|\n|  7|Polly|   23|\n|  8|  Tom|   23|\n|  9| Rose|   24|\n| 10|Jerry|   24|\n+---+-----+-----+\nonly showing top 10 rows\n"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsx)(n.p,{children:"By default, if you do not specify the number of rows you want to read, Spark returns the first 20 rows."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"read-data-using-spark-rdd",children:"Read data using Spark RDD"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run the following command in the Spark directory to start Spark Shell:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain",children:"sh spark-shell\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Run the following command to create an RDD named ",(0,s.jsx)(n.code,{children:"starrocksSparkRDD"})," on the ",(0,s.jsx)(n.code,{children:"score_board"})," table which belongs to the ",(0,s.jsx)(n.code,{children:"test"})," database."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:'scala> import com.starrocks.connector.spark._\nscala> val starrocksSparkRDD = sc.starrocksRDD\n           (\n           tableIdentifier = Some("test.score_board"),\n           cfg = Some(Map(\n               "starrocks.fenodes" -> "<fe_host>:<fe_http_port>",\n               "starrocks.request.auth.user" -> "root",\n               "starrocks.request.auth.password" -> ""\n           ))\n           )\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Read data from the RDD. For example, if you want to read the first 10 elements, run the following command:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:"scala> starrocksSparkRDD.take(10)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Spark returns the following data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:"res0: Array[AnyRef] = Array([1, Bob, 21], [2, Stan, 21], [3, Sam, 22], [4, Tony, 22], [5, Alice, 22], [6, Lucy, 23], [7, Polly, 23], [8, Tom, 23], [9, Rose, 24], [10, Jerry, 24])\n"})}),"\n",(0,s.jsx)(n.p,{children:"To read the entire RDD, run the following command:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:"scala> starrocksSparkRDD.collect()\n"})}),"\n",(0,s.jsx)(n.p,{children:"Spark returns the following data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:"res1: Array[AnyRef] = Array([1, Bob, 21], [2, Stan, 21], [3, Sam, 22], [4, Tony, 22], [5, Alice, 22], [6, Lucy, 23], [7, Polly, 23], [8, Tom, 23], [9, Rose, 24], [10, Jerry, 24], [11, Jason, 24], [12, Lily, 25], [13, Stephen, 25], [14, David, 25], [15, Eddie, 26], [16, Kate, 27], [17, Cathy, 27], [18, Judy, 27], [19, Julia, 28], [20, Robert, 28], [21, Jack, 29])\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best practices"}),"\n",(0,s.jsxs)(n.p,{children:["When you read data from StarRocks using the Spark connector, you can use the ",(0,s.jsx)(n.code,{children:"starrocks.filter.query"})," parameter to specify filter conditions based on which Spark prunes partitions, buckets, and prefix indexes to reduce the cost of data pulling. This section uses Spark DataFrame as an example to show how this is achieved."]}),"\n",(0,s.jsx)(n.h3,{id:"environment-setup",children:"Environment setup"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"Version"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Spark"}),(0,s.jsx)(n.td,{children:"Spark 2.4.4 and Scala 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_302)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"StarRocks"}),(0,s.jsx)(n.td,{children:"2.2.0"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Spark connector"}),(0,s.jsx)(n.td,{children:"starrocks-spark2_2.11-1.0.0.jar"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"data-example-1",children:"Data example"}),"\n",(0,s.jsx)(n.p,{children:"Do as follows to prepare a sample table:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Go to the ",(0,s.jsx)(n.code,{children:"test"})," database and create a table named  ",(0,s.jsx)(n.code,{children:"mytable"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'MySQL [test]> CREATE TABLE `mytable`\n(\n    `k` int(11) NULL COMMENT "bucket",\n    `b` int(11) NULL COMMENT "",\n    `dt` datetime NULL COMMENT "",\n    `v` int(11) NULL COMMENT ""\n)\nENGINE=OLAP\nDUPLICATE KEY(`k`,`b`, `dt`)\nCOMMENT "OLAP"\nPARTITION BY RANGE(`dt`)\n(\n    PARTITION p202201 VALUES [(\'2022-01-01 00:00:00\'), (\'2022-02-01 00:00:00\')),\n    PARTITION p202202 VALUES [(\'2022-02-01 00:00:00\'), (\'2022-03-01 00:00:00\')),\n    PARTITION p202203 VALUES [(\'2022-03-01 00:00:00\'), (\'2022-04-01 00:00:00\'))\n)\nDISTRIBUTED BY HASH(`k`)\nPROPERTIES (\n    "replication_num" = "3"\n);\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Insert data into ",(0,s.jsx)(n.code,{children:"mytable"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"MySQL [test]> INSERT INTO mytable\nVALUES\n     (1, 11, '2022-01-02 08:00:00', 111),\n     (2, 22, '2022-02-02 08:00:00', 222),\n     (3, 33, '2022-03-02 08:00:00', 333);\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Query the ",(0,s.jsx)(n.code,{children:"mytable"})," table."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"MySQL [test]> select * from mytable;\n+------+------+---------------------+------+\n| k    | b    | dt                  | v    |\n+------+------+---------------------+------+\n|    1 |   11 | 2022-01-02 08:00:00 |  111 |\n|    2 |   22 | 2022-02-02 08:00:00 |  222 |\n|    3 |   33 | 2022-03-02 08:00:00 |  333 |\n+------+------+---------------------+------+\n3 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"full-table-scan",children:"Full table scan"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Run the following command in the Spark directory to create a DataFrame named ",(0,s.jsx)(n.code,{children:"df"})," on ",(0,s.jsx)(n.code,{children:"mytable"})," table which belongs to the ",(0,s.jsx)(n.code,{children:"test"})," database:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:'scala>  val df = spark.read.format("starrocks")\n        .option("starrocks.table.identifier", s"test.mytable")\n        .option("starrocks.fenodes", s"<fe_host>:<fe_http_port>")\n        .option("user", s"root")\n        .option("password", s"")\n        .load()\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["View the FE log file ",(0,s.jsx)(n.strong,{children:"fe.log"})," of your StarRocks cluster, and find the SQL statement executed to read data. Example:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"2022-08-09 18:57:38,091 INFO (nioEventLoopGroup-3-10|196) [TableQueryPlanAction.executeWithoutPassword():126] receive SQL statement [select `k`,`b`,`dt`,`v` from `test`.`mytable`] from external service [ user ['root'@'%']] for database [test] table [mytable]\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In the ",(0,s.jsx)(n.code,{children:"test"})," database, use EXPLAIN to obtain the execution plan of the SELECT ",(0,s.jsx)(n.code,{children:"k"}),",",(0,s.jsx)(n.code,{children:"b"}),",",(0,s.jsx)(n.code,{children:"dt"}),",",(0,s.jsx)(n.code,{children:"v"})," from ",(0,s.jsx)(n.code,{children:"test"}),".",(0,s.jsx)(n.code,{children:"mytable"})," statement:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:"MySQL [test]> EXPLAIN select `k`,`b`,`dt`,`v` from `test`.`mytable`;\n+-----------------------------------------------------------------------+\n| Explain String                                                        |\n+-----------------------------------------------------------------------+\n| PLAN FRAGMENT 0                                                       |\n|  OUTPUT EXPRS:1: k | 2: b | 3: dt | 4: v                              |\n|   PARTITION: UNPARTITIONED                                            |\n|                                                                       |\n|   RESULT SINK                                                         |\n|                                                                       |\n|   1:EXCHANGE                                                          |\n|                                                                       |\n| PLAN FRAGMENT 1                                                       |\n|  OUTPUT EXPRS:                                                        |\n|   PARTITION: RANDOM                                                   |\n|                                                                       |\n|   STREAM DATA SINK                                                    |\n|     EXCHANGE ID: 01                                                   |\n|     UNPARTITIONED                                                     |\n|                                                                       |\n|   0:OlapScanNode                                                      |\n|      TABLE: mytable                                                   |\n|      PREAGGREGATION: ON                                               |\n|      partitions=3/3                                                   |\n|      rollup: mytable                                                  |\n|      tabletRatio=9/9                                                  |\n|      tabletList=41297,41299,41301,41303,41305,41307,41309,41311,41313 |\n|      cardinality=3                                                    |\n|      avgRowSize=4.0                                                   |\n|      numNodes=0                                                       |\n+-----------------------------------------------------------------------+\n26 rows in set (0.00 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In this example, no pruning is performed. Therefore, Spark scans all of the three partitions (as suggested by ",(0,s.jsx)(n.code,{children:"partitions=3/3"}),") that hold data, and scans all of the 9 tablets (as suggested by ",(0,s.jsx)(n.code,{children:"tabletRatio=9/9"}),") in those three partitions."]}),"\n",(0,s.jsx)(n.h3,{id:"partition-pruning",children:"Partition pruning"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Run the following command, in which you use the ",(0,s.jsx)(n.code,{children:"starrocks.filter.query"})," parameter to specify a filter condition ",(0,s.jsx)(n.code,{children:"dt='2022-01-02 08:00:00"})," for partition pruning, in the Spark directory to create a DataFrame named ",(0,s.jsx)(n.code,{children:"df"})," on the ",(0,s.jsx)(n.code,{children:"mytable"})," table which belongs to the ",(0,s.jsx)(n.code,{children:"test"})," database:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:'scala> val df = spark.read.format("starrocks")\n       .option("starrocks.table.identifier", s"test.mytable")\n       .option("starrocks.fenodes", s"<fe_host>:<fe_http_port>")\n       .option("user", s"root")\n       .option("password", s"")\n       .option("starrocks.filter.query", "dt=\'2022-01-02 08:00:00\'")\n       .load()\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["View the FE log file ",(0,s.jsx)(n.strong,{children:"fe.log"})," of your StarRocks cluster, and find the SQL statement executed to read data. Example:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"2022-08-09 19:02:31,253 INFO (nioEventLoopGroup-3-14|204) [TableQueryPlanAction.executeWithoutPassword():126] receive SQL statement [select `k`,`b`,`dt`,`v` from `test`.`mytable` where dt='2022-01-02 08:00:00'] from external service [ user ['root'@'%']] for database [test] table [mytable]\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In the ",(0,s.jsx)(n.code,{children:"test"})," database, use EXPLAIN to obtain the execution plan of the SELECT ",(0,s.jsx)(n.code,{children:"k"}),",",(0,s.jsx)(n.code,{children:"b"}),",",(0,s.jsx)(n.code,{children:"dt"}),",",(0,s.jsx)(n.code,{children:"v"})," from ",(0,s.jsx)(n.code,{children:"test"}),".",(0,s.jsx)(n.code,{children:"mytable"})," where dt='2022-01-02 08:00:00' statement:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:"MySQL [test]> EXPLAIN select `k`,`b`,`dt`,`v` from `test`.`mytable` where dt='2022-01-02 08:00:00';\n+------------------------------------------------+\n| Explain String                                 |\n+------------------------------------------------+\n| PLAN FRAGMENT 0                                |\n|  OUTPUT EXPRS:1: k | 2: b | 3: dt | 4: v       |\n|   PARTITION: UNPARTITIONED                     |\n|                                                |\n|   RESULT SINK                                  |\n|                                                |\n|   1:EXCHANGE                                   |\n|                                                |\n| PLAN FRAGMENT 1                                |\n|  OUTPUT EXPRS:                                 |\n|   PARTITION: RANDOM                            |\n|                                                |\n|   STREAM DATA SINK                             |\n|     EXCHANGE ID: 01                            |\n|     UNPARTITIONED                              |\n|                                                |\n|   0:OlapScanNode                               |\n|      TABLE: mytable                            |\n|      PREAGGREGATION: ON                        |\n|      PREDICATES: 3: dt = '2022-01-02 08:00:00' |\n|      partitions=1/3                            |\n|      rollup: mytable                           |\n|      tabletRatio=3/3                           |\n|      tabletList=41297,41299,41301              |\n|      cardinality=1                             |\n|      avgRowSize=20.0                           |\n|      numNodes=0                                |\n+------------------------------------------------+\n27 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In this example, only partition pruning is performed, whereas bucket pruning is not. Therefore, Spark scans one of the three partitions (as suggested by ",(0,s.jsx)(n.code,{children:"partitions=1/3"}),") and all of the tablets (as suggested by ",(0,s.jsx)(n.code,{children:"tabletRatio=3/3"}),") in that partition."]}),"\n",(0,s.jsx)(n.h3,{id:"bucket-pruning",children:"Bucket pruning"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Run the following command, in which you use the ",(0,s.jsx)(n.code,{children:"starrocks.filter.query"})," parameter to specify a filter condition ",(0,s.jsx)(n.code,{children:"k=1"})," for bucket pruning, in the Spark directory to create a DataFrame named ",(0,s.jsx)(n.code,{children:"df"})," on the ",(0,s.jsx)(n.code,{children:"mytable"})," table which belongs to the ",(0,s.jsx)(n.code,{children:"test"})," database:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:'scala> val df = spark.read.format("starrocks")\n       .option("starrocks.table.identifier", s"test.mytable")\n       .option("starrocks.fenodes", s"<fe_host>:<fe_http_port>")\n       .option("user", s"root")\n       .option("password", s"")\n       .option("starrocks.filter.query", "k=1")\n       .load()\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["View the FE log file  ",(0,s.jsx)(n.strong,{children:"fe.log"})," of your StarRocks cluster, and find the SQL statement executed to read data. Example:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"2022-08-09 19:04:44,479 INFO (nioEventLoopGroup-3-16|208) [TableQueryPlanAction.executeWithoutPassword():126] receive SQL statement [select `k`,`b`,`dt`,`v` from `test`.`mytable` where k=1] from external service [ user ['root'@'%']] for database [test] table [mytable]\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In the ",(0,s.jsx)(n.code,{children:"test"})," database, use EXPLAIN to obtain the execution plan of the SELECT ",(0,s.jsx)(n.code,{children:"k"}),",",(0,s.jsx)(n.code,{children:"b"}),",",(0,s.jsx)(n.code,{children:"dt"}),",",(0,s.jsx)(n.code,{children:"v"})," from ",(0,s.jsx)(n.code,{children:"test"}),".",(0,s.jsx)(n.code,{children:"mytable"})," where k=1 statement:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:"MySQL [test]> EXPLAIN select `k`,`b`,`dt`,`v` from `test`.`mytable` where k=1;\n+------------------------------------------+\n| Explain String                           |\n+------------------------------------------+\n| PLAN FRAGMENT 0                          |\n|  OUTPUT EXPRS:1: k | 2: b | 3: dt | 4: v |\n|   PARTITION: UNPARTITIONED               |\n|                                          |\n|   RESULT SINK                            |\n|                                          |\n|   1:EXCHANGE                             |\n|                                          |\n| PLAN FRAGMENT 1                          |\n|  OUTPUT EXPRS:                           |\n|   PARTITION: RANDOM                      |\n|                                          |\n|   STREAM DATA SINK                       |\n|     EXCHANGE ID: 01                      |\n|     UNPARTITIONED                        |\n|                                          |\n|   0:OlapScanNode                         |\n|      TABLE: mytable                      |\n|      PREAGGREGATION: ON                  |\n|      PREDICATES: 1: k = 1                |\n|      partitions=3/3                      |\n|      rollup: mytable                     |\n|      tabletRatio=3/9                     |\n|      tabletList=41299,41305,41311        |\n|      cardinality=1                       |\n|      avgRowSize=20.0                     |\n|      numNodes=0                          |\n+------------------------------------------+\n27 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In this example, only bucket pruning is performed, whereas partition pruning is not. Therefore, Spark scans all of the three partitions (as suggested by ",(0,s.jsx)(n.code,{children:"partitions=3/3"}),") that hold data, and scans all of the three tablets (as suggested by ",(0,s.jsx)(n.code,{children:"tabletRatio=3/9"}),") to retrieve Hash values that meet the ",(0,s.jsx)(n.code,{children:"k = 1"})," filter condition within those three partitions."]}),"\n",(0,s.jsx)(n.h3,{id:"partition-pruning-and-bucket-pruning",children:"Partition pruning and bucket pruning"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Run the following command, in which you use the ",(0,s.jsx)(n.code,{children:"starrocks.filter.query"})," parameter to specify two filter conditions ",(0,s.jsx)(n.code,{children:"k=7"})," and ",(0,s.jsx)(n.code,{children:"dt='2022-01-02 08:00:00'"})," for bucket pruning and partition pruning, in the Spark directory to create a DataFrame named ",(0,s.jsx)(n.code,{children:"df"})," on the ",(0,s.jsx)(n.code,{children:"mytable"})," table on the ",(0,s.jsx)(n.code,{children:"test"})," database:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:'scala> val df = spark.read.format("starrocks")\n       .option("starrocks.table.identifier", s"test.mytable")\n       .option("starrocks.fenodes", s"<fe_host>:<fe_http_port>")\n       .option("user", s"")\n       .option("password", s"")\n       .option("starrocks.filter.query", "k=7 and dt=\'2022-01-02 08:00:00\'")\n       .load()\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["View the FE log file ",(0,s.jsx)(n.strong,{children:"fe.log"})," of your StarRocks cluster, and find the SQL statement executed to read data. Example:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"2022-08-09 19:06:34,939 INFO (nioEventLoopGroup-3-18|212) [TableQueryPlanAction.executeWithoutPassword():126] receive SQL statement [select `k`,`b`,`dt`,`v` from `test`.`mytable` where k=7 and dt='2022-01-02 08:00:00'] from external service [ user ['root'@'%']] for database [test] t\nable [mytable]\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In the ",(0,s.jsx)(n.code,{children:"test"})," database, use EXPLAIN to obtain the execution plan of the SELECT ",(0,s.jsx)(n.code,{children:"k"}),",",(0,s.jsx)(n.code,{children:"b"}),",",(0,s.jsx)(n.code,{children:"dt"}),",",(0,s.jsx)(n.code,{children:"v"})," from ",(0,s.jsx)(n.code,{children:"test"}),".",(0,s.jsx)(n.code,{children:"mytable"})," where k=7 and dt='2022-01-02 08:00:00' statement:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:"MySQL [test]> EXPLAIN select `k`,`b`,`dt`,`v` from `test`.`mytable` where k=7 and dt='2022-01-02 08:00:00';\n+----------------------------------------------------------+\n| Explain String                                           |\n+----------------------------------------------------------+\n| PLAN FRAGMENT 0                                          |\n|  OUTPUT EXPRS:1: k | 2: b | 3: dt | 4: v                 |\n|   PARTITION: RANDOM                                      |\n|                                                          |\n|   RESULT SINK                                            |\n|                                                          |\n|   0:OlapScanNode                                         |\n|      TABLE: mytable                                      |\n|      PREAGGREGATION: ON                                  |\n|      PREDICATES: 1: k = 7, 3: dt = '2022-01-02 08:00:00' |\n|      partitions=1/3                                      |\n|      rollup: mytable                                     |\n|      tabletRatio=1/3                                     |\n|      tabletList=41301                                    |\n|      cardinality=1                                       |\n|      avgRowSize=20.0                                     |\n|      numNodes=0                                          |\n+----------------------------------------------------------+\n17 rows in set (0.00 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In this example, both partition pruning and bucket pruning are performed. Therefore, Spark scans only one of the three partitions (as suggested by ",(0,s.jsx)(n.code,{children:"partitions=1/3"}),") and only one tablet (as suggested by ",(0,s.jsx)(n.code,{children:"tabletRatio=1/3"}),") in that partition."]}),"\n",(0,s.jsx)(n.h3,{id:"prefix-index-filtering",children:"Prefix index filtering"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Insert more data records into a partition of the ",(0,s.jsx)(n.code,{children:"mytable"})," table which belongs to the ",(0,s.jsx)(n.code,{children:"test"})," database:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:'MySQL [test]> INSERT INTO mytable\nVALUES\n    (1, 11, "2022-01-02 08:00:00", 111), \n    (3, 33, "2022-01-02 08:00:00", 333), \n    (3, 33, "2022-01-02 08:00:00", 333), \n    (3, 33, "2022-01-02 08:00:00", 333);\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Query the ",(0,s.jsx)(n.code,{children:"mytable"})," table:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:"MySQL [test]> SELECT * FROM mytable;\n+------+------+---------------------+------+\n| k    | b    | dt                  | v    |\n+------+------+---------------------+------+\n|    1 |   11 | 2022-01-02 08:00:00 |  111 |\n|    1 |   11 | 2022-01-02 08:00:00 |  111 |\n|    3 |   33 | 2022-01-02 08:00:00 |  333 |\n|    3 |   33 | 2022-01-02 08:00:00 |  333 |\n|    3 |   33 | 2022-01-02 08:00:00 |  333 |\n|    2 |   22 | 2022-02-02 08:00:00 |  222 |\n|    3 |   33 | 2022-03-02 08:00:00 |  333 |\n+------+------+---------------------+------+\n7 rows in set (0.01 sec)\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Run the following command, in which you use the ",(0,s.jsx)(n.code,{children:"starrocks.filter.query"})," parameter to specify a filter condition ",(0,s.jsx)(n.code,{children:"k=1"})," for prefix index filtering, in the Spark directory to create a DataFrame named ",(0,s.jsx)(n.code,{children:"df"})," on the ",(0,s.jsx)(n.code,{children:"mytable"})," table which belongs to the ",(0,s.jsx)(n.code,{children:"test"})," database:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Scala",children:'scala> val df = spark.read.format("starrocks")\n       .option("starrocks.table.identifier", s"test.mytable")\n       .option("starrocks.fenodes", s"<fe_host>:<fe_http_port>")\n       .option("user", s"root")\n       .option("password", s"")\n       .option("starrocks.filter.query", "k=1")\n       .load()\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In the ",(0,s.jsx)(n.code,{children:"test"})," database, set ",(0,s.jsx)(n.code,{children:"is_report_success"})," to ",(0,s.jsx)(n.code,{children:"true"})," to enable profile reporting:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"MySQL [test]> SET is_report_success = true;\nQuery OK, 0 rows affected (0.00 sec)\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Use a browser to open the ",(0,s.jsx)(n.code,{children:"http://<fe_host>:<http_http_port>/query"})," page, and view the profile of the SELECT * FROM mytable where k=1 statement. Example:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"OLAP_SCAN (plan_node_id=0):\n  CommonMetrics:\n     - CloseTime: 1.255ms\n     - OperatorTotalTime: 1.404ms\n     - PeakMemoryUsage: 0.00 \n     - PullChunkNum: 8\n     - PullRowNum: 2\n       - __MAX_OF_PullRowNum: 2\n       - __MIN_OF_PullRowNum: 0\n     - PullTotalTime: 148.60us\n     - PushChunkNum: 0\n     - PushRowNum: 0\n     - PushTotalTime: 0ns\n     - SetFinishedTime: 136ns\n     - SetFinishingTime: 129ns\n  UniqueMetrics:\n     - Predicates: 1: k = 1\n     - Rollup: mytable\n     - Table: mytable\n     - BytesRead: 88.00 B\n       - __MAX_OF_BytesRead: 88.00 B\n       - __MIN_OF_BytesRead: 0.00 \n     - CachedPagesNum: 0\n     - CompressedBytesRead: 844.00 B\n       - __MAX_OF_CompressedBytesRead: 844.00 B\n       - __MIN_OF_CompressedBytesRead: 0.00 \n     - CreateSegmentIter: 18.582us\n     - IOTime: 4.425us\n     - LateMaterialize: 17.385us\n     - PushdownPredicates: 3\n     - RawRowsRead: 2\n       - __MAX_OF_RawRowsRead: 2\n       - __MIN_OF_RawRowsRead: 0\n     - ReadPagesNum: 12\n       - __MAX_OF_ReadPagesNum: 12\n       - __MIN_OF_ReadPagesNum: 0\n     - RowsRead: 2\n       - __MAX_OF_RowsRead: 2\n       - __MIN_OF_RowsRead: 0\n     - ScanTime: 154.367us\n     - SegmentInit: 95.903us\n       - BitmapIndexFilter: 0ns\n       - BitmapIndexFilterRows: 0\n       - BloomFilterFilterRows: 0\n       - ShortKeyFilterRows: 3\n         - __MAX_OF_ShortKeyFilterRows: 3\n         - __MIN_OF_ShortKeyFilterRows: 0\n       - ZoneMapIndexFilterRows: 0\n     - SegmentRead: 2.559us\n       - BlockFetch: 2.187us\n       - BlockFetchCount: 2\n         - __MAX_OF_BlockFetchCount: 2\n         - __MIN_OF_BlockFetchCount: 0\n       - BlockSeek: 7.789us\n       - BlockSeekCount: 2\n         - __MAX_OF_BlockSeekCount: 2\n         - __MIN_OF_BlockSeekCount: 0\n       - ChunkCopy: 25ns\n       - DecompressT: 0ns\n       - DelVecFilterRows: 0\n       - IndexLoad: 0ns\n       - PredFilter: 353ns\n       - PredFilterRows: 0\n       - RowsetsReadCount: 7\n       - SegmentsReadCount: 3\n         - __MAX_OF_SegmentsReadCount: 2\n         - __MIN_OF_SegmentsReadCount: 0\n       - TotalColumnsDataPageCount: 8\n         - __MAX_OF_TotalColumnsDataPageCount: 8\n         - __MIN_OF_TotalColumnsDataPageCount: 0\n     - UncompressedBytesRead: 508.00 B\n       - __MAX_OF_UncompressedBytesRead: 508.00 B\n       - __MIN_OF_UncompressedBytesRead: 0.00 \n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In this example, the filter condition ",(0,s.jsx)(n.code,{children:"k = 1"})," can hit the prefix index. Therefore, Spark can filter out three rows (as suggested by ",(0,s.jsx)(n.code,{children:"ShortKeyFilterRows: 3"}),")."]})]})}const h=function(e={}){const{wrapper:n}=Object.assign({},(0,t.ah)(),e.components);return n?(0,s.jsx)(n,Object.assign({},e,{children:(0,s.jsx)(l,e)})):l(e)}},11151:(e,n,r)=>{r.d(n,{Zo:()=>d,ah:()=>a});var s=r(67294);const t=s.createContext({});function a(e){const n=s.useContext(t);return s.useMemo((()=>"function"==typeof e?e(n):{...n,...e}),[n,e])}const o={};function d({components:e,children:n,disableParentContext:r}){let d;return d=r?"function"==typeof e?e({}):e||o:a(e),s.createElement(t.Provider,{value:d},n)}}}]);