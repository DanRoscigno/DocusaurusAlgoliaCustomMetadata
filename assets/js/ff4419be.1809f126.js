"use strict";(self.webpackChunkstarrocks=self.webpackChunkstarrocks||[]).push([[46553,75525],{5251:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>d});var s=a(85893),o=a(11151);const t={},l=void 0,i={id:"assets/commonMarkdown/insertPrivNote",title:"insertPrivNote",description:"NOTICE",source:"@site/versioned_docs/version-3.1/assets/commonMarkdown/insertPrivNote.md",sourceDirName:"assets/commonMarkdown",slug:"/assets/commonMarkdown/insertPrivNote",permalink:"/docusaurusv3/docs/assets/commonMarkdown/insertPrivNote",draft:!1,unlisted:!1,editUrl:"https://github.com/StarRocks/starrocks/tree/main/versioned_docs/version-3.1/assets/commonMarkdown/insertPrivNote.md",tags:[],version:"3.1",frontMatter:{}},r={},d=[];function c(e){const n=Object.assign({blockquote:"blockquote",p:"p",strong:"strong",a:"a"},(0,o.ah)(),e.components);return(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,s.jsxs)(n.p,{children:["You can load data into StarRocks tables only as a user who has the INSERT privilege on those StarRocks tables. If you do not have the INSERT privilege, follow the instructions provided in ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/account-management/GRANT",children:"GRANT"})," to grant the INSERT privilege to the user that you use to connect to your StarRocks cluster."]}),"\n"]})}const h=function(e={}){const{wrapper:n}=Object.assign({},(0,o.ah)(),e.components);return n?(0,s.jsx)(n,Object.assign({},e,{children:(0,s.jsx)(c,e)})):c(e)}},75092:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>u,frontMatter:()=>l,metadata:()=>r,toc:()=>c});var s=a(85893),o=a(11151),t=a(5251);const l={},i="Load data from cloud storage",r={id:"loading/cloud_storage_load",title:"Load data from cloud storage",description:"StarRocks supports using one of the following methods to load huge amounts of data from cloud storage: Broker Load and INSERT.",source:"@site/versioned_docs/version-3.1/loading/cloud_storage_load.md",sourceDirName:"loading",slug:"/loading/cloud_storage_load",permalink:"/docusaurusv3/docs/loading/cloud_storage_load",draft:!1,unlisted:!1,editUrl:"https://github.com/StarRocks/starrocks/tree/main/versioned_docs/version-3.1/loading/cloud_storage_load.md",tags:[],version:"3.1",frontMatter:{},sidebar:"documentation",previous:{title:"Load data from HDFS",permalink:"/docusaurusv3/docs/loading/hdfs_load"},next:{title:"Load data from Apache Kafka",permalink:"/docusaurusv3/docs/category/load-data-from-apache-kafka"}},d={},c=[{value:"Background information",id:"background-information",level:2},{value:"Supported data file formats",id:"supported-data-file-formats",level:2},{value:"How it works",id:"how-it-works",level:2},{value:"Prepare data examples",id:"prepare-data-examples",level:2},{value:"Load data from AWS S3",id:"load-data-from-aws-s3",level:2},{value:"Load a single data file into a single table",id:"load-a-single-data-file-into-a-single-table",level:3},{value:"Example",id:"example",level:4},{value:"Query data",id:"query-data",level:4},{value:"Load multiple data files into a single table",id:"load-multiple-data-files-into-a-single-table",level:3},{value:"Example",id:"example-1",level:4},{value:"Query data",id:"query-data-1",level:4},{value:"Load multiple data files into multiple tables",id:"load-multiple-data-files-into-multiple-tables",level:3},{value:"Example",id:"example-2",level:4},{value:"Query data",id:"query-data-2",level:4},{value:"Load data from Google GCS",id:"load-data-from-google-gcs",level:2},{value:"Load a single data file into a single table",id:"load-a-single-data-file-into-a-single-table-1",level:3},{value:"Example",id:"example-3",level:4},{value:"Query data",id:"query-data-3",level:4},{value:"Load multiple data files into a single table",id:"load-multiple-data-files-into-a-single-table-1",level:3},{value:"Example",id:"example-4",level:4},{value:"Query data",id:"query-data-4",level:4},{value:"Load multiple data files into multiple tables",id:"load-multiple-data-files-into-multiple-tables-1",level:3},{value:"Example",id:"example-5",level:4},{value:"Query data",id:"query-data-5",level:4},{value:"Load data from Microsoft Azure Storage",id:"load-data-from-microsoft-azure-storage",level:2},{value:"Load a single data file into a single table",id:"load-a-single-data-file-into-a-single-table-2",level:3},{value:"Example",id:"example-6",level:4},{value:"Query data",id:"query-data-6",level:4},{value:"Load multiple data files into a single table",id:"load-multiple-data-files-into-a-single-table-2",level:3},{value:"Example",id:"example-7",level:4},{value:"Query data",id:"query-data-7",level:4},{value:"Load multiple data files into multiple tables",id:"load-multiple-data-files-into-multiple-tables-2",level:3},{value:"Example",id:"example-8",level:4},{value:"Query data",id:"query-data-8",level:4},{value:"Load data from an S3-compatible storage system",id:"load-data-from-an-s3-compatible-storage-system",level:2},{value:"Load a single data file into a single table",id:"load-a-single-data-file-into-a-single-table-3",level:3},{value:"Example",id:"example-9",level:4},{value:"Query data",id:"query-data-9",level:4},{value:"Load multiple data files into a single table",id:"load-multiple-data-files-into-a-single-table-3",level:3},{value:"Example",id:"example-10",level:4},{value:"Query data",id:"query-data-10",level:4},{value:"Load multiple data files into multiple tables",id:"load-multiple-data-files-into-multiple-tables-3",level:3},{value:"Example",id:"example-11",level:4},{value:"Query data",id:"query-data-11",level:4},{value:"View a load job",id:"view-a-load-job",level:2},{value:"Cancel a load job",id:"cancel-a-load-job",level:2},{value:"Job splitting and concurrent running",id:"job-splitting-and-concurrent-running",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2}];function h(e){const n=Object.assign({h1:"h1",p:"p",a:"a",code:"code",h2:"h2",blockquote:"blockquote",strong:"strong",ul:"ul",li:"li",img:"img",ol:"ol",pre:"pre",h3:"h3",h4:"h4"},(0,o.ah)(),e.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"load-data-from-cloud-storage",children:"Load data from cloud storage"}),"\n","\n","\n",(0,s.jsxs)(n.p,{children:["StarRocks supports using one of the following methods to load huge amounts of data from cloud storage: ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/BROKER%20LOAD",children:"Broker Load"})," and ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/insert",children:"INSERT"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["In v3.0 and earlier, StarRocks only supports Broker Load, which runs in asynchronous loading mode. After you submit a load job, StarRocks asynchronously runs the job. You can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsx)(n.p,{children:"Broker Load ensures the transactional atomicity of each load job that is run to load multiple data files, which means that the loading of multiple data files in one load job must all succeed or fail. It never happens that the loading of some data files succeeds while the loading of the other files fails."}),"\n",(0,s.jsxs)(n.p,{children:["Additionally, Broker Load supports data transformation at data loading and supports data changes made by UPSERT and DELETE operations during data loading. For more information, see ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/loading/Etl_in_loading",children:"Transform data at loading"})," and ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/loading/Load_to_Primary_Key_tables",children:"Change data through loading"}),"."]}),"\n",(0,s.jsx)(t.default,{}),"\n",(0,s.jsxs)(n.p,{children:["From v3.1 onwards, StarRocks supports directly loading the data of Parquet-formatted or ORC-formatted files from AWS S3 by using the INSERT command and the FILES keyword, saving you from the trouble of creating an external table first. For more information, see ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/loading/InsertInto#insert-data-directly-from-files-in-an-external-source-using-table-keyword",children:"INSERT > Insert data directly from files in an external source using FILES keyword"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["This topic focuses on using ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/BROKER%20LOAD",children:"Broker Load"})," to load data from cloud storage."]}),"\n",(0,s.jsx)(n.h2,{id:"background-information",children:"Background information"}),"\n",(0,s.jsxs)(n.p,{children:["In v2.4 and earlier, StarRocks depends on brokers to set up connections between your StarRocks cluster and your external storage system when it runs a Broker Load job. Therefore, you need to input ",(0,s.jsx)(n.code,{children:'WITH BROKER "<broker_name>"'}),' to specify the broker you want to use in the load statement. This is called "broker-based loading." A broker is an independent, stateless service that is integrated with a file-system interface. With brokers, StarRocks can access and read data files that are stored in your external storage system, and can use its own computing resources to pre-process and load the data of these data files.']}),"\n",(0,s.jsxs)(n.p,{children:["From v2.5 onwards, StarRocks no longer depends on brokers to set up connections between your StarRocks cluster and your external storage system when it runs a Broker Load job. Therefore, you no longer need to specify a broker in the load statement, but you still need to retain the ",(0,s.jsx)(n.code,{children:"WITH BROKER"}),' keyword. This is called "broker-free loading."']}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsxs)(n.p,{children:["You can use the ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/Administration/SHOW%20BROKER",children:"SHOW BROKER"})," statement to check for brokers that are deployed in your StarRocks cluster. If no brokers are deployed, you can deploy brokers by following the instructions provided in ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/deployment/deploy_broker",children:"Deploy a broker"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"supported-data-file-formats",children:"Supported data file formats"}),"\n",(0,s.jsx)(n.p,{children:"Broker Load supports the following data file formats:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"CSV"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Parquet"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"ORC"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsx)(n.p,{children:"For CSV data, take note of the following points:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"You can use a UTF-8 string, such as a comma (,), tab, or pipe (|), whose length does not exceed 50 bytes as a text delimiter."}),"\n",(0,s.jsxs)(n.li,{children:["Null values are denoted by using ",(0,s.jsx)(n.code,{children:"\\N"}),". For example, a data file consists of three columns, and a record from that data file holds data in the first and third columns but no data in the second column. In this situation, you need to use ",(0,s.jsx)(n.code,{children:"\\N"})," in the second column to denote a null value. This means the record must be compiled as ",(0,s.jsx)(n.code,{children:"a,\\N,b"})," instead of ",(0,s.jsx)(n.code,{children:"a,,b"}),". ",(0,s.jsx)(n.code,{children:"a,,b"})," denotes that the second column of the record holds an empty string."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"how-it-works",children:"How it works"}),"\n",(0,s.jsx)(n.p,{children:"After you submit a load job to an FE, the FE generates a query plan, splits the query plan into portions based on the number of available BEs and the size of the data file you want to load, and then assigns each portion of the query plan to an available BE. During the load, each involved BE pulls the data of the data file from your external storage system, pre-processes the data, and then loads the data into your StarRocks cluster. After all BEs finish their portions of the query plan, the FE determines whether the load job is successful."}),"\n",(0,s.jsx)(n.p,{children:"The following figure shows the workflow of a Broker Load job."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Workflow of Broker Load",src:a(97619).Z+"",width:"1478",height:"1064"})}),"\n",(0,s.jsx)(n.h2,{id:"prepare-data-examples",children:"Prepare data examples"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Log in to your local file system and create two CSV-formatted data files, ",(0,s.jsx)(n.code,{children:"file1.csv"})," and ",(0,s.jsx)(n.code,{children:"file2.csv"}),". Both files consist of three columns, which represent user ID, user name, and user score in sequence."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.code,{children:"file1.csv"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain",children:"1,Lily,21\n2,Rose,22\n3,Alice,23\n4,Julia,24\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.code,{children:"file2.csv"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain",children:"5,Tony,25\n6,Adam,26\n7,Allen,27\n8,Jacky,28\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Upload ",(0,s.jsx)(n.code,{children:"file1.csv"})," and ",(0,s.jsx)(n.code,{children:"file2.csv"})," to the ",(0,s.jsx)(n.code,{children:"input"})," folder of your AWS S3 bucket ",(0,s.jsx)(n.code,{children:"bucket_s3"}),", to the ",(0,s.jsx)(n.code,{children:"input"})," folder of your Google GCS bucket ",(0,s.jsx)(n.code,{children:"bucket_gcs"}),", to the ",(0,s.jsx)(n.code,{children:"input"})," folder of your S3-compatible storage object (such as MinIO) bucket ",(0,s.jsx)(n.code,{children:"bucket_minio"}),", and to the specified paths of your Azure Storage."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Log in to your StarRocks database (for example, ",(0,s.jsx)(n.code,{children:"test_db"}),") and create two Primary Key tables, ",(0,s.jsx)(n.code,{children:"table1"})," and ",(0,s.jsx)(n.code,{children:"table2"}),". Both tables consist of three columns: ",(0,s.jsx)(n.code,{children:"id"}),", ",(0,s.jsx)(n.code,{children:"name"}),", and ",(0,s.jsx)(n.code,{children:"score"}),", of which ",(0,s.jsx)(n.code,{children:"id"})," is the primary key."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE TABLE `table1`\n   (\n       `id` int(11) NOT NULL COMMENT "user ID",\n       `name` varchar(65533) NULL DEFAULT "" COMMENT "user name",\n       `score` int(11) NOT NULL DEFAULT "0" COMMENT "user score"\n   )\n       ENGINE=OLAP\n       PRIMARY KEY(`id`)\n       DISTRIBUTED BY HASH(`id`);\n          \nCREATE TABLE `table2`\n   (\n       `id` int(11) NOT NULL COMMENT "user ID",\n       `name` varchar(65533) NULL DEFAULT "" COMMENT "user name",\n       `score` int(11) NOT NULL DEFAULT "0" COMMENT "user score"\n   )\n       ENGINE=OLAP\n       PRIMARY KEY(`id`)\n       DISTRIBUTED BY HASH(`id`);\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"load-data-from-aws-s3",children:"Load data from AWS S3"}),"\n",(0,s.jsxs)(n.p,{children:["Note that Broker Load supports accessing AWS S3 according to the S3 or S3A protocol. Therefore, when you load data from AWS S3, you can include ",(0,s.jsx)(n.code,{children:"s3://"})," or ",(0,s.jsx)(n.code,{children:"s3a://"})," as the prefix in the S3 URI that you pass as the file path (",(0,s.jsx)(n.code,{children:"DATA INFILE"}),")."]}),"\n",(0,s.jsxs)(n.p,{children:["Also, note that the following examples use the CSV file format and the instance profile-based authentication method. For information about how to load data in other formats and about the authentication parameters that you need to configure when using other authentication methods, see ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/BROKER%20LOAD",children:"BROKER LOAD"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"load-a-single-data-file-into-a-single-table",children:"Load a single data file into a single table"}),"\n",(0,s.jsx)(n.h4,{id:"example",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["Execute the following statement to load the data of ",(0,s.jsx)(n.code,{children:"file1.csv"})," stored in the ",(0,s.jsx)(n.code,{children:"input"})," folder of your AWS S3 bucket ",(0,s.jsx)(n.code,{children:"bucket_s3"})," into ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label_brokerloadtest_101\n(\n    DATA INFILE("s3a://bucket_s3/input/file1.csv")\n    INTO TABLE table1\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n)\nWITH BROKER\n(\n    "aws.s3.use_instance_profile" = "true",\n    "aws.s3.region" = "<aws_s3_region>"\n)\nPROPERTIES\n(\n    "timeout" = "3600"\n);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"query-data",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After you submit the load job, you can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsxs)(n.p,{children:["After you confirm that the load job is successful, you can use ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," to query the data of ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    21 |\n|    2 | Rose  |    22 |\n|    3 | Alice |    23 |\n|    4 | Julia |    24 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"load-multiple-data-files-into-a-single-table",children:"Load multiple data files into a single table"}),"\n",(0,s.jsx)(n.h4,{id:"example-1",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["Execute the following statement to load the data of all data files (",(0,s.jsx)(n.code,{children:"file1.csv"})," and ",(0,s.jsx)(n.code,{children:"file2.csv"}),") stored in the ",(0,s.jsx)(n.code,{children:"input"})," folder of your AWS S3 bucket ",(0,s.jsx)(n.code,{children:"bucket_s3"})," into ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label_brokerloadtest_102\n(\n    DATA INFILE("s3a://bucket_s3/input/*")\n    INTO TABLE table1\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n)\nWITH BROKER\n(\n    "aws.s3.use_instance_profile" = "true",\n    "aws.s3.region" = "<aws_s3_region>"\n)\nPROPERTIES\n(\n    "timeout" = "3600"\n);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"query-data-1",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After you submit the load job, you can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsxs)(n.p,{children:["After you confirm that the load job is successful, you can use ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," to query the data of ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    21 |\n|    2 | Rose  |    22 |\n|    3 | Alice |    23 |\n|    4 | Julia |    24 |\n|    5 | Tony  |    25 |\n|    6 | Adam  |    26 |\n|    7 | Allen |    27 |\n|    8 | Jacky |    28 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"load-multiple-data-files-into-multiple-tables",children:"Load multiple data files into multiple tables"}),"\n",(0,s.jsx)(n.h4,{id:"example-2",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["Execute the following statement to load the data of ",(0,s.jsx)(n.code,{children:"file1.csv"})," and ",(0,s.jsx)(n.code,{children:"file2.csv"})," stored in the ",(0,s.jsx)(n.code,{children:"input"})," folder of your AWS S3 bucket ",(0,s.jsx)(n.code,{children:"bucket_s3"})," into ",(0,s.jsx)(n.code,{children:"table1"})," and ",(0,s.jsx)(n.code,{children:"table2"}),", respectively:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label_brokerloadtest_103\n(\n    DATA INFILE("s3a://bucket_s3/input/file1.csv")\n    INTO TABLE table1\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n    ,\n    DATA INFILE("s3a://bucket_s3/input/file2.csv")\n    INTO TABLE table2\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n)\nWITH BROKER\n(\n    "aws.s3.use_instance_profile" = "true",\n    "aws.s3.region" = "<aws_s3_region>"\n)\nPROPERTIES\n(\n    "timeout" = "3600"\n);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"query-data-2",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After you submit the load job, you can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsxs)(n.p,{children:["After you confirm that the load job is successful, you can use ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," to query the data of ",(0,s.jsx)(n.code,{children:"table1"})," and ",(0,s.jsx)(n.code,{children:"table2"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Query ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    21 |\n|    2 | Rose  |    22 |\n|    3 | Alice |    23 |\n|    4 | Julia |    24 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Query ",(0,s.jsx)(n.code,{children:"table2"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table2;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    5 | Tony  |    25 |\n|    6 | Adam  |    26 |\n|    7 | Allen |    27 |\n|    8 | Jacky |    28 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"load-data-from-google-gcs",children:"Load data from Google GCS"}),"\n",(0,s.jsxs)(n.p,{children:["Note that Broker Load supports accessing Google GCS only according to the gs protocol. Therefore, when you load data from Google GCS, you must include ",(0,s.jsx)(n.code,{children:"gs://"})," as the prefix in the GCS URI that you pass as the file path (",(0,s.jsx)(n.code,{children:"DATA INFILE"}),")."]}),"\n",(0,s.jsxs)(n.p,{children:["Also, note that the following examples use the CSV file format and the VM-based authentication method. For information about how to load data in other formats and about the authentication parameters that you need to configure when using other authentication methods, see ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/BROKER%20LOAD",children:"BROKER LOAD"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"load-a-single-data-file-into-a-single-table-1",children:"Load a single data file into a single table"}),"\n",(0,s.jsx)(n.h4,{id:"example-3",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["Execute the following statement to load the data of ",(0,s.jsx)(n.code,{children:"file1.csv"})," stored in the ",(0,s.jsx)(n.code,{children:"input"})," folder of your Google GCS bucket ",(0,s.jsx)(n.code,{children:"bucket_gcs"})," into ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label_brokerloadtest_201\n(\n    DATA INFILE("gs://bucket_gcs/input/file1.csv")\n    INTO TABLE table1\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n)\nWITH BROKER\n(\n    "gcp.gcs.use_compute_engine_service_account" = "true"\n)\nPROPERTIES\n(\n    "timeout" = "3600"\n);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"query-data-3",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After you submit the load job, you can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsxs)(n.p,{children:["After you confirm that the load job is successful, you can use ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," to query the data of ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    21 |\n|    2 | Rose  |    22 |\n|    3 | Alice |    23 |\n|    4 | Julia |    24 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"load-multiple-data-files-into-a-single-table-1",children:"Load multiple data files into a single table"}),"\n",(0,s.jsx)(n.h4,{id:"example-4",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["Execute the following statement to load the data of all data files (",(0,s.jsx)(n.code,{children:"file1.csv"})," and ",(0,s.jsx)(n.code,{children:"file2.csv"}),") stored in the ",(0,s.jsx)(n.code,{children:"input"})," folder of your Google GCS bucket ",(0,s.jsx)(n.code,{children:"bucket_gcs"})," into ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label_brokerloadtest_202\n(\n    DATA INFILE("gs://bucket_gcs/input/*")\n    INTO TABLE table1\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n)\nWITH BROKER\n(\n    "gcp.gcs.use_compute_engine_service_account" = "true"\n)\nPROPERTIES\n(\n    "timeout" = "3600"\n);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"query-data-4",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After you submit the load job, you can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsxs)(n.p,{children:["After you confirm that the load job is successful, you can use ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," to query the data of ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    21 |\n|    2 | Rose  |    22 |\n|    3 | Alice |    23 |\n|    4 | Julia |    24 |\n|    5 | Tony  |    25 |\n|    6 | Adam  |    26 |\n|    7 | Allen |    27 |\n|    8 | Jacky |    28 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"load-multiple-data-files-into-multiple-tables-1",children:"Load multiple data files into multiple tables"}),"\n",(0,s.jsx)(n.h4,{id:"example-5",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["Execute the following statement to load the data of ",(0,s.jsx)(n.code,{children:"file1.csv"})," and ",(0,s.jsx)(n.code,{children:"file2.csv"})," stored in the ",(0,s.jsx)(n.code,{children:"input"})," folder of your Google GCS bucket ",(0,s.jsx)(n.code,{children:"bucket_gcs"})," into ",(0,s.jsx)(n.code,{children:"table1"})," and ",(0,s.jsx)(n.code,{children:"table2"}),", respectively:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label_brokerloadtest_203\n(\n    DATA INFILE("gs://bucket_gcs/input/file1.csv")\n    INTO TABLE table1\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n    ,\n    DATA INFILE("gs://bucket_gcs/input/file2.csv")\n    INTO TABLE table2\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n)\nWITH BROKER\n(\n    "gcp.gcs.use_compute_engine_service_account" = "true"\n);\nPROPERTIES\n(\n    "timeout" = "3600"\n);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"query-data-5",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After you submit the load job, you can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsxs)(n.p,{children:["After you confirm that the load job is successful, you can use ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," to query the data of ",(0,s.jsx)(n.code,{children:"table1"})," and ",(0,s.jsx)(n.code,{children:"table2"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Query ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    21 |\n|    2 | Rose  |    22 |\n|    3 | Alice |    23 |\n|    4 | Julia |    24 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Query ",(0,s.jsx)(n.code,{children:"table2"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table2;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    5 | Tony  |    25 |\n|    6 | Adam  |    26 |\n|    7 | Allen |    27 |\n|    8 | Jacky |    28 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"load-data-from-microsoft-azure-storage",children:"Load data from Microsoft Azure Storage"}),"\n",(0,s.jsx)(n.p,{children:"Note that when you load data from Azure Storage, you need to determine which prefix to use based on the access protocol and specific storage service that you use:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["When you load data from Blob Storage, you must include ",(0,s.jsx)(n.code,{children:"wasb://"})," or ",(0,s.jsx)(n.code,{children:"wasbs://"})," as a prefix in the file path (",(0,s.jsx)(n.code,{children:"DATA INFILE"}),") based on the protocol that is used to access your storage account:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["If your Blob Storage allows access only through HTTP, use ",(0,s.jsx)(n.code,{children:"wasb://"})," as the prefix, for example, ",(0,s.jsx)(n.code,{children:"wasb://<container>@<storage_account>.blob.core.windows.net/<path>/<file_name>/*"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["If your Blob Storage allows access only through HTTPS, use ",(0,s.jsx)(n.code,{children:"wasbs://"})," as the prefix, for example, ",(0,s.jsx)(n.code,{children:"wasbs://<container>@<storage_account>.blob.core.windows``.net/<path>/<file_name>/*"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["When you load data from Data Lake Storage Gen1, you must include ",(0,s.jsx)(n.code,{children:"adl://"})," as a prefix in the file path (",(0,s.jsx)(n.code,{children:"DATA INFILE"}),"), for example, ",(0,s.jsx)(n.code,{children:"adl://<data_lake_storage_gen1_name>.azuredatalakestore.net/<path>/<file_name>"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["When you load data from Data Lake Storage Gen2, you must include ",(0,s.jsx)(n.code,{children:"abfs://"})," or ",(0,s.jsx)(n.code,{children:"abfss://"})," as a prefix in the file path (",(0,s.jsx)(n.code,{children:"DATA INFILE"}),") based on the protocol that is used to access your storage account:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["If your Data Lake Storage Gen2 allows access only via HTTP, use ",(0,s.jsx)(n.code,{children:"abfs://"})," as the prefix, for example, ",(0,s.jsx)(n.code,{children:"abfs://<container>@<storage_account>.dfs.core.windows.net/<file_name>"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["If your Data Lake Storage Gen2 allows access only via HTTPS, use ",(0,s.jsx)(n.code,{children:"abfss://"})," as the prefix, for example, ",(0,s.jsx)(n.code,{children:"abfss://<container>@<storage_account>.dfs.core.windows.net/<file_name>"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Also, note that the following examples use the CSV file format, Azure Blob Storage, and the shared key-based authentication method. For information about how to load data in other formats and about the authentication parameters that you need to configure when using other Azure storage services and authentication methods, see ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/BROKER%20LOAD",children:"BROKER LOAD"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"load-a-single-data-file-into-a-single-table-2",children:"Load a single data file into a single table"}),"\n",(0,s.jsx)(n.h4,{id:"example-6",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["Execute the following statement to load the data of ",(0,s.jsx)(n.code,{children:"file1.csv"})," stored in the specified path of your Azure Storage into ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label_brokerloadtest_301\n(\n    DATA INFILE("wasb[s]://<container>@<storage_account>.blob.core.windows.net/<path>/file1.csv")\n    INTO TABLE table1\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n)\nWITH BROKER\n(\n    "azure.blob.storage_account" = "<blob_storage_account_name>",\n    "azure.blob.shared_key" = "<blob_storage_account_shared_key>"\n)\nPROPERTIES\n(\n    "timeout" = "3600"\n);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"query-data-6",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After you submit the load job, you can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsxs)(n.p,{children:["After you confirm that the load job is successful, you can use ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," to query the data of ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    21 |\n|    2 | Rose  |    22 |\n|    3 | Alice |    23 |\n|    4 | Julia |    24 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"load-multiple-data-files-into-a-single-table-2",children:"Load multiple data files into a single table"}),"\n",(0,s.jsx)(n.h4,{id:"example-7",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["Execute the following statement to load the data of all data files (",(0,s.jsx)(n.code,{children:"file1.csv"})," and ",(0,s.jsx)(n.code,{children:"file2.csv"}),") stored in the specified path of your Azure Storage into ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label_brokerloadtest_302\n(\n    DATA INFILE("wasb[s]://<container>@<storage_account>.blob.core.windows.net/<path>/*")\n    INTO TABLE table1\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n)\nWITH BROKER\n(\n    "azure.blob.storage_account" = "<blob_storage_account_name>",\n    "azure.blob.shared_key" = "<blob_storage_account_shared_key>"\n)\nPROPERTIES\n(\n    "timeout" = "3600"\n);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"query-data-7",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After you submit the load job, you can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsxs)(n.p,{children:["After you confirm that the load job is successful, you can use ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," to query the data of ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    21 |\n|    2 | Rose  |    22 |\n|    3 | Alice |    23 |\n|    4 | Julia |    24 |\n|    5 | Tony  |    25 |\n|    6 | Adam  |    26 |\n|    7 | Allen |    27 |\n|    8 | Jacky |    28 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"load-multiple-data-files-into-multiple-tables-2",children:"Load multiple data files into multiple tables"}),"\n",(0,s.jsx)(n.h4,{id:"example-8",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["Execute the following statement to load the data of ",(0,s.jsx)(n.code,{children:"file1.csv"})," and ",(0,s.jsx)(n.code,{children:"file2.csv"})," stored in the specified path of your Azure Storage into ",(0,s.jsx)(n.code,{children:"table1"})," and ",(0,s.jsx)(n.code,{children:"table2"}),", respectively:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label_brokerloadtest_303\n(\n    DATA INFILE("wasb[s]://<container>@<storage_account>.blob.core.windows.net/<path>/file1.csv")\n    INTO TABLE table1\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n    ,\n    DATA INFILE("wasb[s]://<container>@<storage_account>.blob.core.windows.net/<path>/file2.csv")\n    INTO TABLE table2\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n)\nWITH BROKER\n(\n    "azure.blob.storage_account" = "<blob_storage_account_name>",\n    "azure.blob.shared_key" = "<blob_storage_account_shared_key>"\n);\nPROPERTIES\n(\n    "timeout" = "3600"\n);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"query-data-8",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After you submit the load job, you can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsxs)(n.p,{children:["After you confirm that the load job is successful, you can use ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," to query the data of ",(0,s.jsx)(n.code,{children:"table1"})," and ",(0,s.jsx)(n.code,{children:"table2"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Query ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    21 |\n|    2 | Rose  |    22 |\n|    3 | Alice |    23 |\n|    4 | Julia |    24 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Query ",(0,s.jsx)(n.code,{children:"table2"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table2;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    5 | Tony  |    25 |\n|    6 | Adam  |    26 |\n|    7 | Allen |    27 |\n|    8 | Jacky |    28 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"load-data-from-an-s3-compatible-storage-system",children:"Load data from an S3-compatible storage system"}),"\n",(0,s.jsxs)(n.p,{children:["The following examples use the CSV file format and the MinIO storage system. For information about how to load data in other formats, see ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/BROKER%20LOAD",children:"BROKER LOAD"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"load-a-single-data-file-into-a-single-table-3",children:"Load a single data file into a single table"}),"\n",(0,s.jsx)(n.h4,{id:"example-9",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["Execute the following statement to load the data of ",(0,s.jsx)(n.code,{children:"file1.csv"})," stored in the ",(0,s.jsx)(n.code,{children:"input"})," folder of your MinIO bucket ",(0,s.jsx)(n.code,{children:"bucket_minio"})," into ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label_brokerloadtest_401\n(\n    DATA INFILE("obs://bucket_minio/input/file1.csv")\n    INTO TABLE table1\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n)\nWITH BROKER\n(\n    "aws.s3.enable_ssl" = "false",\n    "aws.s3.enable_path_style_access" = "true",\n    "aws.s3.endpoint" = "<s3_endpoint>",\n    "aws.s3.access_key" = "<iam_user_access_key>",\n    "aws.s3.secret_key" = "<iam_user_secret_key>"\n)\nPROPERTIES\n(\n    "timeout" = "3600"\n);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"query-data-9",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After you submit the load job, you can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsxs)(n.p,{children:["After you confirm that the load job is successful, you can use ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," to query the data of ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    21 |\n|    2 | Rose  |    22 |\n|    3 | Alice |    23 |\n|    4 | Julia |    24 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"load-multiple-data-files-into-a-single-table-3",children:"Load multiple data files into a single table"}),"\n",(0,s.jsx)(n.h4,{id:"example-10",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["Execute the following statement to load the data of all data files (",(0,s.jsx)(n.code,{children:"file1.csv"})," and ",(0,s.jsx)(n.code,{children:"file2.csv"}),") stored in the ",(0,s.jsx)(n.code,{children:"input"})," folder of your MinIO bucket ",(0,s.jsx)(n.code,{children:"bucket_minio"})," into ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label_brokerloadtest_402\n(\n    DATA INFILE("obs://bucket_minio/input/*")\n    INTO TABLE table1\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n)\nWITH BROKER\n(\n    "aws.s3.enable_ssl" = "false",\n    "aws.s3.enable_path_style_access" = "true",\n    "aws.s3.endpoint" = "<s3_endpoint>",\n    "aws.s3.access_key" = "<iam_user_access_key>",\n    "aws.s3.secret_key" = "<iam_user_secret_key>"\n)\nPROPERTIES\n(\n    "timeout" = "3600"\n);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"query-data-10",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After you submit the load job, you can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsxs)(n.p,{children:["After you confirm that the load job is successful, you can use ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," to query the data of ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    21 |\n|    2 | Rose  |    22 |\n|    3 | Alice |    23 |\n|    4 | Julia |    24 |\n|    5 | Tony  |    25 |\n|    6 | Adam  |    26 |\n|    7 | Allen |    27 |\n|    8 | Jacky |    28 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"load-multiple-data-files-into-multiple-tables-3",children:"Load multiple data files into multiple tables"}),"\n",(0,s.jsx)(n.h4,{id:"example-11",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["Execute the following statement to load the data of all data files (",(0,s.jsx)(n.code,{children:"file1.csv"})," and ",(0,s.jsx)(n.code,{children:"file2.csv"}),") stored in the ",(0,s.jsx)(n.code,{children:"input"})," folder of your MinIO bucket ",(0,s.jsx)(n.code,{children:"bucket_minio"})," into ",(0,s.jsx)(n.code,{children:"table1"})," and ",(0,s.jsx)(n.code,{children:"table2"}),", respectively:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label_brokerloadtest_403\n(\n    DATA INFILE("obs://bucket_minio/input/file1.csv")\n    INTO TABLE table1\n    COLUMNS TERMINATED BY ","\n    (id, name, score)\n    ,\n    DATA INFILE("obs://bucket_minio/input/file2.csv")\n    INTO TABLE table2\n    COLUMNS TERMINATED BY ","\n    (id, city)\n)\nWITH BROKER\n(\n    "aws.s3.enable_ssl" = "false",\n    "aws.s3.enable_path_style_access" = "true",\n    "aws.s3.endpoint" = "<s3_endpoint>",\n    "aws.s3.access_key" = "<iam_user_access_key>",\n    "aws.s3.secret_key" = "<iam_user_secret_key>"\n);\nPROPERTIES\n(\n    "timeout" = "3600"\n);\n'})}),"\n",(0,s.jsx)(n.h4,{id:"query-data-11",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After you submit the load job, you can use ",(0,s.jsx)(n.code,{children:"SELECT * FROM information_schema.loads"}),' to query the job result. This feature is supported from v3.1 onwards. For more information, see the "',(0,s.jsx)(n.a,{href:"#view-a-load-job",children:"View a load job"}),'" section of this topic.']}),"\n",(0,s.jsxs)(n.p,{children:["After you confirm that the load job is successful, you can use ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," to query the data of ",(0,s.jsx)(n.code,{children:"table1"})," and ",(0,s.jsx)(n.code,{children:"table2"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Query ",(0,s.jsx)(n.code,{children:"table1"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    21 |\n|    2 | Rose  |    22 |\n|    3 | Alice |    23 |\n|    4 | Julia |    24 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Query ",(0,s.jsx)(n.code,{children:"table2"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table2;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    5 | Tony  |    25 |\n|    6 | Adam  |    26 |\n|    7 | Allen |    27 |\n|    8 | Jacky |    28 |\n+------+-------+-------+\n4 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"view-a-load-job",children:"View a load job"}),"\n",(0,s.jsxs)(n.p,{children:["Use the ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/SELECT",children:"SELECT"})," statement to query the results of one or more load jobs from the ",(0,s.jsx)(n.code,{children:"loads"})," table in the ",(0,s.jsx)(n.code,{children:"information_schema"})," database. This feature is supported from v3.1 onwards."]}),"\n",(0,s.jsxs)(n.p,{children:["Example 1: Query the results of load jobs executed on the ",(0,s.jsx)(n.code,{children:"test_db"})," database. In the query statement, specify that a maximum of two results can be returned and the return results must be sorted by creation time (",(0,s.jsx)(n.code,{children:"CREATE_TIME"}),") in descending order."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM information_schema.loads\nWHERE database_name = 'test_db'\nORDER BY create_time DESC\nLIMIT 2\\G\n"})}),"\n",(0,s.jsx)(n.p,{children:"The following results are returned:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'*************************** 1. row ***************************\n              JOB_ID: 20686\n               LABEL: label_brokerload_unqualifiedtest_83\n       DATABASE_NAME: test_db\n               STATE: FINISHED\n            PROGRESS: ETL:100%; LOAD:100%\n                TYPE: BROKER\n            PRIORITY: NORMAL\n           SCAN_ROWS: 8\n       FILTERED_ROWS: 0\n     UNSELECTED_ROWS: 0\n           SINK_ROWS: 8\n            ETL_INFO:\n           TASK_INFO: resource:N/A; timeout(s):14400; max_filter_ratio:1.0\n         CREATE_TIME: 2023-08-02 15:25:22\n      ETL_START_TIME: 2023-08-02 15:25:24\n     ETL_FINISH_TIME: 2023-08-02 15:25:24\n     LOAD_START_TIME: 2023-08-02 15:25:24\n    LOAD_FINISH_TIME: 2023-08-02 15:25:27\n         JOB_DETAILS: {"All backends":{"77fe760e-ec53-47f7-917d-be5528288c08":[10006],"0154f64e-e090-47b7-a4b2-92c2ece95f97":[10005]},"FileNumber":2,"FileSize":84,"InternalTableLoadBytes":252,"InternalTableLoadRows":8,"ScanBytes":84,"ScanRows":8,"TaskNumber":2,"Unfinished backends":{"77fe760e-ec53-47f7-917d-be5528288c08":[],"0154f64e-e090-47b7-a4b2-92c2ece95f97":[]}}\n           ERROR_MSG: NULL\n        TRACKING_URL: NULL\n        TRACKING_SQL: NULL\nREJECTED_RECORD_PATH: NULL\n*************************** 2. row ***************************\n              JOB_ID: 20624\n               LABEL: label_brokerload_unqualifiedtest_82\n       DATABASE_NAME: test_db\n               STATE: FINISHED\n            PROGRESS: ETL:100%; LOAD:100%\n                TYPE: BROKER\n            PRIORITY: NORMAL\n           SCAN_ROWS: 12\n       FILTERED_ROWS: 4\n     UNSELECTED_ROWS: 0\n           SINK_ROWS: 8\n            ETL_INFO:\n           TASK_INFO: resource:N/A; timeout(s):14400; max_filter_ratio:1.0\n         CREATE_TIME: 2023-08-02 15:23:29\n      ETL_START_TIME: 2023-08-02 15:23:34\n     ETL_FINISH_TIME: 2023-08-02 15:23:34\n     LOAD_START_TIME: 2023-08-02 15:23:34\n    LOAD_FINISH_TIME: 2023-08-02 15:23:34\n         JOB_DETAILS: {"All backends":{"78f78fc3-8509-451f-a0a2-c6b5db27dcb6":[10010],"a24aa357-f7de-4e49-9e09-e98463b5b53c":[10006]},"FileNumber":2,"FileSize":158,"InternalTableLoadBytes":333,"InternalTableLoadRows":8,"ScanBytes":158,"ScanRows":12,"TaskNumber":2,"Unfinished backends":{"78f78fc3-8509-451f-a0a2-c6b5db27dcb6":[],"a24aa357-f7de-4e49-9e09-e98463b5b53c":[]}}\n           ERROR_MSG: NULL\n        TRACKING_URL: http://172.26.195.69:8540/api/_load_error_log?file=error_log_78f78fc38509451f_a0a2c6b5db27dcb7\n        TRACKING_SQL: select tracking_log from information_schema.load_tracking_logs where job_id=20624\nREJECTED_RECORD_PATH: 172.26.95.92:/home/disk1/sr/be/storage/rejected_record/test_db/label_brokerload_unqualifiedtest_0728/6/404a20b1e4db4d27_8aa9af1e8d6d8bdc\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Example 2: Query the result of the load job (whose label is ",(0,s.jsx)(n.code,{children:"label_brokerload_unqualifiedtest_82"}),") executed on the ",(0,s.jsx)(n.code,{children:"test_db"})," database:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM information_schema.loads\nWHERE database_name = 'test_db' and label = 'label_brokerload_unqualifiedtest_82'\\G\n"})}),"\n",(0,s.jsx)(n.p,{children:"The following result is returned:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'*************************** 1. row ***************************\n              JOB_ID: 20624\n               LABEL: label_brokerload_unqualifiedtest_82\n       DATABASE_NAME: test_db\n               STATE: FINISHED\n            PROGRESS: ETL:100%; LOAD:100%\n                TYPE: BROKER\n            PRIORITY: NORMAL\n           SCAN_ROWS: 12\n       FILTERED_ROWS: 4\n     UNSELECTED_ROWS: 0\n           SINK_ROWS: 8\n            ETL_INFO:\n           TASK_INFO: resource:N/A; timeout(s):14400; max_filter_ratio:1.0\n         CREATE_TIME: 2023-08-02 15:23:29\n      ETL_START_TIME: 2023-08-02 15:23:34\n     ETL_FINISH_TIME: 2023-08-02 15:23:34\n     LOAD_START_TIME: 2023-08-02 15:23:34\n    LOAD_FINISH_TIME: 2023-08-02 15:23:34\n         JOB_DETAILS: {"All backends":{"78f78fc3-8509-451f-a0a2-c6b5db27dcb6":[10010],"a24aa357-f7de-4e49-9e09-e98463b5b53c":[10006]},"FileNumber":2,"FileSize":158,"InternalTableLoadBytes":333,"InternalTableLoadRows":8,"ScanBytes":158,"ScanRows":12,"TaskNumber":2,"Unfinished backends":{"78f78fc3-8509-451f-a0a2-c6b5db27dcb6":[],"a24aa357-f7de-4e49-9e09-e98463b5b53c":[]}}\n           ERROR_MSG: NULL\n        TRACKING_URL: http://172.26.195.69:8540/api/_load_error_log?file=error_log_78f78fc38509451f_a0a2c6b5db27dcb7\n        TRACKING_SQL: select tracking_log from information_schema.load_tracking_logs where job_id=20624\nREJECTED_RECORD_PATH: 172.26.95.92:/home/disk1/sr/be/storage/rejected_record/test_db/label_brokerload_unqualifiedtest_0728/6/404a20b1e4db4d27_8aa9af1e8d6d8bdc\n'})}),"\n",(0,s.jsxs)(n.p,{children:["For information about the fields in the return results, see ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/administration/information_schema#loads",children:"Information Schema > loads"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"cancel-a-load-job",children:"Cancel a load job"}),"\n",(0,s.jsxs)(n.p,{children:["When a load job is not in the ",(0,s.jsx)(n.strong,{children:"CANCELLED"})," or ",(0,s.jsx)(n.strong,{children:"FINISHED"})," stage, you can use the ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/sql-reference/sql-statements/data-manipulation/CANCEL%20LOAD",children:"CANCEL LOAD"})," statement to cancel the job."]}),"\n",(0,s.jsxs)(n.p,{children:["For example, you can execute the following statement to cancel a load job, whose label is ",(0,s.jsx)(n.code,{children:"label1"}),", in the database ",(0,s.jsx)(n.code,{children:"test_db"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CANCEL LOAD\nFROM test_db\nWHERE LABEL = "label";\n'})}),"\n",(0,s.jsx)(n.h2,{id:"job-splitting-and-concurrent-running",children:"Job splitting and concurrent running"}),"\n",(0,s.jsxs)(n.p,{children:["A Broker Load job can be split into one or more tasks that concurrently run. The tasks within a load job are run within a single transaction. They must all succeed or fail. StarRocks splits each load job based on how you declare ",(0,s.jsx)(n.code,{children:"data_desc"})," in the ",(0,s.jsx)(n.code,{children:"LOAD"})," statement:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If you declare multiple ",(0,s.jsx)(n.code,{children:"data_desc"})," parameters, each of which specifies a distinct table, a task is generated to load the data of each table."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If you declare multiple ",(0,s.jsx)(n.code,{children:"data_desc"})," parameters, each of which specifies a distinct partition for the same table, a task is generated to load the data of each partition."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Additionally, each task can be further split into one or more instances, which are evenly distributed to and concurrently run on the BEs of your StarRocks cluster. StarRocks splits each task based on the following ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/administration/Configuration#fe-configuration-items",children:"FE configurations"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"min_bytes_per_broker_scanner"}),": the minimum amount of data processed by each instance. The default amount is 64 MB."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"load_parallel_instance_num"}),": the number of concurrent instances allowed in each load job on an individual BE. The default number is 1."]}),"\n",(0,s.jsx)(n.p,{children:"You can use the following formula to calculate the number of instances in an individual task:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.strong,{children:["Number of instances in an individual task = min(Amount of data to be loaded by an individual task/",(0,s.jsx)(n.code,{children:"min_bytes_per_broker_scanner"}),",",(0,s.jsx)(n.code,{children:"load_parallel_instance_num"})," x Number of BEs)"]})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In most cases, only one ",(0,s.jsx)(n.code,{children:"data_desc"})," is declared for each load job, each load job is split into only one task, and the task is split into the same number of instances as the number of BEs."]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsxs)(n.p,{children:["See ",(0,s.jsx)(n.a,{href:"/docusaurusv3/docs/faq/loading/Broker_load_faq",children:"Broker Load FAQ"}),"."]})]})}const u=function(e={}){const{wrapper:n}=Object.assign({},(0,o.ah)(),e.components);return n?(0,s.jsx)(n,Object.assign({},e,{children:(0,s.jsx)(h,e)})):h(e)}},97619:(e,n,a)=>{a.d(n,{Z:()=>s});const s=a.p+"assets/images/broker_load_how-to-work_en-6132d562d6e87eac4b5c0e494ffdb34d.png"},11151:(e,n,a)=>{a.d(n,{Zo:()=>i,ah:()=>t});var s=a(67294);const o=s.createContext({});function t(e){const n=s.useContext(o);return s.useMemo((()=>"function"==typeof e?e(n):{...n,...e}),[n,e])}const l={};function i({components:e,children:n,disableParentContext:a}){let i;return i=a?"function"==typeof e?e({}):e||l:t(e),s.createElement(o.Provider,{value:i},n)}}}]);