"use strict";(self.webpackChunkstarrocks=self.webpackChunkstarrocks||[]).push([[39590],{39606:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var n=a(85893),o=a(11151);const r={displayed_sidebar:"English"},s="STREAM LOAD",i={id:"sql-reference/sql-statements/data-manipulation/STREAM_LOAD",title:"STREAM LOAD",description:"description",source:"@site/versioned_docs/version-2.1/sql-reference/sql-statements/data-manipulation/STREAM_LOAD.md",sourceDirName:"sql-reference/sql-statements/data-manipulation",slug:"/sql-reference/sql-statements/data-manipulation/STREAM_LOAD",permalink:"/docs/2.1/sql-reference/sql-statements/data-manipulation/STREAM_LOAD",draft:!1,unlisted:!1,editUrl:"https://github.com/StarRocks/starrocks/edit/main/docs/sql-reference/sql-statements/data-manipulation/STREAM_LOAD.md",tags:[],version:"2.1",frontMatter:{displayed_sidebar:"English"},sidebar:"English",previous:{title:"STOP ROUTINE LOAD",permalink:"/docs/2.1/sql-reference/sql-statements/data-manipulation/STOP_ROUTINE_LOAD"},next:{title:"ALTER ROUTINE LOAD",permalink:"/docs/2.1/sql-reference/sql-statements/data-manipulation/alter-routine-load"}},l={},c=[{value:"description",id:"description",level:2},{value:"example",id:"example",level:2},{value:"keyword",id:"keyword",level:2}];function h(e){const t=Object.assign({h1:"h1",h2:"h2",pre:"pre",code:"code",p:"p",ol:"ol",li:"li"},(0,o.ah)(),e.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"stream-load",children:"STREAM LOAD"}),"\n",(0,n.jsx)(t.h2,{id:"description",children:"description"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-plain",metastring:"text",children:'NAME:\nstream-load: load data to table in streaming\n\nSYNOPSIS\ncurl --location-trusted -u user:passwd [-H ""...] -T data.file -XPUT \\\n    http://fe_host:http_port/api/{db}/{table}/_stream_load\n\nDESCRIPTION\nThis statement is used to import data into the specified table. The difference from normal Load is that this import method is synchronous import.\nThis import method can still ensure the atomicity of a batch of import tasks. Either all data is imported successfully or all data fails.\nThis operation will update the data of rollup table related to this base table at the same time.\nThis is a synchronous operation. After the entire data import is completed, the import results will be returned to the user.\nCurrently, HTTP chunked and non chunked uploads are supported. For non chunked uploads, Content_Length must be used to indicate the length of the uploaded content, so as to ensure the integrity of the data.\nIn addition, you\'d better set the Expected Header field content 100_ continue to avoid unnecessary data transmission in some error scenarios.\n'})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-plain",metastring:"text",children:'OPTIONS\nUsers can pass in import parameters through the Header part of HTTP\n\nlabel: label imported at one time. Data of the same label cannot be imported multiple times. Users can avoid the problem of repeated import of a copy of data by specifying a Label.\nCurrently, StarRocks keeps the most recently successful label within 30 minutes.\n\ncolumn_separator\uff1aused to specify the column separator in the import file. The default is \\t. If it is an invisible character, you need to prefix it with \\x and use hexadecimal to represent the separator.\nFor example, the separator of hive file \\x01 should be specified as - H "column_separator: \\x01"\n\ncolumns\uff1aused to specify the correspondence between the columns in the import file and the columns in the table. If the column in the source file exactly corresponds to the content in the table, you do not need to specify the content of this field.\nIf the source file does not correspond to the table schema, this field needs some data conversion. There are two forms of column. One is to directly correspond to the fields in the import file, which are directly represented by the field name;\nOne is derived column, and the syntax is column - name = expression. Give a few examples to help understand.\nExample 1: there are three columns "c1, c2, c3" in the table, and the three columns in the source file correspond to "c3, c2, c1" at one time; Then you need to specify - H "columns: c3, c2, c1"\nExample 2: there are three columns "c1, c2, c3" in the table. The first three columns in the source file correspond in turn, but there is more than one column. Then you need to specify - H "columns: c1, c2, c3, XXX";\nThe last column can be filled with any name\nExample 3: there are three columns "year, month and day" in the table, and there is only one time column in the source file in the format of "2018-06-01 01:02:03";\nThen you can specify - H "columns: column, year = year (column), month = month (column), day = day (Col)" to complete the import\n\nwhere: used to extract some data. If users need to filter out unwanted data, they can set this option.\nExample 1: if you only import data with columns greater than K1 and equal to 20180601, you can specify - H "where: k1 = 20180601" during import\n\nmax_filter_ratio\uff1athe maximum allowable data proportion that can be filtered (due to data irregularity, etc.). The default is zero tolerance. Data irregularity does not include rows filtered through the where condition.\n\npartitions: used to specify the partition designed for this import. If the user can determine the partition corresponding to the data, it is recommended to specify this item. Data that does not meet these partitions will be filtered out.\nFor example, specify to import to p1, p2 partitions, - H "partitions: p1, p2"\n\ntimeout: Specifies the timeout of import. The unit is seconds. The default is 600 seconds. The settable range is 1 second ~ 259200 seconds.\n\nstrict_mode: the user specifies whether strict mode is enabled for this import. It is off by default. The enabling method is - H "strict_mode: true".\n\ntimezone: Specifies the time zone used for this import. The default is Dongba zone. This parameter will affect the results of all functions related to time zone involved in import.\n\nexec_mem_limit: import memory limit. The default is 2GB. The unit is bytes.\n\nformat: Specifies the import data format. The default is csv. json format is supported.\n'})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-PALIN",metastring:"TEXT",children:'jsonpaths: there are two ways to import json: simple mode and precise mode.\nSimple mode: it is a simple mode without setting the jsonpaths parameter. In this mode, json data is required to be an object type, for example:\n{"k1": 1, "k2": 2, "k3": "hello"}, where k1, k2 and k3 are column names.\n\nMatching pattern: the json data is relatively complex, and the corresponding value needs to be matched through the jsonpaths parameter.\n\nstrip_ outer_ array: Boolean type. true means that json data starts with an array object and flattens the array object. The default value is false. For example:\n[\n{"k1" : 1, "v1" : 2},\n{"k1" : 3, "v1" : 4}\n]\nWhen strip_ outer_ array is true, and two rows of data will be generated when it is finally imported into starrocks.\n\njson_ root: json_ root is a legal jsonpath string, which is used to specify the root node of json document. The default value is\' \'.\n\nRETURN VALUES\nAfter the import is completed, the relevant contents of the import will be returned in Json format. The following fields are currently included:\nStatus: imports the last status.\nSuccess: indicates that the import is successful and the data is visible;\nPublish Timeout: indicates that the import job has been successfully committed, but it cannot be seen immediately for some reason. The user can be regarded as having succeeded without having to retry the import\nLabel Already Exists: indicates that the Label has been occupied by other jobs. It may be imported successfully or being imported.\nUsers need to use the get label state command to determine subsequent operations\nOther: the import failed. The user can specify Label to retry the job\nMessage: detailed description of import status. The specific failure reason will be returned in case of failure.\nNumberTotalRows: the total number of rows read from the data stream\nNumberLoadedRows: the number of data rows imported this time, which is valid only when Success\nNumberFilteredRows: the number of rows filtered in this import, that is, the number of rows with unqualified data quality\nNumberunselectedrows: the number of rows filtered through the where condition in this import\nLoadBytes: the amount of source file data imported this time\nLoadTimeMs: time taken for this import\nErrorURL: the specific content of the filtered data. Only the first 1000 items are reserved\n'})}),"\n",(0,n.jsx)(t.p,{children:"ERRORS\nErrors you can view the import error details through the following statement:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-SQL",children:"SHOW LOAD WARNINGS ON 'url'\n"})}),"\n",(0,n.jsx)(t.p,{children:"Where url is the url given by ErrorURL."}),"\n",(0,n.jsx)(t.h2,{id:"example",children:"example"}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Import the data in the local file 'testData' into the table 'testTbl' in the database 'testDb', and use label for deduplication. Specify a timeout of 100 seconds"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:'curl --location-trusted -u root -H "label:123" -H "timeout:100" -T testData \\\n    http://host:port/api/testDb/testTbl/_stream_load\n'})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Import the data in the local file 'testData' into the table testTbl' in the database 'testDb', use label for de duplication, and only import the data with k1 equal to 20180601"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:'curl --location-trusted -u root -H "label:123" -H "where: k1=20180601" -T testData \\\n    http://host:port/api/testDb/testTbl/_stream_load\n'})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Import the data in the local file 'testData' into the table 'testTbl' in the database 'testDb', allowing an error rate of 20% (the user is in the defalut_cluster)"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:'curl --location-trusted -u root -H "label:123" -H "max_filter_ratio:0.2" -T testData \\\n    http://host:port/api/testDb/testTbl/_stream_load\n'})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Import the data in the local file 'testData' into the table 'testTbl' in the database 'testDb', allow an error rate of 20%, and specify the column name of the file (the user is in the defalut_cluster)"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:'curl --location-trusted -u root  -H "label:123" -H "max_filter_ratio:0.2" \\\n    -H "columns: k2, k1, v1" -T testData \\\n    http://host:port/api/testDb/testTbl/_stream_load\n'})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Import the data in the local file 'testData' into the p1 and p2 partitions in the table 'testTbl' in the database 'testDb', allowing an error rate of 20%."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:'curl --location-trusted -u root  -H "label:123" -H "max_filter_ratio:0.2" \\\n    -H "partitions: p1, p2" -T testData \\\n    http://host:port/api/testDb/testTbl/_stream_load\n'})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Import using streaming (the user is from the default_cluster)"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-sql",children:"seq 1 10 | awk '{OFS=\"\\t\"}{print $1, $1 * 10}' | curl --location-trusted -u root -T - \\\n http://host:port/api/testDb/testTbl/_stream_load\n"})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Import a table containing HLL columns, which can be columns in the table or columns in the data. It can be used to generate HLL columns, or use hll_ empty to supplement columns that are not in the data"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:'curl --location-trusted -u root \\\n    -H "columns: k1, k2, v1=hll_hash(k1), v2=hll_empty()" -T testData \\\n    http://host:port/api/testDb/testTbl/_stream_load\n'})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Import data for strict mode filtering, and set the time zone to Africa / Abidjan"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:'curl --location-trusted -u root -H "strict_mode: true" \\\n    -H "timezone: Africa/Abidjan" -T testData \\\n    http://host:port/api/testDb/testTbl/_stream_load\n'})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Import a table containing BITMAP columns, which can be columns in the table or columns in the data. It can be used to generate BITMAP columns or use bitmap_ empty to fill"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:'curl --location-trusted -u root \\\n    -H "columns: k1, k2, v1=to_bitmap(k1), v2=bitmap_empty()" -T testData \\\n    http://host:port/api/testDb/testTbl/_stream_load\n'})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Simple mode, importing json data"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-plain",metastring:"text",children:'Table structure:\n`category` varchar(512) NULL COMMENT "",\n`author` varchar(512) NULL COMMENT "",\n`title` varchar(512) NULL COMMENT "",\n`price` double NULL COMMENT ""\njson data format:\n{"category":"C++","author":"avc","title":"C++ primer","price":895}\nImport command:\ncurl --location-trusted -u root  -H "label:123" -H "format: json" -T testData \\\n    http://host:port/api/testDb/testTbl/_stream_load\nIn order to improve throughput, it supports one-time import of data. json data format:\n[\n{"category":"C++","author":"avc","title":"C++ primer","price":89.5},\n{"category":"Java","author":"avc","title":"Effective Java","price":95},\n{"category":"Linux","author":"avc","title":"Linux kernel","price":195}\n]\n'})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Matching patterns, importing json data"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-plain",metastring:"text",children:'json data format:\n[\n{"category":"xuxb111","author":"1avc","title":"SayingsoftheCentury","price":895},\n{"category":"xuxb222","author":"2avc","title":"SayingsoftheCentury","price":895},\n{"category":"xuxb333","author":"3avc","title":"SayingsoftheCentury","price":895}\n]\nImport precisely by specifying jsonpath. For example, only category, author and price attributes are imported\ncurl --location-trusted -u root \\\n    -H "columns: category, price, author" -H "label:123" -H "format: json" -H "jsonpaths: [\\"$.category\\",\\"$.price\\",\\"$.author\\"]" -H "strip_outer_array: true" -T testData \\\n    http://host:port/api/testDb/testTbl/_stream_load\n'})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-plain",metastring:"text",children:"Note\uff1a\n1) If the json data starts with an array and each object in the array is a record, you need to set strip_ outer_ array to true to flatten the array.\n2) If the json data starts with an array and each object in the array is a record, when setting the jsonpath, our ROOT node is actually an object in the array.\n"})}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"User specified json root node"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-plain",metastring:"text",children:'json data format:\n{\n"RECORDS":[\n{"category":"11","title":"SayingsoftheCentury","price":895,"timestamp":1589191587},\n{"category":"22","author":"2avc","price":895,"timestamp":1589191487},\n{"category":"33","author":"3avc","title":"SayingsoftheCentury","timestamp":1589191387}\n]\n}\nImport precisely by specifying jsonpath. For example, only category, author and price attributes are imported\ncurl --location-trusted -u root \\\n    -H "columns: category, price, author" -H "label:123" -H "format: json" -H "jsonpaths: [\\"$.category\\",\\"$.price\\",\\"$.author\\"]" -H "strip_outer_array: true" -H "json_root: $.RECORDS" -T testData \\\n    http://host:port/api/testDb/testTbl/_stream_load\n'})}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"keyword",children:"keyword"}),"\n",(0,n.jsx)(t.p,{children:"STREAM,LOAD"})]})}const d=function(e={}){const{wrapper:t}=Object.assign({},(0,o.ah)(),e.components);return t?(0,n.jsx)(t,Object.assign({},e,{children:(0,n.jsx)(h,e)})):h(e)}},11151:(e,t,a)=>{a.d(t,{Zo:()=>i,ah:()=>r});var n=a(67294);const o=n.createContext({});function r(e){const t=n.useContext(o);return n.useMemo((()=>"function"==typeof e?e(t):{...t,...e}),[t,e])}const s={};function i({components:e,children:t,disableParentContext:a}){let i;return i=a?"function"==typeof e?e({}):e||s:r(e),n.createElement(o.Provider,{value:i},t)}}}]);