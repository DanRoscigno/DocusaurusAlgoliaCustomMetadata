"use strict";(self.webpackChunkstarrocks=self.webpackChunkstarrocks||[]).push([[25496],{3386:(a,e,t)=>{t.r(e),t.d(e,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var n=t(85893),o=t(11151);const r={displayed_sidebar:"English"},s="Routine Load",i={id:"faq/loading/Routine_load_faq",title:"Routine Load",description:"Does mysql binlog data produced by kafka count as text format data?",source:"@site/versioned_docs/version-2.2/faq/loading/Routine_load_faq.md",sourceDirName:"faq/loading",slug:"/faq/loading/Routine_load_faq",permalink:"/docs/2.2/faq/loading/Routine_load_faq",draft:!1,unlisted:!1,editUrl:"https://github.com/StarRocks/starrocks/edit/main/docs/faq/loading/Routine_load_faq.md",tags:[],version:"2.2",frontMatter:{displayed_sidebar:"English"},sidebar:"English",previous:{title:"Loading",permalink:"/docs/2.2/faq/loading/Loading_faq"},next:{title:"Stream Load",permalink:"/docs/2.2/faq/loading/Stream_load_faq"}},d={},c=[{value:"Does mysql binlog data produced by kafka count as text format data?",id:"does-mysql-binlog-data-produced-by-kafka-count-as-text-format-data",level:2},{value:"Can &#39;semantic consistency&#39; be guaranteed by consuming data from kafka, writing it to StarRocks and directly connecting to StarRocks?",id:"can-semantic-consistency-be-guaranteed-by-consuming-data-from-kafka-writing-it-to-starrocks-and-directly-connecting-to-starrocks",level:2},{value:"recompile librdkafka with libsasl2 or openssl support",id:"recompile-librdkafka-with-libsasl2-or-openssl-support",level:2}];function l(a){const e=Object.assign({h1:"h1",h2:"h2",p:"p",strong:"strong",pre:"pre",code:"code"},(0,o.ah)(),a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h1,{id:"routine-load",children:"Routine Load"}),"\n",(0,n.jsx)(e.h2,{id:"does-mysql-binlog-data-produced-by-kafka-count-as-text-format-data",children:"Does mysql binlog data produced by kafka count as text format data?"}),"\n",(0,n.jsx)(e.p,{children:"When imported into kafka via canal, it is in json format."}),"\n",(0,n.jsx)(e.h2,{id:"can-semantic-consistency-be-guaranteed-by-consuming-data-from-kafka-writing-it-to-starrocks-and-directly-connecting-to-starrocks",children:"Can 'semantic consistency' be guaranteed by consuming data from kafka, writing it to StarRocks and directly connecting to StarRocks?"}),"\n",(0,n.jsx)(e.p,{children:"Yes it can!"}),"\n",(0,n.jsx)(e.h2,{id:"recompile-librdkafka-with-libsasl2-or-openssl-support",children:"recompile librdkafka with libsasl2 or openssl support"}),"\n",(0,n.jsx)(e.p,{children:(0,n.jsx)(e.strong,{children:"Issue description:"})}),"\n",(0,n.jsx)(e.p,{children:"Failure to perform Routine Load to Kafka cluster. Errors are reported as:"}),"\n",(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:"language-plain",metastring:"text",children:"ErrorReason{errCode = 4, msg='Job failed to fetch all current partition with error [Failed to send proxy request: failed to send proxy request: [PAUSE: failed to create kafka consumer: No provider for SASL mechanism GSSAPI: recompile librdkafka with libsasl2 or openssl support. Current build options: PLAIN SASL_SCRAM]]'}\n"})}),"\n",(0,n.jsx)(e.p,{children:(0,n.jsx)(e.strong,{children:"Cause:"})}),"\n",(0,n.jsx)(e.p,{children:"The current librdkafka does not support sasl authentication."}),"\n",(0,n.jsx)(e.p,{children:(0,n.jsx)(e.strong,{children:"Solution:"})}),"\n",(0,n.jsx)(e.p,{children:"Compile librdkafka again. Please refer to 'compile librdkafka'."})]})}const u=function(a={}){const{wrapper:e}=Object.assign({},(0,o.ah)(),a.components);return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(l,a)})):l(a)}},11151:(a,e,t)=>{t.d(e,{Zo:()=>i,ah:()=>r});var n=t(67294);const o=n.createContext({});function r(a){const e=n.useContext(o);return n.useMemo((()=>"function"==typeof a?a(e):{...e,...a}),[e,a])}const s={};function i({components:a,children:e,disableParentContext:t}){let i;return i=t?"function"==typeof a?a({}):a||s:r(a),n.createElement(o.Provider,{value:i},e)}}}]);