"use strict";(self.webpackChunkstarrocks=self.webpackChunkstarrocks||[]).push([[78398],{90216:(e,a,o)=>{o.r(a),o.d(a,{assets:()=>r,contentTitle:()=>i,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>l});var n=o(85893),t=o(11151);const s={displayed_sidebar:"English"},i="Load data from a local file system or a streaming data source using HTTP push",d={id:"loading/StreamLoad",title:"Load data from a local file system or a streaming data source using HTTP push",description:"StarRocks provides the loading method HTTP-based Stream Load to help you load data from a local file system or a streaming data source.",source:"@site/versioned_docs/version-2.2/loading/StreamLoad.md",sourceDirName:"loading",slug:"/loading/StreamLoad",permalink:"/docs/2.2/loading/StreamLoad",draft:!1,unlisted:!1,editUrl:"https://github.com/StarRocks/starrocks/edit/main/docs/loading/StreamLoad.md",tags:[],version:"2.2",frontMatter:{displayed_sidebar:"English"},sidebar:"English",previous:{title:"Spark Load",permalink:"/docs/2.2/loading/SparkLoad"},next:{title:"quick_start",permalink:"/docs/2.2/quick_start/"}},r={},l=[{value:"Supported data file formats",id:"supported-data-file-formats",level:2},{value:"Limits",id:"limits",level:2},{value:"Principles",id:"principles",level:2},{value:"Load a local data file",id:"load-a-local-data-file",level:2},{value:"Create a load job",id:"create-a-load-job",level:3},{value:"Load CSV data",id:"load-csv-data",level:4},{value:"Data examples",id:"data-examples",level:5},{value:"Load data",id:"load-data",level:5},{value:"Query data",id:"query-data",level:5},{value:"Load JSON data",id:"load-json-data",level:4},{value:"Data examples",id:"data-examples-1",level:5},{value:"Load data",id:"load-data-1",level:5},{value:"Query data",id:"query-data-1",level:5},{value:"View a load job",id:"view-a-load-job",level:3},{value:"Cancel a load job",id:"cancel-a-load-job",level:3},{value:"Load streaming data",id:"load-streaming-data",level:2},{value:"Parameter configurations",id:"parameter-configurations",level:2},{value:"Usage notes",id:"usage-notes",level:2}];function c(e){const a=Object.assign({h1:"h1",p:"p",ul:"ul",li:"li",a:"a",blockquote:"blockquote",h2:"h2",code:"code",img:"img",h3:"h3",h4:"h4",h5:"h5",ol:"ol",pre:"pre",strong:"strong"},(0,t.ah)(),e.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h1,{id:"load-data-from-a-local-file-system-or-a-streaming-data-source-using-http-push",children:"Load data from a local file system or a streaming data source using HTTP push"}),"\n",(0,n.jsx)(a.p,{children:"StarRocks provides the loading method HTTP-based Stream Load to help you load data from a local file system or a streaming data source."}),"\n",(0,n.jsx)(a.p,{children:"Stream Load runs in synchronous loading mode. After you submit a load job, StarRocks synchronously runs the job, and returns the result of the job after the job finishes. You can determine whether the job is successful based on the job result."}),"\n",(0,n.jsx)(a.p,{children:"Stream Load is suitable for the following business scenarios:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"Load a local data file."}),"\n",(0,n.jsx)(a.p,{children:"In most cases, we recommend that you use curl to submit a load job, which is run to load the data of a local data file into StarRocks."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"Load streaming data."}),"\n",(0,n.jsx)(a.p,{children:"In most cases, we recommend that you use programs such as Apache Flink\xae to submit a load job, within which a series of tasks can be generated to continuously load streaming data in real time into StarRocks."}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Additionally, Stream Load supports data transformation at data loading. For more information, see ",(0,n.jsx)(a.a,{href:"/docs/2.2/loading/Etl_in_loading",children:"Transform data at loading"}),"."]}),"\n",(0,n.jsxs)(a.blockquote,{children:["\n",(0,n.jsx)(a.p,{children:"Note: After you load data into a StarRocks table by using Stream Load, the data of the materialized views that are created on that table is also updated."}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"supported-data-file-formats",children:"Supported data file formats"}),"\n",(0,n.jsx)(a.p,{children:"Stream Load supports the following data file formats:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"CSV"}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"JSON"}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["You can use the ",(0,n.jsx)(a.code,{children:"streaming_load_max_mb"}),' parameter to specify the maximum size of each data file you want to load. The default maximum size is 10 GB. We recommend that you retain the default value of this parameter. For more information, see the "',(0,n.jsx)(a.a,{href:"/docs/2.2/loading/StreamLoad#parameter-configurations",children:"Parameter configurations"}),'" section of this topic.']}),"\n",(0,n.jsx)(a.h2,{id:"limits",children:"Limits"}),"\n",(0,n.jsx)(a.p,{children:"Stream Load does not support loading the data of a CSV file that contains a JSON-formatted column."}),"\n",(0,n.jsx)(a.h2,{id:"principles",children:"Principles"}),"\n",(0,n.jsx)(a.p,{children:"If you choose the loading method Stream Load, you must submit a load request on your client to an FE according to HTTP. The FE uses an HTTP redirect to forward the load request to a specific BE."}),"\n",(0,n.jsxs)(a.blockquote,{children:["\n",(0,n.jsx)(a.p,{children:"Note: You can also create a load job on your client to send a load request to a BE of your choice."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"The BE that receives the load request runs as the Coordinator BE to split data based on the used schema into portions and assign each portion of the data to the other involved BEs. After the load finishes, the Coordinator BE returns the result of the load job to your client."}),"\n",(0,n.jsxs)(a.blockquote,{children:["\n",(0,n.jsx)(a.p,{children:"Note: If you send load requests to an FE, the FE uses a polling mechanism to decide which BE will receive the load requests. The polling mechanism helps achieve load balancing within your StarRocks cluster. Therefore, we recommend that you send load requests to an FE and let the FE decide which BE will run as the Coordinator BE to process the load requests."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"The following figure shows the workflow of a Stream Load job."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Workflow of Stream Load",src:o(14750).Z+"",width:"1776",height:"1068"})}),"\n",(0,n.jsx)(a.h2,{id:"load-a-local-data-file",children:"Load a local data file"}),"\n",(0,n.jsx)(a.h3,{id:"create-a-load-job",children:"Create a load job"}),"\n",(0,n.jsxs)(a.p,{children:["This section uses curl as an example to describe how to load the data of a CSV or JSON file from your local file system into StarRocks. For detailed syntax and parameter descriptions, see ",(0,n.jsx)(a.a,{href:"/docs/2.2/sql-reference/sql-statements/data-manipulation/STREAM_LOAD",children:"STREAM LOAD"}),"."]}),"\n",(0,n.jsx)(a.h4,{id:"load-csv-data",children:"Load CSV data"}),"\n",(0,n.jsx)(a.h5,{id:"data-examples",children:"Data examples"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["In your StarRocks database ",(0,n.jsx)(a.code,{children:"test_db"}),", create a table named ",(0,n.jsx)(a.code,{children:"table1"})," that uses the Primary Key model. The table consists of three columns: ",(0,n.jsx)(a.code,{children:"id"}),", ",(0,n.jsx)(a.code,{children:"name"}),", and ",(0,n.jsx)(a.code,{children:"score"}),", of which ",(0,n.jsx)(a.code,{children:"id"})," is the primary key."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-SQL",children:'MySQL [test_db]> CREATE TABLE `table1`\n(\n   `id` int(11) NOT NULL COMMENT "user ID",\n    `name` varchar(65533) NULL COMMENT "user name",\n    `score` int(11) NOT NULL COMMENT "user score"\n)\nENGINE=OLAP\nPRIMARY KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 10;\n'})}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["In your local file system, create a CSV file named ",(0,n.jsx)(a.code,{children:"example1.csv"}),". The file consists of three columns, which represent the user ID, user name, and user score in sequence."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-Plain",children:"1,Lily,23\n2,Rose,23\n3,Alice,24\n4,Julia,25\n"})}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h5,{id:"load-data",children:"Load data"}),"\n",(0,n.jsxs)(a.p,{children:["Run the following command to load the data of ",(0,n.jsx)(a.code,{children:"example1.csv"})," into ",(0,n.jsx)(a.code,{children:"table1"}),":"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-Bash",children:'curl --location-trusted -u root: -H "label:123" \\\n    -H "column_separator:," \\\n    -H "columns: id, name, score" \\\n    -T example1.csv -XPUT \\\n    http://<fe_host>:<fe_http_port>/api/test_db/table1/_stream_load\n'})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.code,{children:"example1.csv"})," consists of three columns, which are separated by commas (,) and can be mapped in sequence onto the ",(0,n.jsx)(a.code,{children:"id"}),", ",(0,n.jsx)(a.code,{children:"name"}),", and ",(0,n.jsx)(a.code,{children:"score"})," columns of ",(0,n.jsx)(a.code,{children:"table1"}),". Therefore, you need to use the ",(0,n.jsx)(a.code,{children:"column_separator"})," parameter to specify the comma (,) as the column separator. You also need to use the ",(0,n.jsx)(a.code,{children:"columns"})," parameter to temporarily name the three columns of ",(0,n.jsx)(a.code,{children:"example1.csv"})," as ",(0,n.jsx)(a.code,{children:"id"}),", ",(0,n.jsx)(a.code,{children:"name"}),", and ",(0,n.jsx)(a.code,{children:"score"}),", which are mapped in sequence onto the three columns of ",(0,n.jsx)(a.code,{children:"table1"}),"."]}),"\n",(0,n.jsx)(a.h5,{id:"query-data",children:"Query data"}),"\n",(0,n.jsxs)(a.p,{children:["After the load is complete, query the data of ",(0,n.jsx)(a.code,{children:"table1"})," to verify that the load is successful:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-SQL",children:"MySQL [test_db]> SELECT * FROM table1;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|    1 | Lily  |    23 |\n|    2 | Rose  |    23 |\n|    3 | Alice |    24 |\n|    4 | Julia |    25 |\n+------+-------+-------+\n4 rows in set (0.00 sec)\n"})}),"\n",(0,n.jsx)(a.h4,{id:"load-json-data",children:"Load JSON data"}),"\n",(0,n.jsx)(a.h5,{id:"data-examples-1",children:"Data examples"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["In your StarRocks database ",(0,n.jsx)(a.code,{children:"test_db"}),", create a table named ",(0,n.jsx)(a.code,{children:"table2"})," that uses the Primary Key model. The table consists of two columns: ",(0,n.jsx)(a.code,{children:"id"})," and ",(0,n.jsx)(a.code,{children:"city"}),", of which ",(0,n.jsx)(a.code,{children:"id"})," is the primary key."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-SQL",children:'MySQL [test_db]> CREATE TABLE `table2`\n(\n    `id` int(11) NOT NULL COMMENT "city ID",\n    `city` varchar(65533) NULL COMMENT "city name"\n)\nENGINE=OLAP\nPRIMARY KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 10;\n'})}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["In your local file system, create a JSON file named ",(0,n.jsx)(a.code,{children:"example2.json"}),". The file consists of two columns, which represent city ID and city name in sequence."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-JSON",children:'{"name": "Beijing", "code": 2}\n'})}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h5,{id:"load-data-1",children:"Load data"}),"\n",(0,n.jsxs)(a.p,{children:["Run the following command to load the data of ",(0,n.jsx)(a.code,{children:"example2.json"})," into ",(0,n.jsx)(a.code,{children:"table2"}),":"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-Bash",children:'curl -v --location-trusted -u root: -H "strict_mode: true" \\\n    -H "format: json" -H "jsonpaths: [\\"$.name\\", \\"$.code\\"]" \\\n    -H "columns: city,tmp_id, id = tmp_id * 100" \\\n    -T example2.json -XPUT \\\n    http://<fe_host>:<fe_http_port>/api/test_db/table2/_stream_load\n'})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.code,{children:"example2.json"})," consists of two keys, ",(0,n.jsx)(a.code,{children:"name"})," and ",(0,n.jsx)(a.code,{children:"code"}),", which are mapped onto the ",(0,n.jsx)(a.code,{children:"id"})," and ",(0,n.jsx)(a.code,{children:"city"})," columns of ",(0,n.jsx)(a.code,{children:"table2"}),", as shown in the following figure."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"img",src:o(95925).Z+"",width:"1540",height:"1030"})}),"\n",(0,n.jsx)(a.p,{children:"The mappings shown in the preceding figure are described as follows:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["StarRocks extracts the ",(0,n.jsx)(a.code,{children:"name"})," and ",(0,n.jsx)(a.code,{children:"code"})," keys of ",(0,n.jsx)(a.code,{children:"example2.json"})," and maps them onto the ",(0,n.jsx)(a.code,{children:"name"})," and ",(0,n.jsx)(a.code,{children:"code"})," fields declared in the ",(0,n.jsx)(a.code,{children:"jsonpaths"})," parameter."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["StarRocks extracts the ",(0,n.jsx)(a.code,{children:"name"})," and ",(0,n.jsx)(a.code,{children:"code"})," fields declared in the ",(0,n.jsx)(a.code,{children:"jsonpaths"})," parameter and ",(0,n.jsx)(a.strong,{children:"maps them in sequence"})," onto the ",(0,n.jsx)(a.code,{children:"city"})," and ",(0,n.jsx)(a.code,{children:"tmp_id"})," fields declared in the ",(0,n.jsx)(a.code,{children:"columns"})," parameter."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["StarRocks extracts the ",(0,n.jsx)(a.code,{children:"city"})," and ",(0,n.jsx)(a.code,{children:"tmp_id"})," fields declared in the ",(0,n.jsx)(a.code,{children:"columns"})," parameter and ",(0,n.jsx)(a.strong,{children:"maps them by name"})," onto the ",(0,n.jsx)(a.code,{children:"city"})," and ",(0,n.jsx)(a.code,{children:"id"})," columns of ",(0,n.jsx)(a.code,{children:"table2"}),"."]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.blockquote,{children:["\n",(0,n.jsxs)(a.p,{children:["Note: In the preceding example, the value of ",(0,n.jsx)(a.code,{children:"code"})," in ",(0,n.jsx)(a.code,{children:"example2.json"})," is multiplied by 100 before it is loaded into the ",(0,n.jsx)(a.code,{children:"id"})," column of ",(0,n.jsx)(a.code,{children:"table2"}),"."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["For detailed mappings between ",(0,n.jsx)(a.code,{children:"jsonpaths"}),", ",(0,n.jsx)(a.code,{children:"columns"}),', and the columns of the StarRocks table, see the "Usage notes" section in ',(0,n.jsx)(a.a,{href:"/docs/2.2/sql-reference/sql-statements/data-manipulation/STREAM_LOAD",children:"STREAM LOAD"}),"."]}),"\n",(0,n.jsx)(a.h5,{id:"query-data-1",children:"Query data"}),"\n",(0,n.jsxs)(a.p,{children:["After the load is complete, query the data of ",(0,n.jsx)(a.code,{children:"table2"})," to verify that the load is successful:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-SQL",children:"MySQL [test_db]> SELECT * FROM table2;\n+------+--------+\n| id   | city   |\n+------+--------+\n| 200  | Beijing|\n+------+--------+\n4 rows in set (0.01 sec)\n"})}),"\n",(0,n.jsx)(a.h3,{id:"view-a-load-job",children:"View a load job"}),"\n",(0,n.jsxs)(a.p,{children:['After a load job is complete, StarRocks returns the result of the job in JSON format. For more information, see the "Return value" section in ',(0,n.jsx)(a.a,{href:"/docs/2.2/sql-reference/sql-statements/data-manipulation/STREAM_LOAD",children:"STREAM LOAD"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"Stream Load does not allow you to query the result of a load job by using the SHOW LOAD statement."}),"\n",(0,n.jsx)(a.h3,{id:"cancel-a-load-job",children:"Cancel a load job"}),"\n",(0,n.jsx)(a.p,{children:"Stream Load does not allow you to cancel a load job. If a load job times out or encounters errors, StarRocks automatically cancels the job."}),"\n",(0,n.jsx)(a.h2,{id:"load-streaming-data",children:"Load streaming data"}),"\n",(0,n.jsx)(a.p,{children:"Stream Load allows you to load streaming data into StarRocks in real time by using programs. For more information, see the following topics:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["For information about how to run Stream Load jobs by using Flink, see ",(0,n.jsx)(a.a,{href:"/docs/2.2/loading/Flink-connector-starrocks",children:"Load data by using flink-connector-starrocks"}),"."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["For information about how to run Stream Load jobs by using Java programs, visit ",(0,n.jsx)(a.a,{href:"https://github.com/StarRocks/demo/tree/master/MiscDemo/stream_load",children:"https://github.com/StarRocks/demo/MiscDemo/stream_load"}),"."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["For information about how to run Stream Load jobs by using Apache Spark\u2122, see ",(0,n.jsx)(a.a,{href:"https://github.com/StarRocks/demo/blob/master/docs/01_sparkStreaming2StarRocks.md",children:"01_sparkStreaming2StarRocks"}),"."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"parameter-configurations",children:"Parameter configurations"}),"\n",(0,n.jsx)(a.p,{children:"This section describes some system parameters that you need to configure if you choose the loading method Stream Load. These parameter configurations take effect on all Stream Load jobs."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.code,{children:"streaming_load_max_mb"}),": the maximum size of each data file you want to load. The default maximum size is 10 GB. For more information, see ",(0,n.jsx)(a.a,{href:"/docs/2.2/administration/Configuration#be-configuration-items",children:"BE configuration items"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"We recommend that you do not load more than 10 GB of data at a time. If the size of a data file exceeds 10 GB, we recommend that you split the data file into small files that each are less than 10 GB in size and then load these files one by one. If you cannot split a data file greater than 10 GB, you can increase the value of this parameter based on the file size."}),"\n",(0,n.jsx)(a.p,{children:"After you increase the value of this parameter, the new value can take effect only after you restart the BEs of your StarRocks cluster. Additionally, system performance may deteriorate, and the costs of retries in the event of load failures also increase."}),"\n",(0,n.jsxs)(a.blockquote,{children:["\n",(0,n.jsx)(a.p,{children:'Note: When you load the data of a JSON file, make sure that the size of each JSON object in the file does not exceed 4 GB. If any JSON object in the file exceeds 4 GB, StarRocks throws an error "This parser can\'t support a document that big."'}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.code,{children:"stream_load_default_timeout_second"}),": the timeout period of each load job. The default timeout period is 600 seconds. For more information, see ",(0,n.jsx)(a.a,{href:"/docs/2.2/administration/Configuration#fe-configuration-items",children:"FE configuration items"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"If many of the load jobs that you create time out, you can increase the value of this parameter based on the calculation result that you obtain from the following formula:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"Timeout period of each load job > Amount of data to be loaded/Average loading speed"})}),"\n",(0,n.jsxs)(a.blockquote,{children:["\n",(0,n.jsxs)(a.p,{children:["Note: ",(0,n.jsx)(a.strong,{children:"Average loading speed"})," in the preceding formula is the average loading speed of your StarRocks cluster. It varies depending on the server configurations and the number of allowed concurrent queries. You need to deduct the average loading speed based on the loading speeds of historical load jobs."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"For example, if the size of the data file that you want to load is 10 GB and the average loading speed of your StarRocks cluster is 10 MB/s, set the timeout period to more than 1024 seconds."}),"\n",(0,n.jsxs)(a.p,{children:["Stream Load also provides the ",(0,n.jsx)(a.code,{children:"timeout"})," parameter, which allows you to specify the timeout period of an individual load job. For more information, see ",(0,n.jsx)(a.a,{href:"/docs/2.2/sql-reference/sql-statements/data-manipulation/STREAM_LOAD",children:"STREAM LOAD"}),"."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"usage-notes",children:"Usage notes"}),"\n",(0,n.jsxs)(a.p,{children:["If a field is missing for a record in the data file you want to load and the column onto which the field is mapped in your StarRocks table is defined as ",(0,n.jsx)(a.code,{children:"NOT NULL"}),", StarRocks automatically fills a ",(0,n.jsx)(a.code,{children:"NULL"})," value in the mapping column of your StarRocks table during the load of the record. You can also use the ",(0,n.jsx)(a.code,{children:"ifnull()"})," function to specify the default value that you want to fill."]}),"\n",(0,n.jsxs)(a.p,{children:["For example, if the field that represents city ID in the preceding ",(0,n.jsx)(a.code,{children:"example2.json"})," file is missing and you want to fill an ",(0,n.jsx)(a.code,{children:"x"})," value in the mapping column of ",(0,n.jsx)(a.code,{children:"table2"}),", you can specify ",(0,n.jsx)(a.code,{children:"\"columns: city, tmp_id, id = ifnull(tmp_id, 'x')\""}),"."]})]})}const h=function(e={}){const{wrapper:a}=Object.assign({},(0,t.ah)(),e.components);return a?(0,n.jsx)(a,Object.assign({},e,{children:(0,n.jsx)(c,e)})):c(e)}},14750:(e,a,o)=>{o.d(a,{Z:()=>n});const n=o.p+"assets/images/4.2-1-b3200ec0a9dd3899f0782aa47b4e66a8.png"},95925:(e,a,o)=>{o.d(a,{Z:()=>n});const n=o.p+"assets/images/4.2-2en-6c8ec83733b0f6582aacb759d6a6fb95.png"},11151:(e,a,o)=>{o.d(a,{Zo:()=>d,ah:()=>s});var n=o(67294);const t=n.createContext({});function s(e){const a=n.useContext(t);return n.useMemo((()=>"function"==typeof e?e(a):{...a,...e}),[a,e])}const i={};function d({components:e,children:a,disableParentContext:o}){let d;return d=o?"function"==typeof e?e({}):e||i:s(e),n.createElement(t.Provider,{value:d},a)}}}]);