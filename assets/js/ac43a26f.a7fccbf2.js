"use strict";(self.webpackChunkstarrocks=self.webpackChunkstarrocks||[]).push([[68831],{45352:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>l});var t=s(85893),a=s(11151);const i={displayed_sidebar:"English"},r="BROKER LOAD",o={id:"sql-reference/sql-statements/data-manipulation/BROKER_LOAD",title:"BROKER LOAD",description:"description",source:"@site/versioned_docs/version-2.2/sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md",sourceDirName:"sql-reference/sql-statements/data-manipulation",slug:"/sql-reference/sql-statements/data-manipulation/BROKER_LOAD",permalink:"/docs/2.2/sql-reference/sql-statements/data-manipulation/BROKER_LOAD",draft:!1,unlisted:!1,editUrl:"https://github.com/StarRocks/starrocks/edit/main/docs/sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md",tags:[],version:"2.2",frontMatter:{displayed_sidebar:"English"},sidebar:"English",previous:{title:"SHOW FUNCTIONS",permalink:"/docs/2.2/sql-reference/sql-statements/data-definition/show-functions"},next:{title:"CANCEL LOAD",permalink:"/docs/2.2/sql-reference/sql-statements/data-manipulation/CANCEL_LOAD"}},d={},l=[{value:"description",id:"description",level:2},{value:"Syntax",id:"syntax",level:3},{value:"keyword",id:"keyword",level:2}];function c(e){const n=Object.assign({h1:"h1",h2:"h2",p:"p",ol:"ol",li:"li",pre:"pre",code:"code",strong:"strong",div:"div",h3:"h3"},(0,a.ah)(),e.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"broker-load",children:"BROKER LOAD"}),"\n",(0,t.jsx)(n.h2,{id:"description",children:"description"}),"\n",(0,t.jsx)(n.p,{children:"Broker Load is performed via brokers deployed with the StarRocks cluster. It will access data from corresponding data source and load data into StarRocks via Broker."}),"\n",(0,t.jsx)(n.p,{children:"Use show broker command to check the deployed broker."}),"\n",(0,t.jsx)(n.p,{children:"It supports the following six data sources:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Apache HDFS\uff1a hdfs of community version."}),"\n",(0,t.jsx)(n.li,{children:"Amazon S3\uff1aAmazon object storage."}),"\n",(0,t.jsx)(n.li,{children:"\u963f\u91cc\u4e91 OSS\uff1aAliyun object storage."}),"\n",(0,t.jsx)(n.li,{children:"\u817e\u8bafCOS\uff1aTencent cloud object stroage."}),"\n",(0,t.jsx)(n.li,{children:"\u767e\u5ea6BOS\uff1aBaidu object storage.\nSyntax:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"LOAD LABEL load_label\n(\ndata_desc1[, data_desc2, ...]\n)\nWITH BROKER broker_name\n[broker_properties]\n[opt_properties];\n"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"load_label"}),"\n",(0,t.jsx)(n.p,{children:"Label of the current load. Unique load label within a database."}),"\n",(0,t.jsx)(n.p,{children:"Syntax:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"[database_name.]your_label\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"data_desc"}),"\n",(0,t.jsx)(n.p,{children:"To describe the data source."}),"\n",(0,t.jsx)(n.p,{children:"Syntax:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'DATA INFILE\n(\n    "file_path1"[, file_path2, ...]\n)\n[NEGATIVE]\nINTO TABLE `table_name`\n[PARTITION (p1, p2)]\n[COLUMNS TERMINATED BY "column_separator"]\n[FORMAT AS "file_type"]\n[(column_list)]\n[COLUMNS FROM PATH AS (column_list)]\n[SET (k1 = func(k2))]\n[WHERE predicate]\n'})}),"\n",(0,t.jsx)(n.p,{children:"Note:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-plain",metastring:"text",children:'file_path:\n\nFile path, which can direct to a file and also to all files in a directory with the * wildcard. Wildcards must match to files, not the directory.\n\nPARTITION:\n\nIf the parameter is specified, data will only be loaded to specified partitions and data out of partition\'s range will be filtered. If not specified, all partitions will be loaded. \n\nNEGATIVE\uff1a\nIf this parameter is specified, it is equivalent to importing a batch of "negative" data to offset the same batch of data loaded before. The parameter applies only to the case where value column exists and the aggregation type of value column is SUM. \n\ncolumn_separator\uff1a\n\nIt is used to specify the column separator in the loaded file. Default is \\t. \nIf the character is invisible, it needs to be prefixed with \\\\x, using hexadecimal to represent the separator. \nFor example, the separator \\x01 of the hive file is specified as \\\\ x01 \n\nfile_type\uff1a\n\nIt is used to specify the type of loaded file, such as parquet, orc, csv. Default values are determined by the file suffix name. \n\ncolumn_list\uff1a\n\nUsed to specify the correspondence between columns in the imported file and columns in the table. \nTo skip a column in the imported file, specify it as a column name that does not exist in the table. \nSyntax: \n(col_name1, col_name2, ...)\n\nCOLUMNS FROM PATH AS:\n\nExtract the partition field from the file path. For example, the loaded file is /path/col_name=col_value/file1 and col_name is a column in the table, and then col_value will be imported to the column corresponding to col_name.\n\nSET:\n\nIf this parameter is specified, a column of the source file can be converted based on a function, and then the transformed result can be loaded into the table. The syntax is column_name = expression. Some examples are given to facilitate understanding. \nExample 1: There are three columns "c1, c2, c3" in the table. The first two columns in the source file correspond in turn (c1, c2), and the sum of the last two columns correspond to c3. Then, column (c1, c2, tmp_c3, tmp_c4) SET (c3 = tmp_c3 + tmp_c4) should be specified. \nExample 2: There are three columns "year, month, day" in the table. There is only one time column in the source file, in the format of "2018-06-01:02:03". Then you can specify columns (tmp_time) set (year = year (tmp_time), month = month (tmp_time), day = day (tmp_time)) to complete the loading. \n\nWHERE:\n\nFilter the data under "transform" d, and data that meets "where" predicates can be loaded. Only column names in tables can be referenced in WHERE statements.\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"broker_name"}),"\n",(0,t.jsxs)(n.p,{children:["The name of the broker being used and can be viewed through the ",(0,t.jsx)(n.code,{children:"show broker"})," command."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"broker_properties"}),"\n",(0,t.jsx)(n.p,{children:"It is used to provide information about accessing data source via broker. For different brokers, and different access methods, different information needs to be provided."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Apache HDFS"}),"\n",(0,t.jsx)(n.p,{children:"hdfs of community version, which supports simple authentication, Kerberos authentication, and HA configuration."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Simple authentication:"})}),"\n",(0,t.jsx)(n.p,{children:"hadoop.security.authentication = simple (default)"}),"\n",(0,t.jsx)(n.p,{children:"username: hdfs username"}),"\n",(0,t.jsx)(n.p,{children:"password: hdfs password"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"kerberos authentication:"})}),"\n",(0,t.jsx)(n.p,{children:"hadoop.security.authentication = kerberos"}),"\n",(0,t.jsx)(n.p,{children:"kerberos_principal: principal of specified kerberos"}),"\n",(0,t.jsx)(n.p,{children:"kerberos_keytab: keytab file path of specified kerberos. This file must be on the server where broker process resides."}),"\n",(0,t.jsx)(n.p,{children:"kerberos_keytab_content: the contents of the KeyTab file in specified Kerberos after base64 encoding, which is optional from the kerberos_keytab configuration."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"namenode HA:"})}),"\n",(0,t.jsx)(n.p,{children:"By configuring namenode HA, new namenode can be automatically identified when the namenode is switched."}),"\n",(0,t.jsx)(n.p,{children:'dfs.nameservices: specify hdfs service name, custom, eg: "dfs.nameservices" = "my_ha"'}),"\n",(0,t.jsx)(n.p,{children:'dfs.ha.namenodes.xxx: customize the name of a namenode, separated by commas. XXX is a custom name in dfs. name services, such as "dfs. ha. namenodes. my_ha" = "my_nn"'}),"\n",(0,t.jsxs)(n.p,{children:['dfs.namenode.rpc-address.xxx.nn: specify rpc address information for namenode, where nn denotes the name of the namenode configured in dfs.ha.namenodes.xxxx, such as: "dfs.namenode.rpc-address.my_ha.my_nn"= "host',(0,t.jsx)(n.div,{}),'"']}),"\n",(0,t.jsx)(n.p,{children:"dfs.client.failover.proxy.provider: specify the provider that client connects to namenode by default: org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Amazon S3"}),"\n",(0,t.jsx)(n.p,{children:"The following need to be provided:"}),"\n",(0,t.jsx)(n.p,{children:"access key of fs.s3a.access.key\uff1aAmazonS3"}),"\n",(0,t.jsx)(n.p,{children:"secret key of fs.s3a.secret.key\uff1aAmazonS3"}),"\n",(0,t.jsx)(n.p,{children:"endpoint of fs.s3a.endpoint\uff1aAmazonS3"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Aliyun OSS"}),"\n",(0,t.jsx)(n.p,{children:"The following need to be provided:"}),"\n",(0,t.jsx)(n.p,{children:"access key of fs.oss.accessKeyId\uff1aAliyun OSS"}),"\n",(0,t.jsx)(n.p,{children:"secret key of fs.oss.accessKeySecret\uff1aAliyun OSS"}),"\n",(0,t.jsx)(n.p,{children:"endpoint of fs.oss.endpoint\uff1aAliyun OSS"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Baidu BOS"}),"\n",(0,t.jsx)(n.p,{children:"The following need to be provided:"}),"\n",(0,t.jsx)(n.p,{children:"Endpoint of bos_endpoint\uff1aBOS"}),"\n",(0,t.jsx)(n.p,{children:"accesskey of bos_ accesskey: public cloud user"}),"\n",(0,t.jsx)(n.p,{children:"secret_accesskey of bos_secret_accesskey: public cloud user"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"opt_properties"}),"\n",(0,t.jsx)(n.p,{children:"It is used to specify some special parameters."}),"\n",(0,t.jsx)(n.p,{children:"Syntax:"}),"\n",(0,t.jsx)(n.p,{children:'[PROPERTIES ("key"="value", \u2026)]'}),"\n",(0,t.jsx)(n.p,{children:"You can specify the following parameters:"}),"\n",(0,t.jsx)(n.p,{children:"timeout: Specifies the timeout time for the import operation. The default timeout is 4 hours and the unit is second."}),"\n",(0,t.jsx)(n.p,{children:"max_filter_ratio: Data ratio of maximum tolerance filterable (data irregularity, etc.). Zero tolerance by default."}),"\n",(0,t.jsx)(n.p,{children:"exc_mem_limit: Memory limit. 2GB by default. The unit is byte."}),"\n",(0,t.jsx)(n.p,{children:"strict_mode: Whether the data is strictly restricted. The default is false."}),"\n",(0,t.jsx)(n.p,{children:'timezone: Specify time zones for functions affected by time zones, such as strftime/alignment_timestamp/from_unixtime, etc. See the [Time Zone] documentation for details. If not specified, use the "Asia/Shanghai" time zone.'}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Format sample of loaded data"}),"\n",(0,t.jsx)(n.p,{children:"Integer\uff08TINYINT/SMALLINT/INT/BIGINT/LARGEINT\uff09: 1, 1000, 1234"}),"\n",(0,t.jsx)(n.p,{children:"Float\uff08FLOAT/DOUBLE/DECIMAL\uff09: 1.1, 0.23, .356"}),"\n",(0,t.jsx)(n.p,{children:"Date\uff08DATE/DATETIME\uff09: 2017-10-03, 2017-06-13 12:34:03. (Note: If it's in other date formats, use strftime or time_format functions to convert in the loading command)"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:'String\uff08CHAR/VARCHAR\uff09: "I am a student", "a"'}),"\n",(0,t.jsx)(n.p,{children:"NULL value: \\N"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"syntax",children:"Syntax"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Load a batch of data from HDFS, specify timeout and filtering ratio. Use the broker with the plaintext my_hdfs_broker. Simple authentication."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label1\n(\nDATA INFILE("hdfs://hdfs_host:hdfs_port/user/starRocks/data/input/file")\nINTO TABLE `my_table`\n)\nWITH BROKER my_hdfs_broker\n(\n    "username" = "hdfs_user",\n    "password" = "hdfs_passwd"\n)\nPROPERTIES\n(\n    "timeout" = "3600",\n    "max_filter_ratio" = "0.1"\n);\n'})}),"\n",(0,t.jsx)(n.p,{children:"Where hdfs_host is the host of the namenode and hdfs_port is the fs.defaultFS port (default 9000)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Load a batch of data from HDFS, specify hive's default delimiter \\x01, and use wildcard * to specify all files under the directory. Use simple authentication and configure namenode HA."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label3\n(\nDATA INFILE("hdfs://hdfs_host:hdfs_port/user/starRocks/data/input/*")\nINTO TABLE `my_table`\nCOLUMNS TERMINATED BY "\\\\x01"\n)\nWITH BROKER my_hdfs_broker\n(\n    "username" = "hdfs_user",\n    "password" = "hdfs_passwd",\n    "dfs.nameservices" = "my_ha",\n    "dfs.ha.namenodes.my_ha" = "my_namenode1, my_namenode2",\n    "dfs.namenode.rpc-address.my_ha.my_namenode1" = "nn1_host:rpc_port",\n    "dfs.namenode.rpc-address.my_ha.my_namenode2" = "nn2_host:rpc_port",\n    "dfs.client.failover.proxy.provider" = "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"\n)\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:'Load a batch of "negative" data from HDFS and use Kerberos authentication to provide KeyTab file path.'}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label4\n(\nDATA INFILE("hdfs://hdfs_host:hdfs_port/user/starRocks/data/input/old_file)\nNEGATIVE\nINTO TABLE `my_table`\nCOLUMNS TERMINATED BY "\\t"\n)\nWITH BROKER my_hdfs_broker\n(\n    "hadoop.security.authentication" = "kerberos",\n    "kerberos_principal"="starrocks@YOUR.COM",\n    "kerberos_keytab"="/home/starRocks/starRocks.keytab"\n)\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Load a batch of data from HDFS, specify partition. At the same time, use Kerberos authentication mode. Provide the KeyTab file content encoded by base64."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label5\n(\nDATA INFILE("hdfs://hdfs_host:hdfs_port/user/starRocks/data/input/file")\nINTO TABLE `my_table`\nPARTITION (p1, p2)\nCOLUMNS TERMINATED BY ","\n(k1, k3, k2, v1, v2)\n)\nWITH BROKER my_hdfs_broker\n(\n    "hadoop.security.authentication"="kerberos",\n    "kerberos_principal"="starrocks@YOUR.COM",\n    "kerberos_keytab_content"="BQIAAABEAAEACUJBSURVLkNPTQAEcGFsbw"\n)\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Load a batch of data from BOS, specify partitions, and make some conversions to the columns of the imported files, as follows:"}),"\n",(0,t.jsx)(n.p,{children:"Table schema:\nk1 varchar(20)\nk2 int"}),"\n",(0,t.jsx)(n.p,{children:"Assuming that the data file has only one row of data:"}),"\n",(0,t.jsx)(n.p,{children:"Adele,1,1"}),"\n",(0,t.jsx)(n.p,{children:"The columns in the data file correspond to the columns specified in the loaded statement:\nk1,tmp_k2,tmp_k3"}),"\n",(0,t.jsx)(n.p,{children:"Conduct the following conversion:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"k1: unchanged"}),"\n",(0,t.jsx)(n.li,{children:"k2\uff1asum of tmp_k2 and tmp_k3"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label6\n(\nDATA INFILE("bos://my_bucket/input/file")\nINTO TABLE `my_table`\nPARTITION (p1, p2)\nCOLUMNS TERMINATED BY ","\n(k1, tmp_k2, tmp_k3)\nSET (\nk2 = tmp_k2 + tmp_k3\n)\n)\nWITH BROKER my_bos_broker\n(\n    "bos_endpoint" = "http://bj.bcebos.com",\n    "bos_accesskey" = "xxxxxxxxxxxxxxxxxxxxxxxxxx",\n    "bos_secret_accesskey"="yyyyyyyyyyyyyyyyyyyy"\n)\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Load data into tables containing HLL columns, which can be columns in tables or columns in data."}),"\n",(0,t.jsx)(n.p,{children:"If there are three columns in the table are (id, v1, v2, v3). The v1 and v2 columns are hll columns. The imported source file has 3 columns.Then declare the first column is id in (column_list) and the second and the third columns are temporarily named k1 and k2."}),"\n",(0,t.jsx)(n.p,{children:"In SET, the HLL column in the table must be specifically declared hll_hash. The v1 column in the table is equal to the hll_hash (k1) column in the original data. The v3 column in the table does not have a corresponding value in the original data, and use empty_hll to supplement the default value."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL example_db.label7\n(\nDATA INFILE("hdfs://hdfs_host:hdfs_port/user/starRocks/data/input/file")\nINTO TABLE `my_table`\nPARTITION (p1, p2)\nCOLUMNS TERMINATED BY ","\n(id, k1, k2)\nSET (\nv1 = hll_hash(k1),\nv2 = hll_hash(k2),\nv3 = empty_hll()\n)\n)\nWITH BROKER hdfs ("username"="hdfs_user", "password"="hdfs_password");\n\nLOAD LABEL example_db.label8\n(\nDATA INFILE("hdfs://hdfs_host:hdfs_port/user/starRocks/data/input/file")\nINTO TABLE `my_table`\nPARTITION (p1, p2)\nCOLUMNS TERMINATED BY ","\n(k1, k2, tmp_k3, tmp_k4, v1, v2)\nSET (\nv1 = hll_hash(tmp_k3),\nv2 = hll_hash(tmp_k4)\n)\n)\nWITH BROKER hdfs ("username"="hdfs_user", "password"="hdfs_password");\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Load data in Parquet file and specify FORMAT as parquet. The default is determined by file suffix."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL example_db.label9\n(\nDATA INFILE("hdfs://hdfs_host:hdfs_port/user/starRocks/data/input/file")\nINTO TABLE `my_table`\nFORMAT AS "parquet"\n(k1, k2, k3)\n)\nWITH BROKER hdfs ("username"="hdfs_user", "password"="hdfs_password");\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Extract partitioned fields in file paths."}),"\n",(0,t.jsx)(n.p,{children:"If necessary, partitioned fields in the file path are resolved based on the field type defined in the table, similar to the Partition Discovery function in Spark."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL example_db.label10\n(\nDATA INFILE("hdfs://hdfs_host:hdfs_port/user/starRocks/data/input/dir/city=beijing/*/*")\nINTO TABLE `my_table`\nFORMAT AS "csv"\n(k1, k2, k3)\nCOLUMNS FROM PATH AS (city, utc_date)\nSET (uniq_id = md5sum(k1, city))\n)\nWITH BROKER hdfs ("username"="hdfs_user", "password"="hdfs_password");\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"hdfs://hdfs_host:hdfs_port/user/starRocks/data/input/dir/city=beijing"})," contains following files:"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"[hdfs://hdfs_host:hdfs_port/user/starRocks/data/input/dir/city=beijing/utc_date=2019-06-26/0000.csv, hdfs://hdfs_host:hdfs_port/user/starRocks/data/input/dir/city=beijing/utc_date=2019-06-26/0001.csv, ...]"})}),"\n",(0,t.jsx)(n.p,{children:"Extract city and utc_date fields in the file path."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Filter the loaded data: columns whose k1 value is bigger than k2 value can be imported."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label10\n(\nDATA INFILE("hdfs://hdfs_host:hdfs_port/user/starRocks/data/input/file")\nINTO TABLE `my_table`\nwhere k1 > k2\n)\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Extract time partitioned fields in file paths, and the time includes %3A (in hdfs path, all ':' will be replaced by '%3A')"}),"\n",(0,t.jsx)(n.p,{children:"Assume we have files:"}),"\n",(0,t.jsx)(n.p,{children:"/user/data/data_time=2020-02-17 00%3A00%3A00/test.txt"}),"\n",(0,t.jsx)(n.p,{children:"/user/data/data_time=2020-02-18 00%3A00%3A00/test.txt"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-PLAIN",metastring:"TEXT",children:"Table structure\uff1a\ndata_time DATETIME,\nk2        INT,\nk3        INT\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL example_db.label11\n(\nDATA INFILE("hdfs://host:port/user/data/*/test.txt")\nINTO TABLE `tbl12`\nCOLUMNS TERMINATED BY ","\n(k2,k3)\nCOLUMNS FROM PATH AS (data_time)\nSET (data_time=str_to_date(data_time, \'%Y-%m-%d %H%%3A%i%%3A%s\'))\n)\nWITH BROKER "hdfs" ("username"="user", "password"="pass");\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Load data in csv format from Aliyun OSS."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL example_db.label12\n(\nDATA INFILE("oss://my_bucket/input/file.csv")\nINTO TABLE `my_table`\n(k1, k2, k3)\n)\nWITH BROKER my_broker\n(\n    "fs.oss.accessKeyId" = "xxxxxxxxxxxxxxxxxxxxxxxxxx",\n    "fs.oss.accessKeySecret" = "yyyyyyyyyyyyyyyyyyyy",\n    "fs.oss.endpoint" = "oss-cn-zhangjiakou-internal.aliyuncs.com"\n)\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Load data in csv format from Tencent Cloud COS."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL example_db.label13\n(\nDATA INFILE("cosn://my_bucket/input/file.csv")\nINTO TABLE `my_table`\n(k1, k2, k3)\n)\nWITH BROKER my_broker\n(\n    "fs.cosn.userinfo.secretId" = "xxxxxxxxxxxxxxxxxxxxxxxxxx",\n    "fs.cosn.userinfo.secretKey" = "yyyyyyyyyyyyyyyyyyyy",\n    "fs.cosn.bucket.endpoint_suffix" = "cos.ap-beijing.myqcloud.com"\n)\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Load data in csv format from Amazon S3."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL example_db.label14\n(\nDATA INFILE("s3a://my_bucket/input/file.csv")\nINTO TABLE `my_table`\n(k1, k2, k3)\n)\nWITH BROKER my_broker\n(\n    "fs.s3a.access.key" = "xxxxxxxxxxxxxxxxxxxxxxxxxx",\n    "fs.s3a.secret.key" = "yyyyyyyyyyyyyyyyyyyy",\n    "fs.s3a.endpoint" = "s3-ap-northeast-1.amazonaws.com"\n)\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"keyword",children:"keyword"}),"\n",(0,t.jsx)(n.p,{children:"BROKER,LOAD"})]})}const h=function(e={}){const{wrapper:n}=Object.assign({},(0,a.ah)(),e.components);return n?(0,t.jsx)(n,Object.assign({},e,{children:(0,t.jsx)(c,e)})):c(e)}},11151:(e,n,s)=>{s.d(n,{Zo:()=>o,ah:()=>i});var t=s(67294);const a=t.createContext({});function i(e){const n=t.useContext(a);return t.useMemo((()=>"function"==typeof e?e(n):{...n,...e}),[n,e])}const r={};function o({components:e,children:n,disableParentContext:s}){let o;return o=s?"function"==typeof e?e({}):e||r:i(e),t.createElement(a.Provider,{value:o},n)}}}]);