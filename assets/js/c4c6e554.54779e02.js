"use strict";(self.webpackChunkstarrocks=self.webpackChunkstarrocks||[]).push([[58483],{82758:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>i,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>c,toc:()=>d});var t=r(85893),s=r(11151);const a={},o="Load data using Spark connector (recommended)",c={id:"loading/Spark-connector-starrocks",title:"Load data using Spark connector (recommended)",description:"StarRocks provides a self-developed connector named StarRocks Connector for Apache Spark\u2122 (Spark connector for short) to help you load data into a StarRocks table by using Spark. The basic principle is to accumulate the data and then load it all at a time into StarRocks through STREAM LOAD. The Spark connector is implemented based on Spark DataSource V2. A DataSource can be created by using Spark DataFrames or Spark SQL. And both batch and structured streaming modes are supported.",source:"@site/versioned_docs/version-3.1/loading/Spark-connector-starrocks.md",sourceDirName:"loading",slug:"/loading/Spark-connector-starrocks",permalink:"/doc/en/loading/Spark-connector-starrocks",draft:!1,unlisted:!1,editUrl:"https://github.com/StarRocks/starrocks/edit/main/docs/loading/Spark-connector-starrocks.md",tags:[],version:"3.1",frontMatter:{},sidebar:"documentation",previous:{title:"Continuously load data from Apache Kafka\xae",permalink:"/doc/en/loading/RoutineLoad"},next:{title:"Load data in bulk using Spark Load",permalink:"/doc/en/loading/SparkLoad"}},i={},d=[{value:"Version requirements",id:"version-requirements",level:2},{value:"Obtain Spark connector",id:"obtain-spark-connector",level:2},{value:"Download the compiled Jar file",id:"download-the-compiled-jar-file",level:3},{value:"Maven Dependency",id:"maven-dependency",level:3},{value:"Compile by yourself",id:"compile-by-yourself",level:3},{value:"Parameters",id:"parameters",level:2},{value:"Data type mapping between Spark and StarRocks",id:"data-type-mapping-between-spark-and-starrocks",level:2},{value:"Upgrade Spark connector",id:"upgrade-spark-connector",level:2},{value:"Upgrade from version 1.1.0 to 1.1.1",id:"upgrade-from-version-110-to-111",level:3},{value:"Examples",id:"examples",level:2},{value:"Preparations",id:"preparations",level:3},{value:"Create a StarRocks table",id:"create-a-starrocks-table",level:4},{value:"Set up your Spark environment",id:"set-up-your-spark-environment",level:4},{value:"Load data with Spark DataFrames",id:"load-data-with-spark-dataframes",level:3},{value:"Batch",id:"batch",level:4},{value:"Structured Streaming",id:"structured-streaming",level:4},{value:"Load data with Spark SQL",id:"load-data-with-spark-sql",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Load data to Primary Key table",id:"load-data-to-primary-key-table",level:3},{value:"Preparations",id:"preparations-1",level:4},{value:"Partial updates",id:"partial-updates",level:4},{value:"Conditional updates",id:"conditional-updates",level:4},{value:"Load data into columns of BITMAP type",id:"load-data-into-columns-of-bitmap-type",level:3},{value:"Load data into columns of HLL type",id:"load-data-into-columns-of-hll-type",level:3},{value:"Load data into columns of ARRAY type",id:"load-data-into-columns-of-array-type",level:3}];function l(e){const n=Object.assign({h1:"h1",p:"p",a:"a",blockquote:"blockquote",strong:"strong",h2:"h2",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",ul:"ul",li:"li",code:"code",h3:"h3",ol:"ol",pre:"pre",h4:"h4"},(0,s.ah)(),e.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"load-data-using-spark-connector-recommended",children:"Load data using Spark connector (recommended)"}),"\n",(0,t.jsxs)(n.p,{children:["StarRocks provides a self-developed connector named StarRocks Connector for Apache Spark\u2122 (Spark connector for short) to help you load data into a StarRocks table by using Spark. The basic principle is to accumulate the data and then load it all at a time into StarRocks through ",(0,t.jsx)(n.a,{href:"/doc/en/sql-reference/sql-statements/data-manipulation/STREAM_LOAD",children:"STREAM LOAD"}),". The Spark connector is implemented based on Spark DataSource V2. A DataSource can be created by using Spark DataFrames or Spark SQL. And both batch and structured streaming modes are supported."]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,t.jsxs)(n.p,{children:["Only users with the SELECT and INSERT privileges on a StarRocks table can load data into this table. You can follow the instructions provided in ",(0,t.jsx)(n.a,{href:"/doc/en/sql-reference/sql-statements/account-management/GRANT",children:"GRANT"})," to grant these privileges to a user."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"version-requirements",children:"Version requirements"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Spark connector"}),(0,t.jsx)(n.th,{children:"Spark"}),(0,t.jsx)(n.th,{children:"StarRocks"}),(0,t.jsx)(n.th,{children:"Java"}),(0,t.jsx)(n.th,{children:"Scala"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"1.1.1"}),(0,t.jsx)(n.td,{children:"3.2, 3.3, or 3.4"}),(0,t.jsx)(n.td,{children:"2.5 and later"}),(0,t.jsx)(n.td,{children:"8"}),(0,t.jsx)(n.td,{children:"2.12"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"1.1.0"}),(0,t.jsx)(n.td,{children:"3.2, 3.3, or 3.4"}),(0,t.jsx)(n.td,{children:"2.5 and later"}),(0,t.jsx)(n.td,{children:"8"}),(0,t.jsx)(n.td,{children:"2.12"})]})]})]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Please see ",(0,t.jsx)(n.a,{href:"#upgrade-spark-connector",children:"Upgrade Spark connector"})," for behaviour changes among different versions of the Spark connector."]}),"\n",(0,t.jsxs)(n.li,{children:["The Spark connector does not provide MySQL JDBC driver since version 1.1.1, and you need import the driver to the spark classpath manually. You can find the driver on ",(0,t.jsx)(n.a,{href:"https://dev.mysql.com/downloads/connector/j/",children:"MySQL site"})," or ",(0,t.jsx)(n.a,{href:"https://repo1.maven.org/maven2/mysql/mysql-connector-java/",children:"Maven Central"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"obtain-spark-connector",children:"Obtain Spark connector"}),"\n",(0,t.jsx)(n.p,{children:"You can obtain the Spark connector JAR file in the following ways:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Directly download the compiled Spark Connector JAR file."}),"\n",(0,t.jsx)(n.li,{children:"Add the Spark connector as a dependency in your Maven project and then download the JAR file."}),"\n",(0,t.jsx)(n.li,{children:"Compile the source code of the Spark Connector into a JAR file by yourself."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The naming format of the Spark connector JAR file is ",(0,t.jsx)(n.code,{children:"starrocks-spark-connector-${spark_version}_${scala_version}-${connector_version}.jar"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["For example, if you install Spark 3.2 and Scala 2.12 in your environment and you want to use Spark connector 1.1.0, you can use ",(0,t.jsx)(n.code,{children:"starrocks-spark-connector-3.2_2.12-1.1.0.jar"}),"."]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,t.jsx)(n.p,{children:"In general, the latest version of the Spark connector only maintains compatibility with the three most recent versions of Spark."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"download-the-compiled-jar-file",children:"Download the compiled Jar file"}),"\n",(0,t.jsxs)(n.p,{children:["Directly download the corresponding version of the Spark connector JAR from the ",(0,t.jsx)(n.a,{href:"https://repo1.maven.org/maven2/com/starrocks",children:"Maven Central Repository"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"maven-dependency",children:"Maven Dependency"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["In your Maven project's ",(0,t.jsx)(n.code,{children:"pom.xml"})," file, add the Spark connector as a dependency according to the following format. Replace ",(0,t.jsx)(n.code,{children:"spark_version"}),", ",(0,t.jsx)(n.code,{children:"scala_version"}),", and ",(0,t.jsx)(n.code,{children:"connector_version"})," with the respective versions."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:"<dependency>\n<groupId>com.starrocks</groupId>\n<artifactId>starrocks-spark-connector-${spark_version}_${scala_version}</artifactId>\n<version>${connector_version}</version>\n</dependency>\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"For example, if the version of Spark in your environment is 3.2, the version of Scala is 2.12, and you choose Spark connector 1.1.0, you need to add the following dependency:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:"<dependency>\n<groupId>com.starrocks</groupId>\n<artifactId>starrocks-spark-connector-3.2_2.12</artifactId>\n<version>1.1.0</version>\n</dependency>\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"compile-by-yourself",children:"Compile by yourself"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Download the ",(0,t.jsx)(n.a,{href:"https://github.com/StarRocks/starrocks-connector-for-apache-spark",children:"Spark connector package"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Execute the following command to compile the source code of Spark connector into a JAR file. Note that  ",(0,t.jsx)(n.code,{children:"spark_version"})," is replaced with the corresponding Spark version."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sh build.sh <spark_version>\n"})}),"\n",(0,t.jsx)(n.p,{children:"For example, if the Spark version in your environment is 3.2, you need to execute the following command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sh build.sh 3.2\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Go to the ",(0,t.jsx)(n.code,{children:"target/"})," directory to find the Spark connector JAR file, such as ",(0,t.jsx)(n.code,{children:"starrocks-spark-connector-3.2_2.12-1.1.0-SNAPSHOT.jar"})," , generated upon compilation."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,t.jsxs)(n.p,{children:["The name of Spark connector which is not formally released contains the ",(0,t.jsx)(n.code,{children:"SNAPSHOT"})," suffix."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"parameters",children:"Parameters"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Parameter"}),(0,t.jsx)(n.th,{children:"Required"}),(0,t.jsx)(n.th,{children:"Default value"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.fe.http.url"}),(0,t.jsx)(n.td,{children:"YES"}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsxs)(n.td,{children:["The HTTP URL of the FE in your StarRocks cluster. You can specify multiple URLs, which must be separated by a comma (,). Format: ",(0,t.jsx)(n.code,{children:"<fe_host1>:<fe_http_port1>,<fe_host2>:<fe_http_port2>"}),". Since version 1.1.1, you can also add ",(0,t.jsx)(n.code,{children:"http://"})," prefix to the URL, such as ",(0,t.jsx)(n.code,{children:"http://<fe_host1>:<fe_http_port1>,http://<fe_host2>:<fe_http_port2>"}),"."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.fe.jdbc.url"}),(0,t.jsx)(n.td,{children:"YES"}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsxs)(n.td,{children:["The address that is used to connect to the MySQL server of the FE. Format: ",(0,t.jsx)(n.code,{children:"jdbc:mysql://<fe_host>:<fe_query_port>"}),"."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.table.identifier"}),(0,t.jsx)(n.td,{children:"YES"}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsxs)(n.td,{children:["The name of the StarRocks table. Format: ",(0,t.jsx)(n.code,{children:"<database_name>.<table_name>"}),"."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.user"}),(0,t.jsx)(n.td,{children:"YES"}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsxs)(n.td,{children:["The username of your StarRocks cluster account. The user needs the ",(0,t.jsx)(n.a,{href:"/doc/en/sql-reference/sql-statements/account-management/GRANT",children:"SELECT and INSERT privileges"})," on the StarRocks table."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.password"}),(0,t.jsx)(n.td,{children:"YES"}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsx)(n.td,{children:"The password of your StarRocks cluster account."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.label.prefix"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"spark-"}),(0,t.jsx)(n.td,{children:"The label prefix used by Stream Load."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.enable.transaction-stream-load"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"TRUE"}),(0,t.jsxs)(n.td,{children:["Whether to use ",(0,t.jsx)(n.a,{href:"../loading/Stream_Load_transaction_interface",children:"Stream Load transaction interface"})," to load data. It requires StarRocks v2.5 or later. This feature can load more data in a transaction with less memory usage, and improve performance. ",(0,t.jsx)("br",{})," ",(0,t.jsx)(n.strong,{children:"NOTICE:"})," Since 1.1.1, this parameter takes effect only when the value of ",(0,t.jsx)(n.code,{children:"starrocks.write.max.retries"})," is non-positive because Stream Load transaction interface does not support retry."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.buffer.size"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"104857600"}),(0,t.jsx)(n.td,{children:"The maximum size of data that can be accumulated in memory before being sent to StarRocks at a time. Setting this parameter to a larger value can improve loading performance but may increase loading latency."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.buffer.rows"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"Integer.MAX_VALUE"}),(0,t.jsx)(n.td,{children:"Supported since version 1.1.1. The maximum number of rows that can be accumulated in memory before being sent to StarRocks at a time."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.flush.interval.ms"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"300000"}),(0,t.jsx)(n.td,{children:"The interval at which data is sent to StarRocks. This parameter is used to control the loading latency."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.max.retries"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"3"}),(0,t.jsxs)(n.td,{children:["Supported since version 1.1.1. The number of times that the connector retries to perform the Stream Load for the same batch of data if the load fails. ",(0,t.jsx)("br",{})," ",(0,t.jsx)(n.strong,{children:"NOTICE:"})," Because Stream Load transaction interface does not support retry. If this parameter is positive, the connector always use Stream Load interface and ignore the value of ",(0,t.jsx)(n.code,{children:"starrocks.write.enable.transaction-stream-load"}),"."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.retry.interval.ms"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"10000"}),(0,t.jsx)(n.td,{children:"Supported since version 1.1.1. The interval to retry the Stream Load for the same batch of data if the load fails."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.columns"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsxs)(n.td,{children:["The StarRocks table column into which you want to load data. You can specify multiple columns, which must be separated by commas (,), for example, ",(0,t.jsx)(n.code,{children:'"col0,col1,col2"'}),"."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.column.types"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsxs)(n.td,{children:["Supported since version 1.1.1. Customize the column data types for Spark instead of using the defaults inferred from the StarRocks table and the ",(0,t.jsx)(n.a,{href:"#data-type-mapping-between-spark-and-starrocks",children:"default mapping"}),". The parameter value is a schema in DDL format same as the output of Spark ",(0,t.jsx)(n.a,{href:"https://github.com/apache/spark/blob/master/sql/api/src/main/scala/org/apache/spark/sql/types/StructType.scala#L449",children:"StructType#toDDL"})," , such as ",(0,t.jsx)(n.code,{children:"col0 INT, col1 STRING, col2 BIGINT"}),". Note that you only need to specify columns that need customization. One use case is to load data into columns of ",(0,t.jsx)(n.a,{href:"#load-data-into-columns-of-bitmap-type",children:"BITMAP"})," or ",(0,t.jsx)(n.a,{href:"#load-data-into-columns-of-hll-type",children:"HLL"})," type."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.properties.*"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsxs)(n.td,{children:["The parameters that are used to control Stream Load behavior.  For example, the parameter ",(0,t.jsx)(n.code,{children:"starrocks.write.properties.format"})," specifies the format of the data to be loaded, such as CSV or JSON. For a list of supported parameters and their descriptions, see ",(0,t.jsx)(n.a,{href:"/doc/en/sql-reference/sql-statements/data-manipulation/STREAM_LOAD",children:"STREAM LOAD"}),"."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.properties.format"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"CSV"}),(0,t.jsx)(n.td,{children:"The file format based on which the Spark connector transforms each batch of data before the data is sent to StarRocks. Valid values: CSV and JSON."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.properties.row_delimiter"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"\\n"}),(0,t.jsx)(n.td,{children:"The row delimiter for CSV-formatted data."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.properties.column_separator"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"\\t"}),(0,t.jsx)(n.td,{children:"The column separator for CSV-formatted data."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.num.partitions"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsx)(n.td,{children:"The number of partitions into which Spark can write data in parallel. When the data volume is small, you can reduce the number of partitions to lower the loading concurrency and frequency. The default value for this parameter is determined by Spark. However, this method may cause Spark Shuffle cost."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.write.partition.columns"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"None"}),(0,t.jsxs)(n.td,{children:["The partitioning columns in Spark. The parameter takes effect only when ",(0,t.jsx)(n.code,{children:"starrocks.write.num.partitions"})," is specified. If this parameter is not specified, all columns being written are used for partitioning."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"starrocks.timezone"}),(0,t.jsx)(n.td,{children:"NO"}),(0,t.jsx)(n.td,{children:"Default timezone of JVM"}),(0,t.jsxs)(n.td,{children:["Supported since 1.1.1. The timezone used to convert Spark ",(0,t.jsx)(n.code,{children:"TimestampType"})," to StarRocks ",(0,t.jsx)(n.code,{children:"DATETIME"}),". The default is the timezone of JVM returned by ",(0,t.jsx)(n.code,{children:"ZoneId#systemDefault()"}),". The format can be a timezone name such as ",(0,t.jsx)(n.code,{children:"Asia/Shanghai"}),", or a zone offset such as ",(0,t.jsx)(n.code,{children:"+08:00"}),"."]})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"data-type-mapping-between-spark-and-starrocks",children:"Data type mapping between Spark and StarRocks"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The default data type mapping is as follows:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Spark data type"}),(0,t.jsx)(n.th,{children:"StarRocks data type"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"BooleanType"}),(0,t.jsx)(n.td,{children:"BOOLEAN"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"ByteType"}),(0,t.jsx)(n.td,{children:"TINYINT"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"ShortType"}),(0,t.jsx)(n.td,{children:"SMALLINT"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"IntegerType"}),(0,t.jsx)(n.td,{children:"INT"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"LongType"}),(0,t.jsx)(n.td,{children:"BIGINT"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"StringType"}),(0,t.jsx)(n.td,{children:"LARGEINT"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"FloatType"}),(0,t.jsx)(n.td,{children:"FLOAT"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"DoubleType"}),(0,t.jsx)(n.td,{children:"DOUBLE"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"DecimalType"}),(0,t.jsx)(n.td,{children:"DECIMAL"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"StringType"}),(0,t.jsx)(n.td,{children:"CHAR"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"StringType"}),(0,t.jsx)(n.td,{children:"VARCHAR"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"StringType"}),(0,t.jsx)(n.td,{children:"STRING"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"DateType"}),(0,t.jsx)(n.td,{children:"DATE"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"TimestampType"}),(0,t.jsx)(n.td,{children:"DATETIME"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"ArrayType"}),(0,t.jsxs)(n.td,{children:["ARRAY ",(0,t.jsx)("br",{})," ",(0,t.jsx)(n.strong,{children:"NOTE:"})," ",(0,t.jsx)("br",{})," ",(0,t.jsx)(n.strong,{children:"Supported since version 1.1.1"}),". For detailed steps, see ",(0,t.jsx)(n.a,{href:"#load-data-into-columns-of-array-type",children:"Load data into columns of ARRAY type"}),"."]})]})]})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"You can also customize the data type mapping."}),"\n",(0,t.jsxs)(n.p,{children:["For example, a StarRocks table contains BITMAP and HLL columns, but Spark does not support the two data types. You need to customize the corresponding data types in Spark. For detailed steps, see load data into ",(0,t.jsx)(n.a,{href:"#load-data-into-columns-of-bitmap-type",children:"BITMAP"})," and ",(0,t.jsx)(n.a,{href:"#load-data-into-columns-of-hll-type",children:"HLL"})," columns. ",(0,t.jsx)(n.strong,{children:"BITMAP and HLL are supported since version 1.1.1"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"upgrade-spark-connector",children:"Upgrade Spark connector"}),"\n",(0,t.jsx)(n.h3,{id:"upgrade-from-version-110-to-111",children:"Upgrade from version 1.1.0 to 1.1.1"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Since 1.1.1, the Spark connector does not provide ",(0,t.jsx)(n.code,{children:"mysql-connector-java"})," which is the official JDBC driver for MySQL, because of the limitations of the GPL license used by ",(0,t.jsx)(n.code,{children:"mysql-connector-java"}),".\nHowever, the Spark connector still needs the MySQL JDBC driver to connect to StarRocks for the table metadata, so you need to add the driver to the Spark classpath manually. You can find the\ndriver on ",(0,t.jsx)(n.a,{href:"https://dev.mysql.com/downloads/connector/j/",children:"MySQL site"})," or ",(0,t.jsx)(n.a,{href:"https://repo1.maven.org/maven2/mysql/mysql-connector-java/",children:"Maven Central"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Since 1.1.1, the connector uses Stream Load interface by default rather than Stream Load transaction interface in version 1.1.0. If you still want to use Stream Load transaction interface, you\ncan set the option ",(0,t.jsx)(n.code,{children:"starrocks.write.max.retries"})," to ",(0,t.jsx)(n.code,{children:"0"}),". Please see the description of ",(0,t.jsx)(n.code,{children:"starrocks.write.enable.transaction-stream-load"})," and ",(0,t.jsx)(n.code,{children:"starrocks.write.max.retries"}),"\nfor details."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,t.jsx)(n.p,{children:"The following examples show how to use the Spark connector to load data into a StarRocks table with Spark DataFrames or Spark SQL. The Spark DataFrames supports both Batch and Structured Streaming modes."}),"\n",(0,t.jsxs)(n.p,{children:["For more examples, see ",(0,t.jsx)(n.a,{href:"https://github.com/StarRocks/starrocks-connector-for-apache-spark/tree/main/src/test/java/com/starrocks/connector/spark/examples",children:"Spark Connector Examples"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"preparations",children:"Preparations"}),"\n",(0,t.jsx)(n.h4,{id:"create-a-starrocks-table",children:"Create a StarRocks table"}),"\n",(0,t.jsxs)(n.p,{children:["Create a database ",(0,t.jsx)(n.code,{children:"test"})," and create a Primary Key table ",(0,t.jsx)(n.code,{children:"score_board"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'CREATE DATABASE `test`;\n\nCREATE TABLE `test`.`score_board`\n(\n    `id` int(11) NOT NULL COMMENT "",\n    `name` varchar(65533) NULL DEFAULT "" COMMENT "",\n    `score` int(11) NOT NULL DEFAULT "0" COMMENT ""\n)\nENGINE=OLAP\nPRIMARY KEY(`id`)\nCOMMENT "OLAP"\nDISTRIBUTED BY HASH(`id`);\n'})}),"\n",(0,t.jsx)(n.h4,{id:"set-up-your-spark-environment",children:"Set up your Spark environment"}),"\n",(0,t.jsxs)(n.p,{children:["Note that the following examples are run in Spark 3.2.4 and use ",(0,t.jsx)(n.code,{children:"spark-shell"}),", ",(0,t.jsx)(n.code,{children:"pyspark"})," and ",(0,t.jsx)(n.code,{children:"spark-sql"}),". Before running the examples, make sure to place the Spark connector JAR file in the ",(0,t.jsx)(n.code,{children:"$SPARK_HOME/jars"})," directory."]}),"\n",(0,t.jsx)(n.h3,{id:"load-data-with-spark-dataframes",children:"Load data with Spark DataFrames"}),"\n",(0,t.jsx)(n.p,{children:"The following two examples explain how to load data with Spark DataFrames Batch or Structured Streaming mode."}),"\n",(0,t.jsx)(n.h4,{id:"batch",children:"Batch"}),"\n",(0,t.jsx)(n.p,{children:"Construct data in memory and load data into the StarRocks table."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"You can write the spark application using Scala or Python."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["For Scala, run the following code snippet in ",(0,t.jsx)(n.code,{children:"spark-shell"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-Scala",children:'// 1. Create a DataFrame from a sequence.\nval data = Seq((1, "starrocks", 100), (2, "spark", 100))\nval df = data.toDF("id", "name", "score")\n\n// 2. Write to StarRocks by configuring the format as "starrocks" and the following options. \n// You need to modify the options according your own environment.\ndf.write.format("starrocks")\n    .option("starrocks.fe.http.url", "127.0.0.1:8030")\n    .option("starrocks.fe.jdbc.url", "jdbc:mysql://127.0.0.1:9030")\n    .option("starrocks.table.identifier", "test.score_board")\n    .option("starrocks.user", "root")\n    .option("starrocks.password", "")\n    .mode("append")\n    .save()\n'})}),"\n",(0,t.jsxs)(n.p,{children:["For Python, run the following code snippet in ",(0,t.jsx)(n.code,{children:"pyspark"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from pyspark.sql import SparkSession\n\nspark = SparkSession \\\n     .builder \\\n     .appName("StarRocks Example") \\\n     .getOrCreate()\n\n # 1. Create a DataFrame from a sequence.\n data = [(1, "starrocks", 100), (2, "spark", 100)]\n df = spark.sparkContext.parallelize(data) \\\n         .toDF(["id", "name", "score"])\n\n # 2. Write to StarRocks by configuring the format as "starrocks" and the following options. \n # You need to modify the options according your own environment.\n df.write.format("starrocks") \\\n     .option("starrocks.fe.http.url", "127.0.0.1:8030") \\\n     .option("starrocks.fe.jdbc.url", "jdbc:mysql://127.0.0.1:9030") \\\n     .option("starrocks.table.identifier", "test.score_board") \\\n     .option("starrocks.user", "root") \\\n     .option("starrocks.password", "") \\\n     .mode("append") \\\n     .save()\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Query data in the StarRocks table."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"MySQL [test]> SELECT * FROM `score_board`;\n+------+-----------+-------+\n| id   | name      | score |\n+------+-----------+-------+\n|    1 | starrocks |   100 |\n|    2 | spark     |   100 |\n+------+-----------+-------+\n2 rows in set (0.00 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"structured-streaming",children:"Structured Streaming"}),"\n",(0,t.jsx)(n.p,{children:"Construct a streaming read of data from a CSV file and load data into the StarRocks table."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["In the directory ",(0,t.jsx)(n.code,{children:"csv-data"}),", create a CSV file ",(0,t.jsx)(n.code,{children:"test.csv"})," with the following data:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csv",children:"3,starrocks,100\n4,spark,100\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"You can write the Spark application using Scala or Python."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["For Scala, run the following code snippet in ",(0,t.jsx)(n.code,{children:"spark-shell"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-Scala",children:'import org.apache.spark.sql.types.StructType\n\n// 1. Create a DataFrame from CSV.\nval schema = (new StructType()\n        .add("id", "integer")\n        .add("name", "string")\n        .add("score", "integer")\n    )\nval df = (spark.readStream\n        .option("sep", ",")\n        .schema(schema)\n        .format("csv") \n        // Replace it with your path to the directory "csv-data".\n        .load("/path/to/csv-data")\n    )\n\n// 2. Write to StarRocks by configuring the format as "starrocks" and the following options. \n// You need to modify the options according your own environment.\nval query = (df.writeStream.format("starrocks")\n        .option("starrocks.fe.http.url", "127.0.0.1:8030")\n        .option("starrocks.fe.jdbc.url", "jdbc:mysql://127.0.0.1:9030")\n        .option("starrocks.table.identifier", "test.score_board")\n        .option("starrocks.user", "root")\n        .option("starrocks.password", "")\n        // replace it with your checkpoint directory\n        .option("checkpointLocation", "/path/to/checkpoint")\n        .outputMode("append")\n        .start()\n    )\n'})}),"\n",(0,t.jsxs)(n.p,{children:["For Python, run the following code snippet in ",(0,t.jsx)(n.code,{children:"pyspark"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from pyspark.sql import SparkSession\nfrom pyspark.sql.types import IntegerType, StringType, StructType, StructField\n\nspark = SparkSession \\\n     .builder \\\n     .appName("StarRocks SS Example") \\\n     .getOrCreate()\n\n # 1. Create a DataFrame from CSV.\n schema = StructType([ \\\n         StructField("id", IntegerType()), \\\n         StructField("name", StringType()), \\\n         StructField("score", IntegerType()) \\\n     ])\n df = spark.readStream \\\n         .option("sep", ",") \\\n         .schema(schema) \\\n         .format("csv") \\\n         # Replace it with your path to the directory "csv-data".\n         .load("/path/to/csv-data")\n\n # 2. Write to StarRocks by configuring the format as "starrocks" and the following options. \n # You need to modify the options according your own environment.\n query = df.writeStream.format("starrocks") \\\n         .option("starrocks.fe.http.url", "127.0.0.1:8030") \\\n         .option("starrocks.fe.jdbc.url", "jdbc:mysql://127.0.0.1:9030") \\\n         .option("starrocks.table.identifier", "test.score_board") \\\n         .option("starrocks.user", "root") \\\n         .option("starrocks.password", "") \\\n         # replace it with your checkpoint directory\n         .option("checkpointLocation", "/path/to/checkpoint") \\\n         .outputMode("append") \\\n         .start()\n     )\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Query data in the StarRocks table."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"MySQL [test]> select * from score_board;\n+------+-----------+-------+\n| id   | name      | score |\n+------+-----------+-------+\n|    4 | spark     |   100 |\n|    3 | starrocks |   100 |\n+------+-----------+-------+\n2 rows in set (0.67 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"load-data-with-spark-sql",children:"Load data with Spark SQL"}),"\n",(0,t.jsxs)(n.p,{children:["The following example explains how to load data with Spark SQL by using the ",(0,t.jsx)(n.code,{children:"INSERT INTO"})," statement in the ",(0,t.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/sql-distributed-sql-engine-spark-sql-cli.html",children:"Spark SQL CLI"}),"."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Execute the following SQL statement in the ",(0,t.jsx)(n.code,{children:"spark-sql"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'-- 1. Create a table by configuring the data source as  `starrocks` and the following options. \n-- You need to modify the options according your own environment.\nCREATE TABLE `score_board`\nUSING starrocks\nOPTIONS(\n"starrocks.fe.http.url"="127.0.0.1:8030",\n"starrocks.fe.jdbc.url"="jdbc:mysql://127.0.0.1:9030",\n"starrocks.table.identifier"="test.score_board",\n"starrocks.user"="root",\n"starrocks.password"=""\n);\n\n-- 2. Insert two rows into the table.\nINSERT INTO `score_board` VALUES (5, "starrocks", 100), (6, "spark", 100);\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Query data in the StarRocks table."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"MySQL [test]> select * from score_board;\n+------+-----------+-------+\n| id   | name      | score |\n+------+-----------+-------+\n|    6 | spark     |   100 |\n|    5 | starrocks |   100 |\n+------+-----------+-------+\n2 rows in set (0.00 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsx)(n.h3,{id:"load-data-to-primary-key-table",children:"Load data to Primary Key table"}),"\n",(0,t.jsxs)(n.p,{children:["This section will show how to load data to StarRocks Primary Key table to achieve partial updates, and conditional updates.\nYou can see ",(0,t.jsx)(n.a,{href:"../loading/Load_to_Primary_Key_tables",children:"Change data through loading"})," for the detailed introduction of these features.\nThese examples use Spark SQL."]}),"\n",(0,t.jsx)(n.h4,{id:"preparations-1",children:"Preparations"}),"\n",(0,t.jsxs)(n.p,{children:["Create a database ",(0,t.jsx)(n.code,{children:"test"})," and create a Primary Key table ",(0,t.jsx)(n.code,{children:"score_board"})," in StarRocks."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'CREATE DATABASE `test`;\n\nCREATE TABLE `test`.`score_board`\n(\n    `id` int(11) NOT NULL COMMENT "",\n    `name` varchar(65533) NULL DEFAULT "" COMMENT "",\n    `score` int(11) NOT NULL DEFAULT "0" COMMENT ""\n)\nENGINE=OLAP\nPRIMARY KEY(`id`)\nCOMMENT "OLAP"\nDISTRIBUTED BY HASH(`id`);\n'})}),"\n",(0,t.jsx)(n.h4,{id:"partial-updates",children:"Partial updates"}),"\n",(0,t.jsxs)(n.p,{children:["This example will show how to only update data in the column ",(0,t.jsx)(n.code,{children:"name"})," through loading:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Insert initial data to StarRocks table in MySQL client."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"mysql> INSERT INTO `score_board` VALUES (1, 'starrocks', 100), (2, 'spark', 100);\n\nmysql> select * from score_board;\n+------+-----------+-------+\n| id   | name      | score |\n+------+-----------+-------+\n|    1 | starrocks |   100 |\n|    2 | spark     |   100 |\n+------+-----------+-------+\n2 rows in set (0.02 sec)\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Create a Spark table ",(0,t.jsx)(n.code,{children:"score_board"})," in Spark SQL client."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Set the option ",(0,t.jsx)(n.code,{children:"starrocks.write.properties.partial_update"})," to ",(0,t.jsx)(n.code,{children:"true"})," which tells the connector to do partial update."]}),"\n",(0,t.jsxs)(n.li,{children:["Set the option ",(0,t.jsx)(n.code,{children:"starrocks.columns"})," to ",(0,t.jsx)(n.code,{children:'"id,name"'})," to tell the connector which columns to write."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'CREATE TABLE `score_board`\nUSING starrocks\nOPTIONS(\n    "starrocks.fe.http.url"="127.0.0.1:8030",\n    "starrocks.fe.jdbc.url"="jdbc:mysql://127.0.0.1:9030",\n    "starrocks.table.identifier"="test.score_board",\n    "starrocks.user"="root",\n    "starrocks.password"="",\n    "starrocks.write.properties.partial_update"="true",\n    "starrocks.columns"="id,name"\n );\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Insert data into the table in Spark SQL client, and only update the column ",(0,t.jsx)(n.code,{children:"name"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO `score_board` VALUES (1, 'starrocks-update'), (2, 'spark-update');\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Query the StarRocks table in MySQL client."}),"\n",(0,t.jsxs)(n.p,{children:["You can see that only values for ",(0,t.jsx)(n.code,{children:"name"})," change, and the values for ",(0,t.jsx)(n.code,{children:"score"})," does not change."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"mysql> select * from score_board;\n+------+------------------+-------+\n| id   | name             | score |\n+------+------------------+-------+\n|    1 | starrocks-update |   100 |\n|    2 | spark-update     |   100 |\n+------+------------------+-------+\n2 rows in set (0.02 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"conditional-updates",children:"Conditional updates"}),"\n",(0,t.jsxs)(n.p,{children:["This example will show how to do conditional updates according to the values of column ",(0,t.jsx)(n.code,{children:"score"}),". The update for an ",(0,t.jsx)(n.code,{children:"id"}),"\ntakes effect only when the new value for ",(0,t.jsx)(n.code,{children:"score"})," is has a greater or equal to the old value."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Insert initial data to StarRocks table in MySQL client."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"mysql> INSERT INTO `score_board` VALUES (1, 'starrocks', 100), (2, 'spark', 100);\n\nmysql> select * from score_board;\n+------+-----------+-------+\n| id   | name      | score |\n+------+-----------+-------+\n|    1 | starrocks |   100 |\n|    2 | spark     |   100 |\n+------+-----------+-------+\n2 rows in set (0.02 sec)\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Create a Spark table ",(0,t.jsx)(n.code,{children:"score_board"})," in the following ways."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Set the option ",(0,t.jsx)(n.code,{children:"starrocks.write.properties.merge_condition"})," to ",(0,t.jsx)(n.code,{children:"score"})," which tells the connector to use the column ",(0,t.jsx)(n.code,{children:"score"})," as the condition."]}),"\n",(0,t.jsx)(n.li,{children:"Make sure that the Spark connector use Stream Load interface to load data, rather than Stream Load transaction interface, because the latter does not support this feature."}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'CREATE TABLE `score_board`\nUSING starrocks\nOPTIONS(\n    "starrocks.fe.http.url"="127.0.0.1:8030",\n    "starrocks.fe.jdbc.url"="jdbc:mysql://127.0.0.1:9030",\n    "starrocks.table.identifier"="test.score_board",\n    "starrocks.user"="root",\n    "starrocks.password"="",\n    "starrocks.write.properties.merge_condition"="score"\n );\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Insert data to the table in Spark SQL client, and update the row whose ",(0,t.jsx)(n.code,{children:"id"})," is 1 with a smaller score value, and the row whose ",(0,t.jsx)(n.code,{children:"id"})," is 2 with a larger score value."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO `score_board` VALUES (1, 'starrocks-update', 99), (2, 'spark-update', 101);\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Query the StarRocks table in MySQL client."}),"\n",(0,t.jsxs)(n.p,{children:["You can see that only the row whose ",(0,t.jsx)(n.code,{children:"id"})," is 2 changes, and the row whose ",(0,t.jsx)(n.code,{children:"id"})," is 1 does not change."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"mysql> select * from score_board;\n+------+--------------+-------+\n| id   | name         | score |\n+------+--------------+-------+\n|    1 | starrocks    |   100 |\n|    2 | spark-update |   101 |\n+------+--------------+-------+\n2 rows in set (0.03 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"load-data-into-columns-of-bitmap-type",children:"Load data into columns of BITMAP type"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/doc/en/sql-reference/sql-statements/data-types/BITMAP",children:(0,t.jsx)(n.code,{children:"BITMAP"})})," is often used to accelerate count distinct, such as counting UV, see ",(0,t.jsx)(n.a,{href:"/doc/en/using_starrocks/Using_bitmap",children:"Use Bitmap for exact Count Distinct"}),".\nHere we take the counting of UV as an example to show how to load data into columns of the ",(0,t.jsx)(n.code,{children:"BITMAP"})," type. ",(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.code,{children:"BITMAP"})," is supported since version 1.1.1"]}),"."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create a StarRocks Aggregate table."}),"\n",(0,t.jsxs)(n.p,{children:["In the database ",(0,t.jsx)(n.code,{children:"test"}),", create an Aggregate table ",(0,t.jsx)(n.code,{children:"page_uv"})," where the column ",(0,t.jsx)(n.code,{children:"visit_users"})," is defined as the ",(0,t.jsx)(n.code,{children:"BITMAP"})," type and configured with the aggregate function ",(0,t.jsx)(n.code,{children:"BITMAP_UNION"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"CREATE TABLE `test`.`page_uv` (\n  `page_id` INT NOT NULL COMMENT 'page ID',\n  `visit_date` datetime NOT NULL COMMENT 'access time',\n  `visit_users` BITMAP BITMAP_UNION NOT NULL COMMENT 'user ID'\n) ENGINE=OLAP\nAGGREGATE KEY(`page_id`, `visit_date`)\nDISTRIBUTED BY HASH(`page_id`);\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create a Spark table."}),"\n",(0,t.jsxs)(n.p,{children:["The schema of the Spark table is inferred from the StarRocks table, and the Spark does not support the ",(0,t.jsx)(n.code,{children:"BITMAP"})," type. So you need to customize the corresponding column data type in Spark, for example as ",(0,t.jsx)(n.code,{children:"BIGINT"}),", by configuring the option ",(0,t.jsx)(n.code,{children:'"starrocks.column.types"="visit_users BIGINT"'}),". When using Stream Load to ingest data, the connector uses the ",(0,t.jsx)(n.a,{href:"/doc/en/sql-reference/sql-functions/bitmap-functions/to_bitmap",children:(0,t.jsx)(n.code,{children:"to_bitmap"})})," function to convert the data of ",(0,t.jsx)(n.code,{children:"BIGINT"})," type into ",(0,t.jsx)(n.code,{children:"BITMAP"})," type."]}),"\n",(0,t.jsxs)(n.p,{children:["Run the following DDL in ",(0,t.jsx)(n.code,{children:"spark-sql"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'CREATE TABLE `page_uv`\nUSING starrocks\nOPTIONS(\n   "starrocks.fe.http.url"="127.0.0.1:8030",\n   "starrocks.fe.jdbc.url"="jdbc:mysql://127.0.0.1:9030",\n   "starrocks.table.identifier"="test.page_uv",\n   "starrocks.user"="root",\n   "starrocks.password"="",\n   "starrocks.column.types"="visit_users BIGINT"\n);\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Load data into StarRocks table."}),"\n",(0,t.jsxs)(n.p,{children:["Run the following DML in ",(0,t.jsx)(n.code,{children:"spark-sql"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO `page_uv` VALUES\n   (1, CAST('2020-06-23 01:30:30' AS TIMESTAMP), 13),\n   (1, CAST('2020-06-23 01:30:30' AS TIMESTAMP), 23),\n   (1, CAST('2020-06-23 01:30:30' AS TIMESTAMP), 33),\n   (1, CAST('2020-06-23 02:30:30' AS TIMESTAMP), 13),\n   (2, CAST('2020-06-23 01:30:30' AS TIMESTAMP), 23);\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Calculate page UVs from the StarRocks table."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"MySQL [test]> SELECT `page_id`, COUNT(DISTINCT `visit_users`) FROM `page_uv` GROUP BY `page_id`;\n+---------+-----------------------------+\n| page_id | count(DISTINCT visit_users) |\n+---------+-----------------------------+\n|       2 |                           1 |\n|       1 |                           3 |\n+---------+-----------------------------+\n2 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"NOTICE:"})}),"\n",(0,t.jsxs)(n.p,{children:["The connector uses ",(0,t.jsx)(n.a,{href:"/doc/en/sql-reference/sql-functions/bitmap-functions/to_bitmap",children:(0,t.jsx)(n.code,{children:"to_bitmap"})}),"\nfunction to convert data of the ",(0,t.jsx)(n.code,{children:"TINYINT"}),", ",(0,t.jsx)(n.code,{children:"SMALLINT"}),", ",(0,t.jsx)(n.code,{children:"INTEGER"}),", and ",(0,t.jsx)(n.code,{children:"BIGINT"})," types in Spark to the ",(0,t.jsx)(n.code,{children:"BITMAP"})," type in StarRocks, and uses\n",(0,t.jsx)(n.a,{href:"/doc/en/sql-reference/sql-functions/bitmap-functions/bitmap_hash",children:(0,t.jsx)(n.code,{children:"bitmap_hash"})})," function for other Spark data types."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"load-data-into-columns-of-hll-type",children:"Load data into columns of HLL type"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/doc/en/sql-reference/sql-statements/data-types/HLL",children:(0,t.jsx)(n.code,{children:"HLL"})})," can be used for approximate count distinct, see ",(0,t.jsx)(n.a,{href:"/doc/en/using_starrocks/Using_HLL",children:"Use HLL for approximate count distinct"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["Here we take the counting of UV as an example to show how to load data into columns of the ",(0,t.jsx)(n.code,{children:"HLL"})," type.  ",(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.code,{children:"HLL"})," is supported since version 1.1.1"]}),"."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create a StarRocks Aggregate table."}),"\n",(0,t.jsxs)(n.p,{children:["In the database ",(0,t.jsx)(n.code,{children:"test"}),", create an Aggregate table ",(0,t.jsx)(n.code,{children:"hll_uv"})," where the column ",(0,t.jsx)(n.code,{children:"visit_users"})," is defined as the ",(0,t.jsx)(n.code,{children:"HLL"})," type and configured with the aggregate function ",(0,t.jsx)(n.code,{children:"HLL_UNION"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"CREATE TABLE `hll_uv` (\n`page_id` INT NOT NULL COMMENT 'page ID',\n`visit_date` datetime NOT NULL COMMENT 'access time',\n`visit_users` HLL HLL_UNION NOT NULL COMMENT 'user ID'\n) ENGINE=OLAP\nAGGREGATE KEY(`page_id`, `visit_date`)\nDISTRIBUTED BY HASH(`page_id`);\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create a Spark table."}),"\n",(0,t.jsxs)(n.p,{children:["The schema of the Spark table is inferred from the StarRocks table, and the Spark does not support the ",(0,t.jsx)(n.code,{children:"HLL"})," type. So you need to customize the corresponding column data type in Spark, for example as ",(0,t.jsx)(n.code,{children:"BIGINT"}),", by configuring the option ",(0,t.jsx)(n.code,{children:'"starrocks.column.types"="visit_users BIGINT"'}),". When using Stream Load to ingest data, the connector uses the ",(0,t.jsx)(n.a,{href:"../sql-reference/sql-functions/aggregate-functions/hll_hash",children:(0,t.jsx)(n.code,{children:"hll_hash"})})," function to convert the data of ",(0,t.jsx)(n.code,{children:"BIGINT"})," type into ",(0,t.jsx)(n.code,{children:"HLL"})," type."]}),"\n",(0,t.jsxs)(n.p,{children:["Run the following DDL in ",(0,t.jsx)(n.code,{children:"spark-sql"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'CREATE TABLE `hll_uv`\nUSING starrocks\nOPTIONS(\n   "starrocks.fe.http.url"="127.0.0.1:8030",\n   "starrocks.fe.jdbc.url"="jdbc:mysql://127.0.0.1:9030",\n   "starrocks.table.identifier"="test.hll_uv",\n   "starrocks.user"="root",\n   "starrocks.password"="",\n   "starrocks.column.types"="visit_users BIGINT"\n);\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Load data into StarRocks table."}),"\n",(0,t.jsxs)(n.p,{children:["Run the following DML in ",(0,t.jsx)(n.code,{children:"spark-sql"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO `hll_uv` VALUES\n   (3, CAST('2023-07-24 12:00:00' AS TIMESTAMP), 78),\n   (4, CAST('2023-07-24 13:20:10' AS TIMESTAMP), 2),\n   (3, CAST('2023-07-24 12:30:00' AS TIMESTAMP), 674);\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Calculate page UVs from the StarRocks table."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"MySQL [test]> SELECT `page_id`, COUNT(DISTINCT `visit_users`) FROM `hll_uv` GROUP BY `page_id`;\n+---------+-----------------------------+\n| page_id | count(DISTINCT visit_users) |\n+---------+-----------------------------+\n|       4 |                           1 |\n|       3 |                           2 |\n+---------+-----------------------------+\n2 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"load-data-into-columns-of-array-type",children:"Load data into columns of ARRAY type"}),"\n",(0,t.jsxs)(n.p,{children:["The following example explains how to load data into columns of the ",(0,t.jsx)(n.a,{href:"/doc/en/sql-reference/sql-statements/data-types/Array",children:(0,t.jsx)(n.code,{children:"ARRAY"})})," type."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create a StarRocks table."}),"\n",(0,t.jsxs)(n.p,{children:["In the database ",(0,t.jsx)(n.code,{children:"test"}),", create a Primary Key table ",(0,t.jsx)(n.code,{children:"array_tbl"})," that includes one ",(0,t.jsx)(n.code,{children:"INT"})," column and two ",(0,t.jsx)(n.code,{children:"ARRAY"})," columns."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:"CREATE TABLE `array_tbl` (\n    `id` INT NOT NULL,\n    `a0` ARRAY<STRING>,\n    `a1` ARRAY<ARRAY<INT>>\n )\n ENGINE=OLAP\n PRIMARY KEY(`id`)\n DISTRIBUTED BY HASH(`id`)\n ;\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Write data to StarRocks."}),"\n",(0,t.jsxs)(n.p,{children:["Because some versions of StarRocks does not provide the metadata of ",(0,t.jsx)(n.code,{children:"ARRAY"})," column, the connector can not infer the corresponding Spark data type for this column. However, you can explicitly specify the corresponding Spark data type of the column in the option ",(0,t.jsx)(n.code,{children:"starrocks.column.types"}),". In this example, you can configure the option as ",(0,t.jsx)(n.code,{children:"a0 ARRAY<STRING>,a1 ARRAY<ARRAY<INT>>"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["Run the following codes in ",(0,t.jsx)(n.code,{children:"spark-shell"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-scala",children:' val data = Seq(\n    |  (1, Seq("hello", "starrocks"), Seq(Seq(1, 2), Seq(3, 4))),\n    |  (2, Seq("hello", "spark"), Seq(Seq(5, 6, 7), Seq(8, 9, 10)))\n    | )\n val df = data.toDF("id", "a0", "a1")\n df.write\n      .format("starrocks")\n      .option("starrocks.fe.http.url", "127.0.0.1:8030")\n      .option("starrocks.fe.jdbc.url", "jdbc:mysql://127.0.0.1:9030")\n      .option("starrocks.table.identifier", "test.array_tbl")\n      .option("starrocks.user", "root")\n      .option("starrocks.password", "")\n      .option("starrocks.column.types", "a0 ARRAY<STRING>,a1 ARRAY<ARRAY<INT>>")\n      .mode("append")\n      .save()\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Query data in the StarRocks table."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-SQL",children:'MySQL [test]> SELECT * FROM `array_tbl`;\n+------+-----------------------+--------------------+\n| id   | a0                    | a1                 |\n+------+-----------------------+--------------------+\n|    1 | ["hello","starrocks"] | [[1,2],[3,4]]      |\n|    2 | ["hello","spark"]     | [[5,6,7],[8,9,10]] |\n+------+-----------------------+--------------------+\n2 rows in set (0.01 sec)\n'})}),"\n"]}),"\n"]})]})}const h=function(e={}){const{wrapper:n}=Object.assign({},(0,s.ah)(),e.components);return n?(0,t.jsx)(n,Object.assign({},e,{children:(0,t.jsx)(l,e)})):l(e)}},11151:(e,n,r)=>{r.d(n,{Zo:()=>c,ah:()=>a});var t=r(67294);const s=t.createContext({});function a(e){const n=t.useContext(s);return t.useMemo((()=>"function"==typeof e?e(n):{...n,...e}),[n,e])}const o={};function c({components:e,children:n,disableParentContext:r}){let c;return c=r?"function"==typeof e?e({}):e||o:a(e),t.createElement(s.Provider,{value:c},n)}}}]);