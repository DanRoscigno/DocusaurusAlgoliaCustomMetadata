"use strict";(self.webpackChunkstarrocks=self.webpackChunkstarrocks||[]).push([[97270],{30028:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>i});var s=a(85893),d=a(11151);const o={displayed_sidebar:"English"},r="Change data through loading",t={id:"loading/Load_to_Primary_Key_tables",title:"Change data through loading",description:"Primary Key tables provided by StarRocks allows you to make data changes to StarRocks tables by running Stream Load, Broker Load, or Routine Load jobs. These data changes include inserts, updates, and deletions. However, Primary Key tables do not support changing data by using Spark Load or INSERT.",source:"@site/versioned_docs/version-2.5/loading/Load_to_Primary_Key_tables.md",sourceDirName:"loading",slug:"/loading/Load_to_Primary_Key_tables",permalink:"/doc/docs/2.5/loading/Load_to_Primary_Key_tables",draft:!1,unlisted:!1,editUrl:"https://github.com/StarRocks/starrocks/edit/main/docs/loading/Load_to_Primary_Key_tables.md",tags:[],version:"2.5",frontMatter:{displayed_sidebar:"English"},sidebar:"English",previous:{title:"Load data using Kafka connector",permalink:"/doc/docs/2.5/loading/Kafka-connector-starrocks"},next:{title:"Overview of data loading",permalink:"/doc/docs/2.5/loading/Loading_intro"}},l={},i=[{value:"Implementation",id:"implementation",level:2},{value:"Usage notes",id:"usage-notes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Broker Load",id:"broker-load",level:3},{value:"Routine load",id:"routine-load",level:3},{value:"Basic operations",id:"basic-operations",level:2},{value:"UPSERT",id:"upsert",level:3},{value:"Data examples",id:"data-examples",level:4},{value:"Load data",id:"load-data",level:4},{value:"Query data",id:"query-data",level:4},{value:"DELETE",id:"delete",level:3},{value:"Data examples",id:"data-examples-1",level:4},{value:"Load data",id:"load-data-1",level:4},{value:"Query data",id:"query-data-1",level:4},{value:"UPSERT and DELETE",id:"upsert-and-delete",level:3},{value:"Data examples",id:"data-examples-2",level:4},{value:"Load data",id:"load-data-2",level:4},{value:"Query data",id:"query-data-2",level:4},{value:"Partial updates",id:"partial-updates",level:2},{value:"Data examples",id:"data-examples-3",level:3},{value:"Load data",id:"load-data-3",level:3},{value:"Query data",id:"query-data-3",level:3},{value:"Conditional updates",id:"conditional-updates",level:2},{value:"Data examples",id:"data-examples-4",level:3},{value:"Load data",id:"load-data-4",level:3},{value:"Query data",id:"query-data-4",level:3}];function c(e){const n=Object.assign({h1:"h1",p:"p",a:"a",blockquote:"blockquote",strong:"strong",h2:"h2",code:"code",ul:"ul",li:"li",h3:"h3",h4:"h4",ol:"ol",pre:"pre"},(0,d.ah)(),e.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"change-data-through-loading",children:"Change data through loading"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"/doc/docs/2.5/table_design/table_types/primary_key_table",children:"Primary Key tables"})," provided by StarRocks allows you to make data changes to StarRocks tables by running ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/loading/StreamLoad",children:"Stream Load"}),", ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/loading/BrokerLoad",children:"Broker Load"}),", or ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/loading/RoutineLoad",children:"Routine Load"})," jobs. These data changes include inserts, updates, and deletions. However, Primary Key tables do not support changing data by using ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/loading/SparkLoad",children:"Spark Load"})," or ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/loading/InsertInto",children:"INSERT"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"StarRocks also supports partial updates and conditional updates."}),"\n",(0,s.jsx)(n.p,{children:"This topic uses CSV data as an example to describe how to make data changes to a StarRocks table through loading. The data file formats that are supported vary depending on the loading method of your choice."}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsx)(n.p,{children:"For CSV data, you can use a UTF-8 string, such as a comma (,), tab, or pipe (|), whose length does not exceed 50 bytes as a text delimiter."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,s.jsx)(n.p,{children:"Primary Key tables provided by StarRocks support UPSERT and DELETE operations and does not distinguish INSERT operations from UPDATE operations."}),"\n",(0,s.jsxs)(n.p,{children:["When you create a load job, StarRocks supports adding a field named ",(0,s.jsx)(n.code,{children:"__op"})," to the job creation statement or command. The ",(0,s.jsx)(n.code,{children:"__op"})," field is used to specify the type of operation you want to perform."]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsxs)(n.p,{children:["When you create a table, you do not need to add a column named ",(0,s.jsx)(n.code,{children:"__op"})," to that table."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The method of defining the ",(0,s.jsx)(n.code,{children:"__op"})," field varies depending on the loading method of your choice:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If you choose Stream Load, define the ",(0,s.jsx)(n.code,{children:"__op"})," field by using the ",(0,s.jsx)(n.code,{children:"columns"})," parameter."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If you choose Broker Load, define the ",(0,s.jsx)(n.code,{children:"__op"})," field by using the SET clause."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If you choose Routine Load, define the ",(0,s.jsx)(n.code,{children:"__op"})," field by using the ",(0,s.jsx)(n.code,{children:"COLUMNS"})," column."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["You can decide whether to add the ",(0,s.jsx)(n.code,{children:"__op"})," field based on the data changes you want to make. If you do not add the ",(0,s.jsx)(n.code,{children:"__op"})," field, the operation type defaults to UPSERT. The major data change scenarios are as follows:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If the data file you want to load involves only UPSERT operations, you do not need to add the ",(0,s.jsx)(n.code,{children:"__op"})," field."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If the data file you want to load involves only DELETE operations, you must add the ",(0,s.jsx)(n.code,{children:"__op"})," field and specify the operation type as DELETE."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If the data file you want to load involves both UPSERT and DELETE operations, you must add the ",(0,s.jsx)(n.code,{children:"__op"})," field and make sure that the data file contains a column whose values are ",(0,s.jsx)(n.code,{children:"0"})," or ",(0,s.jsx)(n.code,{children:"1"}),". A value of ",(0,s.jsx)(n.code,{children:"0"})," indicates an UPSERT operation, and a value of ",(0,s.jsx)(n.code,{children:"1"})," indicates a DELETE operation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"usage-notes",children:"Usage notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Make sure that each row in your data file has the same number of columns."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The columns that involve data changes must include the primary key column."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.h3,{id:"broker-load",children:"Broker Load"}),"\n",(0,s.jsxs)(n.p,{children:['See the "Background information" section in ',(0,s.jsx)(n.a,{href:"/doc/docs/2.5/loading/BrokerLoad",children:"Load data from HDFS or cloud storage"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"routine-load",children:"Routine load"}),"\n",(0,s.jsxs)(n.p,{children:["If you choose Routine Load, make sure that topics are created in your Apache Kafka\xae cluster. Assume that you have created four topics: ",(0,s.jsx)(n.code,{children:"topic1"}),", ",(0,s.jsx)(n.code,{children:"topic2"}),", ",(0,s.jsx)(n.code,{children:"topic3"}),", and ",(0,s.jsx)(n.code,{children:"topic4"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"basic-operations",children:"Basic operations"}),"\n",(0,s.jsxs)(n.p,{children:["This section provides examples of how to make data changes to a StarRocks table through loading. For detailed syntax and parameter descriptions, see ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/sql-reference/sql-statements/data-manipulation/STREAM_LOAD",children:"STREAM LOAD"}),", ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/sql-reference/sql-statements/data-manipulation/BROKER_LOAD",children:"BROKER LOAD"}),", and ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/sql-reference/sql-statements/data-manipulation/CREATE_ROUTINE_LOAD",children:"CREATE ROUTINE LOAD"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"upsert",children:"UPSERT"}),"\n",(0,s.jsxs)(n.p,{children:["If the data file you want to load involves only UPSERT operations, you do not need to add the ",(0,s.jsx)(n.code,{children:"__op"})," field."]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsxs)(n.p,{children:["If you add the ",(0,s.jsx)(n.code,{children:"__op"})," field:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"You can specify the operation type as UPSERT."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["You can leave the ",(0,s.jsx)(n.code,{children:"__op"})," field empty, because the operation type defaults to UPSERT."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"data-examples",children:"Data examples"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Prepare a data file."}),"\n",(0,s.jsxs)(n.p,{children:["a. Create a CSV file named ",(0,s.jsx)(n.code,{children:"example1.csv"})," in your local file system. The file consists of three columns, which represent user ID, user name, and user score in sequence."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain",children:"101,Lily,100\n102,Rose,100\n"})}),"\n",(0,s.jsxs)(n.p,{children:["b. Publish the data of ",(0,s.jsx)(n.code,{children:"example1.csv"})," to ",(0,s.jsx)(n.code,{children:"topic1"})," of your Kafka cluster."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Prepare a StarRocks table."}),"\n",(0,s.jsxs)(n.p,{children:["a. Create a Primary Key table named ",(0,s.jsx)(n.code,{children:"table1"})," in your StarRocks database ",(0,s.jsx)(n.code,{children:"test_db"}),". The table consists of three columns: ",(0,s.jsx)(n.code,{children:"id"}),", ",(0,s.jsx)(n.code,{children:"name"}),", and ",(0,s.jsx)(n.code,{children:"score"}),", of which ",(0,s.jsx)(n.code,{children:"id"})," is the primary key."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE TABLE `table1`\n(\n    `id` int(11) NOT NULL COMMENT "user ID",\n    `name` varchar(65533) NOT NULL COMMENT "user name",\n    `score` int(11) NOT NULL COMMENT "user score"\n)\nENGINE=OLAP\nPRIMARY KEY(`id`)\nDISTRIBUTED BY HASH(`id`);\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsxs)(n.p,{children:["Since v2.5.7, StarRocks can automatically set the number of buckets (BUCKETS) when you create a table or add a partition. You no longer need to manually set the number of buckets. For detailed information, see ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/table_design/Data_distribution#determine-the-number-of-buckets",children:"determine the number of buckets"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["b. Insert a record into ",(0,s.jsx)(n.code,{children:"table1"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO table1 VALUES\n    (101, 'Lily',80);\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"load-data",children:"Load data"}),"\n",(0,s.jsxs)(n.p,{children:["Run a load job to update the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"101"})," in ",(0,s.jsx)(n.code,{children:"example1.csv"})," to ",(0,s.jsx)(n.code,{children:"table1"})," and insert the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"102"})," in ",(0,s.jsx)(n.code,{children:"example1.csv"})," into ",(0,s.jsx)(n.code,{children:"table1"}),"."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Stream Load job."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If you do not want to include the ",(0,s.jsx)(n.code,{children:"__op"})," field, run the following command:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Bash",children:'curl --location-trusted -u <username>:<password> \\\n    -H "Expect:100-continue" \\\n    -H "label:label1" \\\n    -H "column_separator:," \\\n    -T example1.csv -XPUT \\\n    http://<fe_host>:<fe_http_port>/api/test_db/table1/_stream_load\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If you want to include the ",(0,s.jsx)(n.code,{children:"__op"})," field, run the following command:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Bash",children:'curl --location-trusted -u <username>:<password> \\\n    -H "Expect:100-continue" \\\n    -H "label:label2" \\\n    -H "column_separator:," \\\n    -H "columns:__op =\'upsert\'" \\\n    -T example1.csv -XPUT \\\n    http://<fe_host>:<fe_http_port>/api/test_db/table1/_stream_load\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Broker Load job."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If you do not want to include the ",(0,s.jsx)(n.code,{children:"__op"})," field, run the following command:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label1\n(\n    data infile("hdfs://<hdfs_host>:<hdfs_port>/example1.csv")\n    into table table1\n    columns terminated by ","\n    format as "csv"\n)\nwith broker "broker1";\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If you want to include the ",(0,s.jsx)(n.code,{children:"__op"})," field, run the following command:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label2\n(\n    data infile("hdfs://<hdfs_host>:<hdfs_port>/example1.csv")\n    into table table1\n    columns terminated by ","\n    format as "csv"\n    set (__op = \'upsert\')\n)\nwith broker "broker1";\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Routine Load job."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If you do not want to include the ",(0,s.jsx)(n.code,{children:"__op"})," field, run the following command:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE ROUTINE LOAD test_db.table1 ON table1\nCOLUMNS TERMINATED BY ",",\nCOLUMNS (id, name, score)\nPROPERTIES\n(\n    "desired_concurrent_number" = "3",\n    "max_batch_interval" = "20",\n    "max_batch_rows"= "250000",\n    "max_error_number" = "1000"\n)\nFROM KAFKA\n(\n    "kafka_broker_list" ="<kafka_broker_host>:<kafka_broker_port>",\n    "kafka_topic" = "test1",\n    "property.kafka_default_offsets" ="OFFSET_BEGINNING"\n);\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["If you want to include the ",(0,s.jsx)(n.code,{children:"__op"})," field, run the following command:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE ROUTINE LOAD test_db.table1 ON table1\nCOLUMNS TERMINATED BY ",",\nCOLUMNS (id, name, score, __op =\'upsert\')\nPROPERTIES\n(\n    "desired_concurrent_number" = "3",\n    "max_batch_interval" = "20",\n    "max_batch_rows"= "250000",\n    "max_error_number" = "1000"\n)\nFROM KAFKA\n(\n    "kafka_broker_list" ="<kafka_broker_host>:<kafka_broker_port>",\n    "kafka_topic" = "test1",\n    "property.kafka_default_offsets" ="OFFSET_BEGINNING"\n);\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"query-data",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After the load is complete, query the data of ",(0,s.jsx)(n.code,{children:"table1"})," to verify that the load is successful:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table1;\n+------+------+-------+\n| id   | name | score |\n+------+------+-------+\n|  101 | Lily |   100 |\n|  102 | Rose |   100 |\n+------+------+-------+\n2 rows in set (0.02 sec)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["As shown in the preceding query result, the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"101"})," in ",(0,s.jsx)(n.code,{children:"example1.csv"})," has been updated to ",(0,s.jsx)(n.code,{children:"table1"}),", and the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"102"})," in ",(0,s.jsx)(n.code,{children:"example1.csv"})," has been inserted into ",(0,s.jsx)(n.code,{children:"table1"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"delete",children:"DELETE"}),"\n",(0,s.jsxs)(n.p,{children:["If the data file you want to load involves only DELETE operations, you must add the ",(0,s.jsx)(n.code,{children:"__op"})," field and specify the operation type as DELETE."]}),"\n",(0,s.jsx)(n.h4,{id:"data-examples-1",children:"Data examples"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Prepare a data file."}),"\n",(0,s.jsxs)(n.p,{children:["a. Create a CSV file named ",(0,s.jsx)(n.code,{children:"example2.csv"})," in your local file system. The file consists of three columns, which represent user ID, user name, and user score in sequence."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain",children:"101,Jack,100\n"})}),"\n",(0,s.jsxs)(n.p,{children:["b. Publish the data of ",(0,s.jsx)(n.code,{children:"example2.csv"})," to ",(0,s.jsx)(n.code,{children:"topic2"})," of your Kafka cluster."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Prepare a StarRocks table."}),"\n",(0,s.jsxs)(n.p,{children:["a. Create a Primary Key table named ",(0,s.jsx)(n.code,{children:"table2"})," in your StarRocks table ",(0,s.jsx)(n.code,{children:"test_db"}),". The table consists of three columns: ",(0,s.jsx)(n.code,{children:"id"}),", ",(0,s.jsx)(n.code,{children:"name"}),", and ",(0,s.jsx)(n.code,{children:"score"}),", of which ",(0,s.jsx)(n.code,{children:"id"})," is the primary key."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE TABLE `table2`\n      (\n    `id` int(11) NOT NULL COMMENT "user ID",\n    `name` varchar(65533) NOT NULL COMMENT "user name",\n    `score` int(11) NOT NULL COMMENT "user score"\n)\nENGINE=OLAP\nPRIMARY KEY(`id`)\nDISTRIBUTED BY HASH(`id`);\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsxs)(n.p,{children:["Since v2.5.7, StarRocks can automatically set the number of buckets (BUCKETS) when you create a table or add a partition. You no longer need to manually set the number of buckets. For detailed information, see ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/table_design/Data_distribution#determine-the-number-of-buckets",children:"determine the number of buckets"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["b. Insert two records into ",(0,s.jsx)(n.code,{children:"table2"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO table2 VALUES\n(101, 'Jack', 100),\n(102, 'Bob', 90);\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"load-data-1",children:"Load data"}),"\n",(0,s.jsxs)(n.p,{children:["Run a load job to delete the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"101"})," in ",(0,s.jsx)(n.code,{children:"example2.csv"})," from ",(0,s.jsx)(n.code,{children:"table2"}),"."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Stream Load job."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Bash",children:'curl --location-trusted -u <username>:<password> \\\n    -H "Expect:100-continue" \\\n    -H "label:label3" \\\n    -H "column_separator:," \\\n    -H "columns:__op=\'delete\'" \\\n    -T example2.csv -XPUT \\\n    http://<fe_host>:<fe_http_port>/api/test_db/table2/_stream_load\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Broker Load job."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.label3\n(\n    data infile("hdfs://<hdfs_host>:<hdfs_port>/example2.csv")\n    into table table2\n    columns terminated by ","\n    format as "csv"\n    set (__op = \'delete\')\n)\nwith broker "broker1";  \n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Routine Load job."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE ROUTINE LOAD test_db.table2 ON table2\nCOLUMNS(id, name, score, __op = \'delete\')\nPROPERTIES\n(\n    "desired_concurrent_number" = "3",\n    "max_batch_interval" = "20",\n    "max_batch_rows"= "250000",\n    "max_error_number" = "1000"\n)\nFROM KAFKA\n(\n    "kafka_broker_list" ="<kafka_broker_host>:<kafka_broker_port>",\n    "kafka_topic" = "test2",\n    "property.kafka_default_offsets" ="OFFSET_BEGINNING"\n);\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"query-data-1",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After the load is complete, query the data of ",(0,s.jsx)(n.code,{children:"table2"})," to verify that the load is successful:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table2;\n+------+------+-------+\n| id   | name | score |\n+------+------+-------+\n|  102 | Bob  |    90 |\n+------+------+-------+\n1 row in set (0.00 sec)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["As shown in the preceding query result, the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"101"})," in ",(0,s.jsx)(n.code,{children:"example2.csv"})," has been deleted from ",(0,s.jsx)(n.code,{children:"table2"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"upsert-and-delete",children:"UPSERT and DELETE"}),"\n",(0,s.jsxs)(n.p,{children:["If the data file you want to load involves both UPSERT and DELETE operations, you must add the ",(0,s.jsx)(n.code,{children:"__op"})," field and make sure that the data file contains a column whose values are ",(0,s.jsx)(n.code,{children:"0"})," or ",(0,s.jsx)(n.code,{children:"1"}),". A value of ",(0,s.jsx)(n.code,{children:"0"})," indicates an UPSERT operation, and a value of ",(0,s.jsx)(n.code,{children:"1"})," indicates a DELETE operation."]}),"\n",(0,s.jsx)(n.h4,{id:"data-examples-2",children:"Data examples"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Prepare a data file."}),"\n",(0,s.jsxs)(n.p,{children:["a. Create a CSV file named ",(0,s.jsx)(n.code,{children:"example3.csv"})," in your local file system. The file consists of four columns, which represent user ID, user name, user score, and operation type in sequence."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain",children:"101,Tom,100,1\n102,Sam,70,0\n103,Stan,80,0\n"})}),"\n",(0,s.jsxs)(n.p,{children:["b. Publish the data of ",(0,s.jsx)(n.code,{children:"example3.csv"})," to ",(0,s.jsx)(n.code,{children:"topic3"})," of your Kafka cluster."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Prepare a StarRocks table."}),"\n",(0,s.jsxs)(n.p,{children:["a. Create a Primary Key table named ",(0,s.jsx)(n.code,{children:"table3"})," in your StarRocks database ",(0,s.jsx)(n.code,{children:"test_db"}),". The table consists of three columns: ",(0,s.jsx)(n.code,{children:"id"}),", ",(0,s.jsx)(n.code,{children:"name"}),", and ",(0,s.jsx)(n.code,{children:"score"}),", of which ",(0,s.jsx)(n.code,{children:"id"})," is the primary key."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE TABLE `table3`\n(\n    `id` int(11) NOT NULL COMMENT "user ID",\n    `name` varchar(65533) NOT NULL COMMENT "user name",\n    `score` int(11) NOT NULL COMMENT "user score"\n)\nENGINE=OLAP\nPRIMARY KEY(`id`)\nDISTRIBUTED BY HASH(`id`);\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsxs)(n.p,{children:["Since v2.5.7, StarRocks can automatically set the number of buckets (BUCKETS) when you create a table or add a partition. You no longer need to manually set the number of buckets. For detailed information, see ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/table_design/Data_distribution#determine-the-number-of-buckets",children:"determine the number of buckets"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["b. Insert two records into ",(0,s.jsx)(n.code,{children:"table3"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO table3 VALUES\n    (101, 'Tom', 100),\n    (102, 'Sam', 90);\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"load-data-2",children:"Load data"}),"\n",(0,s.jsxs)(n.p,{children:["Run a load job to delete the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"101"})," in ",(0,s.jsx)(n.code,{children:"example3.csv"})," from ",(0,s.jsx)(n.code,{children:"table3"}),", update the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"102"})," in ",(0,s.jsx)(n.code,{children:"example3.csv"})," to ",(0,s.jsx)(n.code,{children:"table3"}),", and insert the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"103"})," in ",(0,s.jsx)(n.code,{children:"example3.csv"})," into ",(0,s.jsx)(n.code,{children:"table3"}),"."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Stream Load job:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Bash",children:'curl --location-trusted -u <username>:<password> \\\n    -H "Expect:100-continue" \\\n    -H "label:label4" \\\n    -H "column_separator:," \\\n    -H "columns: id, name, score, temp, __op = temp" \\\n    -T example3.csv -XPUT \\\n    http://<fe_host>:<fe_http_port>/api/test_db/table3/_stream_load\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsxs)(n.p,{children:["In the preceding example, the fourth column that represents the operation type in ",(0,s.jsx)(n.code,{children:"example3.csv"})," is temporarily named as ",(0,s.jsx)(n.code,{children:"temp"})," and the ",(0,s.jsx)(n.code,{children:"__op"})," field is mapped onto the ",(0,s.jsx)(n.code,{children:"temp"})," column by using the ",(0,s.jsx)(n.code,{children:"columns"})," parameter. As such, StarRocks can decide whether to perform an UPSERT or DELETE operation depending on the value in the fourth column of ",(0,s.jsx)(n.code,{children:"example3.csv"})," is ",(0,s.jsx)(n.code,{children:"0"})," or ",(0,s.jsx)(n.code,{children:"1"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Broker Load job:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Bash",children:'LOAD LABEL test_db.label4\n(\n    data infile("hdfs://<hdfs_host>:<hdfs_port>/example1.csv")\n    into table table1\n    columns terminated by ","\n    format as "csv"\n    (id, name, score, temp)\n    set (__op=temp)\n)\nwith broker "broker1";\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Routine Load job:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE ROUTINE LOAD test_db.table3 ON table3\nCOLUMNS(id, name, score, temp, __op = temp)\nPROPERTIES\n(\n    "desired_concurrent_number" = "3",\n    "max_batch_interval" = "20",\n    "max_batch_rows"= "250000",\n    "max_error_number" = "1000"\n)\nFROM KAFKA\n(\n    "kafka_broker_list" = "<kafka_broker_host>:<kafka_broker_port>",\n    "kafka_topic" = "test3",\n    "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n);\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"query-data-2",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After the load is complete, query the data of  ",(0,s.jsx)(n.code,{children:"table3"})," to verify that the load is successful:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table3;\n+------+------+-------+\n| id   | name | score |\n+------+------+-------+\n|  102 | Sam  |    70 |\n|  103 | Stan |    80 |\n+------+------+-------+\n2 rows in set (0.01 sec)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["As shown in the preceding query result, the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"101"})," in ",(0,s.jsx)(n.code,{children:"example3.csv"})," has been deleted from ",(0,s.jsx)(n.code,{children:"table3"}),", the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"102"})," in ",(0,s.jsx)(n.code,{children:"example3.csv"})," has been updated to ",(0,s.jsx)(n.code,{children:"table3"}),", and the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"103"})," in ",(0,s.jsx)(n.code,{children:"example3.csv"})," has been inserted into ",(0,s.jsx)(n.code,{children:"table3"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"partial-updates",children:"Partial updates"}),"\n",(0,s.jsx)(n.p,{children:"Since v2.2, StarRocks supports updating only the specified columns of a table that uses the Primary Key table. This section uses CSV as an example to describe how to perform partial updates."}),"\n",(0,s.jsx)(n.h3,{id:"data-examples-3",children:"Data examples"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Prepare a data file."}),"\n",(0,s.jsxs)(n.p,{children:["a. Create a CSV file named ",(0,s.jsx)(n.code,{children:"example4.csv"})," in your local file system. The file consists of two columns, which represent user ID and user name in sequence."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain",children:"101,Lily\n102,Rose\n103,Alice\n"})}),"\n",(0,s.jsxs)(n.p,{children:["b. Publish the data of ",(0,s.jsx)(n.code,{children:"example4.csv"})," to ",(0,s.jsx)(n.code,{children:"topic4"})," of your Kafka cluster."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Prepare a StarRocks table."}),"\n",(0,s.jsxs)(n.p,{children:["a. Create a Primary Key table named ",(0,s.jsx)(n.code,{children:"table4"})," in your StarRocks database ",(0,s.jsx)(n.code,{children:"test_db"}),". The table consists of three columns: ",(0,s.jsx)(n.code,{children:"id"}),", ",(0,s.jsx)(n.code,{children:"name"}),", and ",(0,s.jsx)(n.code,{children:"score"}),", of which ",(0,s.jsx)(n.code,{children:"id"})," is the primary key."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE TABLE `table4`\n(\n    `id` int(11) NOT NULL COMMENT "user ID",\n    `name` varchar(65533) NOT NULL COMMENT "user name",\n    `score` int(11) NOT NULL COMMENT "user score"\n)\nENGINE=OLAP\nPRIMARY KEY(`id`)\nDISTRIBUTED BY HASH(`id`);\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsxs)(n.p,{children:["Since v2.5.7, StarRocks can automatically set the number of buckets (BUCKETS) when you create a table or add a partition. You no longer need to manually set the number of buckets. For detailed information, see ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/table_design/Data_distribution#determine-the-number-of-buckets",children:"determine the number of buckets"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["b. Insert a record into ",(0,s.jsx)(n.code,{children:"table4"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO table4 VALUES\n    (101, 'Tom',80);\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"load-data-3",children:"Load data"}),"\n",(0,s.jsxs)(n.p,{children:["Run a load to update the data in the two columns of ",(0,s.jsx)(n.code,{children:"example4.csv"})," to the ",(0,s.jsx)(n.code,{children:"id"})," and ",(0,s.jsx)(n.code,{children:"name"})," columns of ",(0,s.jsx)(n.code,{children:"table4"}),"."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Stream Load job:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Bash",children:'curl --location-trusted -u <username>:<password> \\\n    -H "Expect:100-continue" \\\n    -H "label:label7" -H "column_separator:," \\\n    -H "partial_update:true" \\\n    -H "columns:id,name" \\\n    -T example4.csv -XPUT \\\n    http://<fe_host>:<fe_http_port>/api/test_db/table4/_stream_load\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsxs)(n.p,{children:["If you choose Stream Load, you must set the ",(0,s.jsx)(n.code,{children:"partial_update"})," parameter to ",(0,s.jsx)(n.code,{children:"true"})," to enable the partial update feature. Additionally, you must use the ",(0,s.jsx)(n.code,{children:"columns"})," parameter to specify the columns you want to update."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Broker Load job:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'LOAD LABEL test_db.table4\n(\n    data infile("hdfs://<hdfs_host>:<hdfs_port>/example4.csv")\n    into table table4\n    format as "csv"\n    (id, name)\n)\nwith broker "broker1"\nproperties\n(\n    "partial_update" = "true"\n);\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsxs)(n.p,{children:["If you choose Broker Load, you must set the ",(0,s.jsx)(n.code,{children:"partial_update"})," parameter to ",(0,s.jsx)(n.code,{children:"true"})," to enable the partial update feature. Additionally, you must use the ",(0,s.jsx)(n.code,{children:"column_list"})," parameter to specify the columns you want to update."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Routine Load job:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE ROUTINE LOAD test_db.table4 on table4\nCOLUMNS (id, name),\nCOLUMNS TERMINATED BY \',\'\nPROPERTIES\n(\n    "partial_update" = "true"\n)\nFROM KAFKA\n(\n    "kafka_broker_list" ="<kafka_broker_host>:<kafka_broker_port>",\n    "kafka_topic" = "test4",\n    "property.kafka_default_offsets" ="OFFSET_BEGINNING"\n);\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsxs)(n.p,{children:["If you choose Broker Load, you must set the ",(0,s.jsx)(n.code,{children:"partial_update"})," parameter to ",(0,s.jsx)(n.code,{children:"true"})," to enable the partial update feature. Additionally, you must use the ",(0,s.jsx)(n.code,{children:"COLUMNS"})," parameter to specify the columns you want to update."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"query-data-3",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After the load is complete, query the data of ",(0,s.jsx)(n.code,{children:"table4"})," to verify that the load is successful:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table4;\n+------+-------+-------+\n| id   | name  | score |\n+------+-------+-------+\n|  102 | Rose  |     0 |\n|  101 | Lily  |    80 |\n|  103 | Alice |     0 |\n+------+-------+-------+\n3 rows in set (0.01 sec)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["As shown in the preceding query result, the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"101"})," in ",(0,s.jsx)(n.code,{children:"example4.csv"})," has been updated to ",(0,s.jsx)(n.code,{children:"table4"}),", and the records whose ",(0,s.jsx)(n.code,{children:"id"})," are ",(0,s.jsx)(n.code,{children:"102"})," and ",(0,s.jsx)(n.code,{children:"103"})," in ",(0,s.jsx)(n.code,{children:"example4.csv"})," have been Inserted into ",(0,s.jsx)(n.code,{children:"table4"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"conditional-updates",children:"Conditional updates"}),"\n",(0,s.jsx)(n.p,{children:"From StarRocks v2.5 onwards, Primary Key tables support conditional updates. You can specify a non-primary key column as the condition to determine whether updates can take effect. As such, the update from a source record to a destination record takes effect only when the source data record has a greater or equal value than the destination data record in the specified column."}),"\n",(0,s.jsx)(n.p,{children:"The conditional update feature is designed to resolve data disorder. If the source data is disordered, you can use this feature to ensure that new data will not be overwritten by old data."}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTICE"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"You cannot specify different columns as update conditions for the same batch of data."}),"\n",(0,s.jsx)(n.li,{children:"DELETE operations do not support conditional updates."}),"\n",(0,s.jsx)(n.li,{children:"Partial updates and conditional updates cannot be used simultaneously."}),"\n",(0,s.jsx)(n.li,{children:"Only Stream Load and Routine Load support conditional updates."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"data-examples-4",children:"Data examples"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Prepare a data file."}),"\n",(0,s.jsxs)(n.p,{children:["a. Create a CSV file named ",(0,s.jsx)(n.code,{children:"example5.csv"})," in your local file system. The file consists of three columns, which represent user ID, version, and user score in sequence."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Plain",children:"101,1,100\n102,3,100\n"})}),"\n",(0,s.jsxs)(n.p,{children:["b. Publish the data of ",(0,s.jsx)(n.code,{children:"example5.csv"})," to ",(0,s.jsx)(n.code,{children:"topic5"})," of your Kafka cluster."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Prepare a StarRocks table."}),"\n",(0,s.jsxs)(n.p,{children:["a. Create a Primary Key table named ",(0,s.jsx)(n.code,{children:"table5"})," in your StarRocks database ",(0,s.jsx)(n.code,{children:"test_db"}),". The table consists of three columns: ",(0,s.jsx)(n.code,{children:"id"}),", ",(0,s.jsx)(n.code,{children:"version"}),", and ",(0,s.jsx)(n.code,{children:"score"}),", of which ",(0,s.jsx)(n.code,{children:"id"})," is the primary key."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE TABLE `table5`\n(\n    `id` int(11) NOT NULL COMMENT "user ID", \n    `version` int NOT NULL COMMENT "version",\n    `score` int(11) NOT NULL COMMENT "user score"\n)\nENGINE=OLAP\nPRIMARY KEY(`id`) DISTRIBUTED BY HASH(`id`) BUCKETS 10;\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,s.jsxs)(n.p,{children:["Since v2.5.7, StarRocks can automatically set the number of buckets (BUCKETS) when you create a table or add a partition. You no longer need to manually set the number of buckets. For detailed information, see ",(0,s.jsx)(n.a,{href:"/doc/docs/2.5/table_design/Data_distribution#determine-the-number-of-buckets",children:"determine the number of buckets"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["b. Insert a record into ",(0,s.jsx)(n.code,{children:"table5"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"INSERT INTO table5 VALUES\n    (101, 2, 80),\n    (102, 2, 90);\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"load-data-4",children:"Load data"}),"\n",(0,s.jsxs)(n.p,{children:["Run a load to update the records whose ",(0,s.jsx)(n.code,{children:"id"})," values are ",(0,s.jsx)(n.code,{children:"101"})," and ",(0,s.jsx)(n.code,{children:"102"}),", respectively, from ",(0,s.jsx)(n.code,{children:"example5.csv"})," into ",(0,s.jsx)(n.code,{children:"table5"}),", and specify that the updates take effect only when the ",(0,s.jsx)(n.code,{children:"verion"})," value in each of the two records is greater or equal to their current ",(0,s.jsx)(n.code,{children:"version"})," values."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Stream Load job:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-Bash",children:'curl --location-trusted -u <username>:<password> \\\n    -H "Expect:100-continue" \\\n    -H "label:label10" \\\n    -H "column_separator:," \\\n    -H "merge_condition:version" \\\n    -T example5.csv -XPUT \\\n    http://<fe_host>:<fe_http_port>/api/test_db/table5/_stream_load\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run a Routine Load job:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:'CREATE ROUTINE LOAD test_db.table5 on table5\nCOLUMNS (id, version, score),\nCOLUMNS TERMINATED BY \',\'\nPROPERTIES\n(\n    "merge_condition" = "version"\n)\nFROM KAFKA\n(\n    "kafka_broker_list" ="<kafka_broker_host>:<kafka_broker_port>",\n    "kafka_topic" = "topic5",\n    "property.kafka_default_offsets" ="OFFSET_BEGINNING"\n);\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"query-data-4",children:"Query data"}),"\n",(0,s.jsxs)(n.p,{children:["After the load is complete, query the data of ",(0,s.jsx)(n.code,{children:"table5"})," to verify that the load is successful:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-SQL",children:"SELECT * FROM table5;\n+------+------+-------+\n| id   | version | score |\n+------+------+-------+\n|  101 |       2 |   80 |\n|  102 |       3 |  100 |\n+------+------+-------+\n2 rows in set (0.02 sec)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["As shown in the preceding query result, the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"101"})," in ",(0,s.jsx)(n.code,{children:"example5.csv"})," is not updated to ",(0,s.jsx)(n.code,{children:"table5"}),", and the record whose ",(0,s.jsx)(n.code,{children:"id"})," is ",(0,s.jsx)(n.code,{children:"102"})," in ",(0,s.jsx)(n.code,{children:"example5.csv"})," has been Inserted into ",(0,s.jsx)(n.code,{children:"table5"}),"."]})]})}const h=function(e={}){const{wrapper:n}=Object.assign({},(0,d.ah)(),e.components);return n?(0,s.jsx)(n,Object.assign({},e,{children:(0,s.jsx)(c,e)})):c(e)}},11151:(e,n,a)=>{a.d(n,{Zo:()=>t,ah:()=>o});var s=a(67294);const d=s.createContext({});function o(e){const n=s.useContext(d);return s.useMemo((()=>"function"==typeof e?e(n):{...n,...e}),[n,e])}const r={};function t({components:e,children:n,disableParentContext:a}){let t;return t=a?"function"==typeof e?e({}):e||r:o(e),s.createElement(d.Provider,{value:t},n)}}}]);